---
layout: post
title: Coarse-Graining, History, and CT Models
tags: [cultural transmission, coarse graining, renormalization, diachronic models, history]
category: "coarse grained model project"
---

{{ page.title }}
----------------

<div class="publish_date">
{{ page.date | date_to_string }}
</div>


By coarse-graining (CG) our CT models, we hope to construct models which can be used to extract observable predictions and thus testable hypotheses at particular scales.

But it's important to keep in mind that the models we usually analyze are simple equilibrium processes (e.g., neutrality, conformism), and their behavior is usually highly convergent. It may not be strictly ergodic, since the exact equilibria of a full process may depend upon initial conditions, or the variants preserved, but unlabled observables (i.e., instead of the exact frequency distribution of variants, we can look at the frequency spectrum of unlabeled variants as Ewens 2004 does) will achieve equilibrium values.

CGing such a process eventually yields a universality class of models, given the strong parallels between stochastic CT models and equilibrium statistical mechanics (in fact, the q-state Potts model can be mapped exactly to neutral k-alleles mutation models). Which means that at some scale, it becomes useless to talk about detailed models. Instead, we're interested in unique histories, and how universal processes shape those unique histories.

When Will CG Equilibrium Models Be Useful?
------------------------------------------

This is not to say that equilibrium models, appropriately CG'd, aren't useful empirically. Consider a situation where we have a series of assemblages, with duration understood at least to rough interval estimates for each, and what we know are the frequencies of observational classes (e.g., artifact classes), and thus the frequency distribution or spectrum.

The data are observations which are coarse-grained in at least two relevant ways from microscopic observables, in addition to taphonomic loss functions which winnow the original variation in artifact discard.

* Accumulative temporal aggregation
* Classificatory aggregation

If we understand how an equilibrium CT model CG's under these two types of transforms, and how detailed models converge to universality (or partial universality -- i.e., equifinality), then the frequency spectra predicted by models can be evaluated against empirical spectra for goodness of fit.

What might such goodness of fit tell us? Broad parameter regions for broad sets of models, perhaps. Or if models fall into partial universality classes with respect to some observables, rather than full convergence, then we can do model selection as a means of "induction to the best model." But in a direct empirical sense, that's pretty much it.

We might also use such models as components of other models, as well -- as the bookkeeping mechanism for models focusing on selection or game theoretic interactions.

Historical CT Models
-------------------- 

But often our interest is going to be in specific historical trajectories. Because our empirical data,and the phenomena from which we measure those data, are a unique historical trajectory.

In some sense, we want to know, "how likely is it, that trajectory X is an outcome from stochastic process Y." We can do this for a sampled time series from a stochastic process, but it's much harder to do with scattered data points. 

And in any case, our processes are rarely in equilibrium and ergodic. So we need a different notion of what the question "what kind of histories does process Y generate?" 

Coalescent theory is the right kind of answer, for a set of time-concurrent samples and an equilibrium, backward-facing process. It's a specialized tool for answering questions about the history and characteristics of a diachronic evolutionary process given samples of its outcomes. Cladistics is another kind of "history" that has shape and characteristics of its own, which differ between processes. 

* How do cladograms vary under different processes?
* This is strongly related to my dissertation question about how seriations vary -- again, a diachronic view on a non-equilibrium process. 

Richer CT Models
---------------- 

Ultimately, our simplest CT models are pure imitation models. B-D and D-B updating give us two "directions" for the causality here, and can have slightly different consequences in structured populations (and, possibly, affect whether a population of learners is "Darwinian enough" in Godrey-Smith's terms to support cumulative, creative, selection). 

But variation tends to both "churn" and "drain" from these types of CT models. And that might be appropriate for neutral models where we don't have a performance-based reason for something to "work." But much of the cultural legacy we seek to explain is also involved in doing real work in supporting us, and thus there are powerful conservative forces (in the physics sense) keeping technologies and artifact complexes relatively stable. There is, in Wimsatt's terms, "generative entrenchment" of some information and designs, which become foundational for other elements, which fit together in both designed and accidental (kludged) ways. 

This kind of structured information has yet to be incorporated into our models, although Gabora to her credit alludes to this in certain ways in her writings (and I need to spend more time with Sperber, who seems to see this as a major criticism of B&R-style CT models). 





Tagged
------
<div class="taglist">
{{ page.tags | array_to_sentence_string }}
</div>
