
<!DOCTYPE html>
<html lang="en">
  <head prefix="dc: http://purl.org/dc/terms/ og: http://ogp.me/ns#">
    <meta charset="utf-8"/>
    <title>Changing Strategy for Cultural Transmission Computing</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="description" content="Lab Notebook for Mark E. Madsen"/>
    <meta name="author" content="Mark E. Madsen"/>

	<!-- Get date last modified from git log. (Uses current time if file entry not found, e.g. projects/)  -->
	


	<!-- For posts, page.date is the date they are published under, which we use as their 'canonical' dc:date -->
	 
	  
	  

	<!-- Posts declare modified timestamps in the sidebar, so would be redundant to put here. But then 
	     pages don't have a dc:modified... unless we give them their own (modified) sidebar?  
	-->
	<!-- Ideally we would want date originally created from the _oldest_ git commit too...-->

	<!-- RDFa Metadata (in DublinCore) -->
	<meta property="dc:title" content="Changing Strategy for Cultural Transmission Computing" />
	<meta property="dc:creator" content="Mark E. Madsen" />
	<meta property="dc:date" content="2015-07-07T00:00:00-07:00" />
	<meta property="dc:format" content="text/html" />
	<meta property="dc:language" content="en" />
	<meta property="dc:identifier" content="/essays/2015/07/07/new-directions-ct-computing.html" />
	<meta property="dc:rights" content="CC BY-NC-SA 3.0" />
	<meta property="dc:source" content="Lab Notebook for Mark E. Madsen" />
	<meta property="dc:subject" content="Anthropology" /> 
	<meta property="dc:type" content="website" /> 
	<!-- RDFa Metadata (in OpenGraph) -->
	<meta property="og:title" content="Changing Strategy for Cultural Transmission Computing" />
	<meta property="og:author" content="http://notebook.madsenlab.org/bio.html" />  <!-- Should be Liquid? URI? -->
	<meta property="og:site_name" content="Lab Notebook for Mark E. Madsen" /> <!-- Same as dc:source? -->
	<meta property="og:url" content="http://notebook.madsenlab.org/essays/2015/07/07/new-directions-ct-computing.html" />
	<meta property="og:type" content="website" /> 
	<!-- Google Scholar Metadata -->
	<meta name="citation_author" content="Mark E. Madsen"/>
	<meta name="citation_date" content="2015-07-07T00:00:00-07:00"/>
	<meta name="citation_title" content="Changing Strategy for Cultural Transmission Computing"/>
	<meta name="citation_journal_title" content="Lab Notebook for Mark E. Madsen"/>

    <!-- Le styles -->
    <link href="/assets/css/bootstrap.css" rel="stylesheet">
    <link href="/assets/css/octicons.css" rel="stylesheet">
    <style type="text/css">
      body {
        padding-top: 60px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }

      @media (max-width: 980px) {
        /* Enable use of floated navbar text */
        .navbar-text.pull-right {
          float: none;
          padding-left: 5px;
          padding-right: 5px;
        }
      }
    </style>
    <link href="/assets/css/bootstrap-responsive.css" rel="stylesheet">
    <link href="/css/syntax.css" rel="stylesheet">
    
  <script type="text/javascript" src="//use.typekit.net/xtg2arw.js"></script>
  <script type="text/javascript">try{Typekit.load();}catch(e){}</script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script>
  MathJax.Hub.Config({
    showProcessingMessages: false,
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
  </script>
<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-11944342-4', 'madsenlab.org');
  ga('send', 'pageview');

</script>

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="../assets/js/html5shiv.js"></script>
    <![endif]-->

    <!-- Fav and touch icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../assets/ico/apple-touch-icon-114-precomposed.png">
      <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../assets/ico/apple-touch-icon-72-precomposed.png">
                    <link rel="apple-touch-icon-precomposed" href="../assets/ico/apple-touch-icon-57-precomposed.png">
                                   <link rel="shortcut icon" href="../assets/ico/favicon.png">
  </head>

  <body>

    <div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <button type="button" class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="brand" href="/">MadsenLabNotebook</a>
          <div class="nav-collapse collapse">
                  <!-- Search site using Google's index -->
      <form class="navbar-search pull-right" method="get" action="http://google.com/search">
        <p>
          <input type="hidden" name="q" value="site:notebook.madsenlab.org" />
          <input type="text" class="search-query" name="q" />
          <button class="btn btn-mini" type="submit"><i class="icon-search"></i></button> 
        </p>
      </form>
            <ul class="nav">
              <li class="active"><a href="/">Home</a></li>
              <li><a href="/bio.html">About Me</a></li>
              <li><a href="http://about.me/mark.e.madsen">Contact</a></li>
              <li><a href="/blog.html">Essays</a></li>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span3">
          <div class="well sidebar-nav">
            <ul class="nav nav-list">
              <li><img src="/images/madsenlabnotebook-350px.png" class="img-responsive" alt="MadsenLab Notebook"/></li>
              <li class="nav-header">General</li>
              <li><a href="/tags.html">Notes by Tag</a></li>
              <li><a href="/allposts.html">All Notes by Date</a></li>
              <li><a href="/blog.html">Essays and General Writings</a></li>
              <li class="nav-header">Projects</li>
              <li><a href="/projects/coarsegraining/">Coarse Grained Models of CT</a></li>
              <li><a href="/projects/nicheconstruction/">Niche Construction and CT Modeling</a></li>
              <li><a href="/projects/structuredinfo/">CT of Structured Information</a></li>
              <li class="nav-header">Simulations and Experiments</li>
              <li><a href="/byexperiment.html">Notes by Experiment</a></li>
              <li><a href="/bymodel.html">Notes by Simulation Model</a></li>
              <li class="nav-header">Topics</li>
              <li><a href="/tag/cultural-transmission.html">Cultural transmission theory</a></li>
              <li><a href="/tag/evolutionary-theory.html">Darwinian evolutionary theory</a></li>
			        <li><a href="/tag/dissertation.html">Dissertation Research</a></li>
              <li><a href="/tag/reading-notes.html">Reading notes</a></li>
              <li><a href="/openproblems.html">Open Research Problems</a></li>
              <li><a href="/projects/slipbox">Slipbox Research Notes</a></li>
              <li class="nav-header">Meta</li>
              <li><a href="/bio.html">Bio</a></li>
              <li><a href="/software.html">Software</a></li>
              <li><a href="/tools.html">Tools I Use</a></li>
              <li><a href="/publications.html">Publications</a></li>
              <li><a href="/openscience.html">About Open Science</a></li>
			<li><a href="/labnotebook.html">About this Notebook</a></li>
            </ul>
          </div><!--/.well -->
        </div><!--/span3-->

            <div class="span8">
  	<h2>Changing Strategy for Cultural Transmission Computing</h2>
	

		<p class="text-info">
		<table class="table table-condensed">
			<tr class="info">
				<td><small><i class="icon-backward"></i>&nbsp;<a href="/blog.html">Back to Essay List</a></small></td>
				<td><small><i class="icon-calendar"></i>&nbsp;Modified: 21 Jul 2020</small></td>
				<td><small><i class="icon-book">&nbsp;</i>&nbsp;<a href=http://greycite.knowledgeblog.org/bib/?uri=http://notebook.madsenlab.org/essays/2015/07/07/new-directions-ct-computing.html target='_blank'>BibTeX Entry</a>
</small></td>
				<td><small><i class="icon-book">&nbsp;</i>&nbsp;<a href=http://greycite.knowledgeblog.org/ris/?uri=http://notebook.madsenlab.org/essays/2015/07/07/new-directions-ct-computing.html target='_blank'>RIS Citation</a>
</small></td>
				<td><small><a href="javascript:window.print()"><i class="icon-print"></i>&nbsp;Print</a></small></td>
			</tr>
		</table>
		</p>


  	
	
  <h3 id="background">Background</h3>
<p>For the last two or three years, my computing strategy for simulations (and post-simulation analysis) of cultural transmission models has been as follows:</p>
<ol type="1">
<li>Develop simulation code in Python, sometimes with a framework which has C++ extensions for speed.<br />
</li>
<li>Test code on my laptop (quad-core, OS X), and occasionally make performance changes.</li>
<li>Run production analyses and large parameter sweeps on clusters of Amazon EC2 instances, gathering results in a database.</li>
<li>Do final analyses locally on my laptop unless they, too, needed a very large machine or a cluster.</li>
</ol>
<p>This isn’t a bad strategy:</p>
<ol type="1">
<li>Python is usually fast enough.<br />
</li>
<li>Using EC2 means that I don’t have to manage extra hardware, and I’m not worried about investing in hardware that goes obsolete quickly.</li>
<li>Using EC2 also means that I can select the right <strong>amount</strong> of hardware for problem, and only pay for what I use.</li>
<li>Using straight Python has meant that development velocity is fast, given the forgiving nature of a dynamically typed scripting language and the huge library support.</li>
</ol>
<p>The negatives been cost and performance. On average, I’ve been spending anywhere from $250 - $600 a month when I’m doing heavy analyses, and nothing sucks worse than having a cluster of 8 big compute-optimized instances grind away for a full week and then discover that you really needed to do the analysis a bit differently (and that you just burned a hundred bucks or more on CPU time that contributes nothing to the final result).</p>
<p>The bigger problem has been the “cycle time” on simulation experiments. When it takes a week or two weeks to go from a new simulation code to a set of results one can assess, it can take months to go from an idea to a full set of results. This is just too long, especially as I try to deal with the last, and hardest, element of my dissertation research. I’d like to reduce this cycle time so that I can test more ideas, so that false starts aren’t so costly, and because the search space for the problem I’m addressing is huge, so I need to be able to do large samples.</p>
<p>I managed to do 27 different numerical experiments in about two months for a conference last winter, where most of the iteration time involved doing a large number of seriations from each round of simulation experiments. While further scripting and automation will help with cycle time, I also need to simply improve computational throughput on seriation and analyses (and secondarily, on the primary CT simulations themselves, although this was less important in the cycle time).</p>
<h3 id="improving-cost">Improving Cost</h3>
<p>Improving my costs should be easy, over time. Routinely, I’ve been using EC2 instances with 8 or 16 cores so I can run multiple simulations, or give our seriation algorithms enough cores to test possible solutions in parallel. Having such a system inhouse will be cheaper than using EC2 for the majority of my work if I keep it working for at least 18-24 months, given what I’m averaging on EC2 right now, and if I don’t need to “burst” higher with EC2 very often. On a couple of occasions, I’ve used clusters of instances with 64 total cores to handle a large batch of simulations that required long run times.</p>
<p>So I set out with the goal to buy one mid-sized system with enough computing power to allow me to run most jobs locally, and save EC2 computing for occasional large tasks.</p>
<h3 id="many-core-architecture">Many Core Architecture</h3>
<p>It’s easy to spec out an 8 core Xeon system, and put a large gamer/prosumer grade NVIDIA GPU into it, and call that a good development system. Except that it’s not clear that the GPU programming model will help me much, although there are certainly areas where being able to offload large matrix calculations to a GPU (using NumbaPro from the <a href="http://continuum.io">Anaconda Python folks at Continuum.io</a>, for example) might help with the fitting of statistical models. GPU programming relies upon a very specific computing model, called “SPMD”, an extension of the general SIMD model for vectorization. In SPMD, a large calculation is broken down into an elemental function which operates on a single cell or small group of cells, performing exactly the same calculation (or “kernel”) on each, which are then combined by the runtime into the result for an entire data block.</p>
<p>This model is perfect for graphics computation, of course, given the origins of the GPU programming model. But it’s terrible for general parallel programming, and worse than the host CPU for general combinations of serial and parallel code. And unfortunately, most simulation programming in the social sciences doesn’t fit neatly into the SPMD/GPU model. Code which contains conditional branches will block the lockstep flow of computation across all of the compute cores in a GPU, and yield performance even slower than the host processor in many cases. And even if I can rethink the way my simulation codes work, and eliminate if/then branching logic in favor of masked pipelines of small kernels run in sequence, the redevelopment time would be long and there’s no guarantee of a major speedup.</p>
<p>But we live in interesting times, and Intel’s recent MIC architecture (embodied in the Xeon Phi coprocessor) offers a different path. The current generation of the Phi (the first production generation, code named “Knight’s Corner”) started shipping to general customers in 2012 and 2013, and are now the cornerstone of the fastest supercomputer on record, <a href="https://en.wikipedia.org/wiki/Tianhe-2">Tianhe-2</a>, which has 16K compute nodes, each of which has two Xeon processors and 3 Phi cards.</p>
<p>The Phi cards shipping today offer an embedded Linux subsystem, with 61 cores capable of over 1 teraflops/sec of double precision floating point performance (if you can feed it enough data smoothly enough). This means that a system with a single Xeon host processor and a Xeon Phi 5110P card, has the same floating point performance as the ASCI Red supercomputer which topped the Top500 supercomputer list in 1999 and 2000. The computing power of a US National Laboratory ten years ago, but it fits on your desktop.</p>
<p>Naturally, I ordered exactly this configuration, and am waiting for <a href="http://www.pugetsystems.com">Puget Systems</a> to deliver it this week or next. The best part? Right now, <a href="https://www.pugetsystems.com/labs/articles/Xeon-Phi-5110p-and-Free-Intel-Parallel-Studio-Cluster-Edition-670/">a developer program with Intel will get you a Xeon Phi 5110P</a> coprocessor card for less than $500, along with a year’s license for the Intel compiler stack and cluster computing tools, which normally are fairly expensive (although in the same realm as <strong>Mathematica</strong> for academic use). The downside is that the passively cooled Phi cards need some extra cooling and good system design and testing, unless you simply want to cook a $4-5000 computer down to slag fairly quickly. That’s where Puget Systems and their expertise come in – sizing the power, selecting and testing the cooling system, and making sure that you’re good to go. Even if you pay them a small premium for integrating the box, the work they’re doing is impressive and I think it’s going to be well worth it.</p>
<p>How to use it?</p>
<h3 id="improving-parallelism-and-code-performance">Improving Parallelism and Code Performance</h3>
<p>The Phi can augment the host processor in several ways, since it runs a full (but basic) Linux distribution. In “native” mode, you can compile code for the coprocessor, for example using OpenMP to parallelize tasks across the cores, and run code against 61 cores as long as the memory present on the Phi itself (8 or 12GB) is adequate. Which it might very well be, for code which deals with small data (common in agent-based simulation models), but has complex branching structure or algorithms which run on each core.</p>
<p>The second mode is “offload” mode, in which your code runs on the host processor, but certain operations can be parallelized onto the coprocessor for a boost. Here’s where it gets interesting. It might be possible to offload directly from Python code, and to offload NumPy expressions directly to the Phi with the main program running on the host processor (or across many of the host cores). NumbaPro and OpenCL are paths to this type of offloading, and even if not done explicitly, if your copy of NumPy is linked against the Intel MKL math libraries on Linux, offloading can be done at the library level even if Python is unaware of it.<br />
The latter offers the possibility of offloading work from libraries like scikit-learn for machine learning and statistical algorithms, even if the ML library itself is coprocessor-unaware.</p>
<p>And if you use R, switch to the RevolutionR Open version of the runtime, which is linked against the Intel MKL math libraries. Again, it may be possible to have R use the Xeon Phi coprocessor for algoritms which involve larger NumPy calculations without modifications to your R code.</p>
<p>See, it’s getting very interesting, isn’t it?</p>
<p>I’m not sure how much of this automatic offloading will work “off the shelf” from Python, but given NumbaPro from the Anaconda folks, I don’t imagine it’ll be long before all of it will. And <em>anything</em> you write in C++ can be made to use the coprocessor immediately, so compute-critical sections in your Python code can always be linked via Cython or SWIG.</p>
<p>The future looks even more interesting. The next generation of Phi chips starts shipping commercially in 2016, so I didn’t go all out this time. Next year, the “Knight’s Landing” architecture will put the Phi in motherboard sockets, eliminating much of the time needed to move data over the PCI bus to the coprocessor card, and yielding a true 75 or 150 core server system. Pair one of those systems with a box running 1 or 2 “normal” Haswell/Broadwell Xeon processors, and you’ve got a small 160-175 core cluster in the space it used to take for a couple of workstations.</p>
<h3 id="next-steps">Next Steps</h3>
<p>So I’m bullish on the opportunity to drastically speed up cultural transmission simulations, seriation analyses, the use of “Approximate Bayesian Computation” for statistical inference on these complex models, and even phylogenetic analysis codes, in fairly short order.</p>
<p>Which is good, because the social sciences are getting more and more computationally complex, and we have less and less funding for buying time on large clusters or renting time from major cloud services.</p>
<p>I’ll report back when I have some code up and running.</p>
  



<div class="well well-small">
<table class="table table-condensed">
			<tr>
				<td><small><i class="icon-backward"></i>&nbsp;<a href="/blog.html">Back to Essay List</a></small></td>
				<td><small><i class="icon-calendar"></i>&nbsp;Modified: 21 Jul 2020</small></td>
				<td><small><i class="icon-book">&nbsp;</i>&nbsp;<a href=http://greycite.knowledgeblog.org/bib/?uri=http://notebook.madsenlab.org/essays/2015/07/07/new-directions-ct-computing.html target='_blank'>BibTeX Entry</a>
</small></td>
				<td><small><i class="icon-book">&nbsp;</i>&nbsp;<a href=http://greycite.knowledgeblog.org/ris/?uri=http://notebook.madsenlab.org/essays/2015/07/07/new-directions-ct-computing.html target='_blank'>RIS Citation</a>
</small></td>
				<td><small><a href="javascript:window.print()"><i class="icon-print"></i>&nbsp;Print</a></small></td>
			</tr>
		</table>
<p class="text-info"><i class="icon-tags"></i>&nbsp;Tags</p>
<ul>

<li><a href='/tag/cultural+transmission.html'>cultural transmission</a></li>

<li><a href='/tag/simulation.html'>simulation</a></li>

<li><a href='/tag/computing.html'>computing</a></li>

<li><a href='/tag/HPC.html'>HPC</a></li>

<li><a href='/tag/seriation.html'>seriation</a></li>

<li><a href='/tag/ML.html'>ML</a></li>

<li><a href='/tag/computational+science.html'>computational science</a></li>

</ul>
</div>




</div>


        
      </div><!--/row-->


    


 
      <hr>

      <footer>
		        <p>&copy; Mark E. Madsen 2011-2020    <a href="mailto:mark@madsenlab.org">Email</a>&nbsp;||
		  <a href="http://orcid.org/0000-0001-9320-8377">ORCId</a>&nbsp;||
		  <a href="http://figshare.com/authors/Mark%20Madsen/279062">Figshare</a>&nbsp;||
		    <a href="https://www.linkedin.com/in/markemadsen/">LinkedIn</a>&nbsp;|| 	    <a href="http://github.com/mmadsen">GitHub</a>&nbsp;||
        <a href="http://www.researchgate.net/profile/Mark_Madsen3/">ResearchGate</a>&nbsp;||
        <a href="https://washington.academia.edu/MarkMadsen">Academia.edu</a>&nbsp;||
        <a href="https://zenodo.org/search?ln=en&cc=software&p=Mark+Madsen&action_search=">Zenodo</a></p>
      </footer>

    </div><!--/.fluid-container-->

    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="/assets/js/jquery.js"></script>
    <script src="/assets/js/bootstrap-transition.js"></script>
    <script src="/assets/js/bootstrap-alert.js"></script>
    <script src="/assets/js/bootstrap-modal.js"></script>
    <script src="/assets/js/bootstrap-dropdown.js"></script>
    <script src="/assets/js/bootstrap-scrollspy.js"></script>
    <script src="/assets/js/bootstrap-tab.js"></script>
    <script src="/assets/js/bootstrap-tooltip.js"></script>
    <script src="/assets/js/bootstrap-popover.js"></script>
    <script src="/assets/js/bootstrap-button.js"></script>
    <script src="/assets/js/bootstrap-collapse.js"></script>
    <script src="/assets/js/bootstrap-carousel.js"></script>
    <script src="/assets/js/bootstrap-typeahead.js"></script>

  </body>
</html>
