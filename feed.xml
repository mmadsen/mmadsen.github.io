<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.2">Jekyll</generator><link href="http://notebook.madsenlab.org/feed.xml" rel="self" type="application/atom+xml" /><link href="http://notebook.madsenlab.org/" rel="alternate" type="text/html" /><updated>2016-03-03T15:17:34-08:00</updated><id>http://notebook.madsenlab.org/</id><title>Lab Notebook for Mark E. Madsen</title><author><name>Mark E. Madsen</name></author><entry><title>Intentionality and Cultural Evolution - Towards a Generalized Learning Theory Account</title><link href="http://notebook.madsenlab.org/essays/2016/03/03/cultural-evolution-intentionality-pac-learning.html" rel="alternate" type="text/html" title="Intentionality and Cultural Evolution - Towards a Generalized Learning Theory Account" /><published>2016-03-03T00:00:00-08:00</published><updated>2016-03-03T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/essays/2016/03/03/cultural-evolution-intentionality-pac-learning</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/2016/03/03/cultural-evolution-intentionality-pac-learning.html">&lt;p&gt;(&lt;strong&gt;the following is a continuation and completion of a &lt;a href=&quot;http://notebook.madsenlab.org/cultural%20transmission/theory/essays/2013/06/13/gabora2013-response-notes.html&quot;&gt;post begun in 2013&lt;/a&gt;&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;Among those who object to framing cultural evolution as a Darwinian theory, one of the most important reasons for objection is the evident importance of intentionality in human behavior (and among many animal species). Liane Gabora has perhaps been one of the most persistent advocates that while culture evolves, it does so by mechanisms other than natural selection, since natural selection requires variation to be “random” &lt;span class=&quot;citation&quot; data-cites=&quot;Gabora2013a&quot;&gt;(Gabora 2013)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;generation-of-variation-in-darwinian-processes&quot;&gt;Generation of Variation in Darwinian Processes&lt;/h3&gt;
&lt;p&gt;Or does it? Lipo and I argued in response that most modern commentators are overinterpreting the term “random” in the core Darwinian paradigm: that the generation of variation is merely &lt;strong&gt;causally unprivileged&lt;/strong&gt; with respect to the differential persistence of that variation &lt;span class=&quot;citation&quot; data-cites=&quot;Madsen2013a&quot;&gt;(Madsen and Lipo 2013)&lt;/span&gt;. David Sloan Wilson, in a recent and very clear article on intentional cultural change &lt;span class=&quot;citation&quot; data-cites=&quot;Wilson:2016bc&quot;&gt;(Wilson 2016)&lt;/span&gt;, makes the same point very crisply:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the standard portrayal of genetic evolution, mutations occur that are arbitrary with respect to their consequences for survival and reproduction (fitness). Those that enhance fitness increase in frequency until they become species-typical. The word ‘arbitrary’ rather than ‘random’ in the previous sentence is deliberate. If a mutation is random, then it results in a new phenotype that deviates from the previous phenotype in any direction with equal probability. The standard portrayal of genetic evolution does not assume that mutations are random in this sense. Instead, the assumption is that mutations do not anticipate the phenotypes that are favored by natural selection. This is the meaning of the word ‘arbitrary’.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In fact, we can look at variation-generating mechanisms along several dimensions, as shown in this diagram. Variation may be “undirected” or “directed.” Directed variations are innovations or errors that occur when a variation-generation mechanism targets specific components of the genome or cultural repertoire. In contrast, undirected variations are errors or innovations which occur in some random component of the genome or cultural repertoire.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/variation-classes.png&quot; alt=&quot;Classes of Variation&quot; /&gt;&lt;figcaption&gt;Classes of Variation&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The classic example of undirected variation is the cosmic ray streaking through a cell nucleus, causing damage to DNA which results in “flipping” one or more nucleotides during the repair process. This is the archetype for what nearly everyone means when they talk about “blind variation” in Darwinian evolution. In cultural evolution, a person copying an artifact will make motor and perceptual errors in copying, which are often random with respect to which physical dimension they occur upon &lt;span class=&quot;citation&quot; data-cites=&quot;Eerkens2005&quot;&gt;(Eerkens and Lipo 2005)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But we now also understand that there are many mechanisms whereby variation can be generated in a “directed” fashion. Environmental stress seems to have a variety of effects on the rate and location of mutations in the genome, and there are a variety of epigenetic mechanisms whereby temporary changes in gene expression can be inherited by the next generation, and thus permanently affect a lineage.&lt;/p&gt;
&lt;p&gt;The important thing to understand about such “directed” mechanisms for generating variation is that while they are &lt;strong&gt;targeted&lt;/strong&gt;, they are &lt;strong&gt;unprivileged&lt;/strong&gt; with respect to knowing how selection will ultimately filter the results of their generation. Causal arrows run one direction, and variation is always generated &lt;strong&gt;before&lt;/strong&gt; selection affects the frequency of variants. Genomic mechanisms which increase the mutation rate in selected regions of the genome are certainly the products of &lt;strong&gt;past&lt;/strong&gt; selection, but there is no causal arrow which gives them information about how the results of their action will fare in &lt;strong&gt;future&lt;/strong&gt; survival and reproduction.&lt;/p&gt;
&lt;p&gt;And this, really, points to the way out of the issue. There is simply no requirement that variation be “random” with respect to…anything. The generation of variation is simply causally uncoupled, or unprivileged, from the “judging” of its fitness or utility down the road. The “two step process” of Darwinian evolution is &lt;strong&gt;defined&lt;/strong&gt; by that uncoupling.&lt;/p&gt;
&lt;h3 id=&quot;intentionality&quot;&gt;Intentionality&lt;/h3&gt;
&lt;p&gt;Which leads us to the thorny issue of “intentional” behavior in a mechanistic, Darwinian theory. My mentor, Robert Dunnell, was vehemently opposed to the inclusion of intentionality in any scientific theory of cultural change, for a variety of reasons which were correct. Mostly, the objection to intentionality is the causal role it plays in most social sciences, short circuiting the “two step” process and asserting that change is often a “one step” process whereby people perceive a problem, choose the best solution to deal with it, and implement that solution. Theories built on this kind of logic include variations of rational actor models in economics, overly simplistic versions of adaptationist evolutionary ecology, public choice theory, and of course a long list of unilinear, progressivist, vitalist, and Lamarckian models of cultural change in anthropology. All of which have been instrumental at various times in preventing us from constructing testable, scientific accounts of cultural change.&lt;/p&gt;
&lt;p&gt;Which presents us with a seeming conundrum. Clearly, humans and many animals exhibit behavior which is “intentional,” and we and other species evolved this capability over a broad span of time (given its taxonomic breadth) by natural selection acting upon variation generated by various directed and undirected mechanisms. But equally clearly, intentional behavior cannot be a “shortcut” around the two step process of variation and selection, since like all other variation, our intentions and strategic planning are causally prior to the outcome of their expression, and often long prior to their downstream affects on our reproductive success, survival, and our ability to spread our ideas and cultural norms.&lt;/p&gt;
&lt;p&gt;In his recent article, David Sloan Wilson &lt;span class=&quot;citation&quot; data-cites=&quot;Wilson:2016bc&quot;&gt;(Wilson 2016)&lt;/span&gt; discussed a number of mechanisms by which intentional behavior has probably evolved in humans and other lineages. The adaptive nature of the vertebrate immune system, for example, is a paradigm example of an adaptive, open-ended process; it is, in fact, a selection process within a selection process. Wilson also describes operant conditioning and explicit decision-making.&lt;/p&gt;
&lt;p&gt;It is worth trying to give a general account of such mechanisms, however, because it may be possible to unify many kinds of “directed” variation mechanisms, including those involved in intentional behavior, and understand what they share in common. The natural framework for unifying such examples is statistical learning theory, which attempts to describe a generalized framework whereby accurate models can be inferred by exposure (in some fashion) to data &lt;span class=&quot;citation&quot; data-cites=&quot;kearns1994introduction&quot;&gt;(M. J. Kearns and Vazirani 1994)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is becoming somewhat fashionable to make connections between learning theory and directed mechanisms in evolution (for example, see &lt;span class=&quot;citation&quot; data-cites=&quot;Power:2015cc&quot;&gt;(Power et al. 2015)&lt;/span&gt; ) after the recent book by Leslie Valiant, one of the founders of formalized statistical learning theory &lt;span class=&quot;citation&quot; data-cites=&quot;valiant2013probably&quot;&gt;(Valiant 2013)&lt;/span&gt;. Valiant’s model, PAC learning, provides a broad guarantee that we can build a statistical model capable of discriminating instances of a target distribution (or “concept” in machine learning). The nature of that guarantee is that, with enough exposure to training samples, we can select a hypothesis with low generalization error (the “approximately correct” part), with high probability (the “probably” part). PAC learning formally underlies many, but not all, “supervised” learning methods in statistics and machine learning, including much regression modeling and various classification and pattern recognition methods.&lt;/p&gt;
&lt;h3 id=&quot;intentional-behavior-and-learning-theories&quot;&gt;Intentional Behavior and Learning Theories&lt;/h3&gt;
&lt;p&gt;But PAC learning is not a “universal” learning theory, as Valiant notes. The basic PAC learning model applies to situations where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The learner receives access to data, in the form of measurements of some number of features or covariates, and a “label” indicating to which model or target class that set of covariates belongs&lt;/li&gt;
&lt;li&gt;The label given can be taken as accurate, and not associated with noise or error&lt;/li&gt;
&lt;li&gt;The learner does not direct the generation of the data, but accepts labeled examples as given&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As Valiant describes in his book, this kind of process doesn’t directly underlie most examples in genetic evolution. PAC learning, self-evidently, does underlie some types of cultural learning, since of course it (and the statistical algorithms that implement it) are cultural constructions by humans, to aid in understanding complex aspects of their environment.&lt;/p&gt;
&lt;p&gt;But the framework is too restrictive to cover all learning in cultural contexts, and thus most of the ways in which humans formulate intentions for action on the basis of information gathered. In fact, each of the restrictions above can be relaxed, and in doing so, result in different learning models.&lt;/p&gt;
&lt;p&gt;In discussing the relation between learning theory and biological evolution, Valiant correctly focuses upon relaxing the first requirement: that the learner see the detailed data. One way to frame biological fitness within learning theory is to treat classes of genomes as “queries” that populations make into the environment, which responds with &lt;strong&gt;summary&lt;/strong&gt; data: average length of survival, average number of offspring for individuals with that class of genome. The population evolves by aggregating this feedback in the form of differential persistence of seemingly successful phenotypes.&lt;/p&gt;
&lt;p&gt;The leaning framework just described is a modification of PAC learning by Kearns called “statistical query learning” &lt;span class=&quot;citation&quot; data-cites=&quot;Kearns:1998:ENL:293347.293351&quot;&gt;(M. Kearns 1998)&lt;/span&gt;, and while the interpretation of fitness as statistical queries against the environment might sound like a bit of a stretch, there are many behavioral contexts which fit such a model nicely. Individuals learning a skill, for example, might make attempts and observe the results, and modify their next attempt accordingly. Rarely will individuals have detailed information about the various contributing factors leading to the outcomes, especially in a complex activity such as hunting or making stone tools. Moreover, the same actions and tools may lead to differing outcomes on different trials, leading to only summary information about the outcome of an action or tool on average.&lt;/p&gt;
&lt;p&gt;But humans do more than simply learn by aggregating data; we can guide the process of data collection and learning to improve performance. In situations where we can make trials, and then consult an “expert” for feedback, we can tune our next trial based upon the feedback, and repeat the loop. Much of human learning and the entire “apprenticeship” model for learning complex skills is based on this kind of model. In formal terms, this is “active learning,” which is the subject of quite active study within machine learning and statistics &lt;span class=&quot;citation&quot; data-cites=&quot;Jamieson:2015vp&quot;&gt;(Jamieson, Jain, and Fernandez 2015)&lt;/span&gt;. Of particular interest is a recent NIPS conference paper which studied active learning when learners have access to both “strong” and “weak” labelers. A strong labeler is very accurate at providing feedback, but expensive to consult, weak labelers are cheap to consult but are occasionally inaccurate. Examples of each in a human context might be asking the master craftsman for feedback rather than students slightly more advanced than oneself, or hiring a skilled attorney rather than Googling for an answer. Zhang and colleagues find, of course, that there are situations where consulting a mix of strong and weak experts can result in highly accurate learning &lt;span class=&quot;citation&quot; data-cites=&quot;Zhang:2015wz&quot;&gt;(Zhang and Chaudhuri 2015)&lt;/span&gt;. This should be unsurprising, because the essential features of multiple-oracle active learning are in play in most human learning environments (e.g., schools, medical residency programs, craft apprenticeships, legal associate programs, and so on).&lt;/p&gt;
&lt;p&gt;Furthermore, learning models must account for structured information. The learning task is rarely simply to recognize or predict a single type of distribution or concept, but instead is multi-stage, with stages building upon one another. Real knowledge has prerequisites, and structure, and we can easily fail to learn a skill if we do not yet have a solid grounding in the information and skills that come before. This will have marked effects on our cultural transmission models, and require close collaboration with learning theorists, but should yield much richer analyses of technological change in particular &lt;span class=&quot;citation&quot; data-cites=&quot;Madsen2015&quot;&gt;(&lt;span class=&quot;citeproc-not-found&quot; data-reference-id=&quot;Madsen2015&quot;&gt;&lt;strong&gt;???&lt;/strong&gt;&lt;/span&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, only occasionally do we learn in a focused way where we are regularly getting labeled feedback, from whatever source. Much of our learning about the world comes in a combination of data points, some of which come with feedback, and much of which doesn’t. Such situations fall within so-called “semi-supervised” learning. Or, we get batched feedback, where we get a single evaluation for a number of different trials which may vary in subtle ways &lt;span class=&quot;citation&quot; data-cites=&quot;Settles:2008wh&quot;&gt;(Settles, Craven, and Ray 2008)&lt;/span&gt;. Finally, we often learn not by exploring an entire space of possibilities, but by actively looking for the most “informative” or representative portions of that space of examples &lt;span class=&quot;citation&quot; data-cites=&quot;Huang:2010uj&quot;&gt;(Huang, Jin, and Zhou 2010)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;
&lt;p&gt;The basic point is that many variations on statistical learning models will be applicable in understanding how humans learn, both from the environment and by cultural transmission and teaching. Note, however, that in none of the models does the learner understand the ultimate consequences of incremental steps. Even in active learning models where the learner can take past knowledge to query for specific and informative data samples to guide future action, the progress in accuracy is local and stochastic; overfitting is still a concern and it is still impossible to know ahead of time what the ultimate “generalization error” of one’s strategies will be.&lt;/p&gt;
&lt;p&gt;Cultural evolution is, undoubtedly, a mixture of intentional and unintentional processes, of unthinkingly copying someone else but also deeply studying carefully chosen mentors and experts. There is room in our theories of cultural evolution for pure diffusion processes that look very much like epidemiological or simple population genetic models, and processes that draw deeply from cognitive science, childhood development, and rich ethnography for their details. There is even room for the subtle combinatorial cognitive processes that lead to “creativity” and true invention.&lt;/p&gt;
&lt;p&gt;All of these, and more, are understandable without exiting the Darwinian paradigm, and various theories of statistical learning promise to play a large role in extending Darwinian evolution to those intentional processes. But intentionality, like creativity, are not a sign that natural selection cannot act on culture, or that culture is not Darwinian. The latter is a continuing misconception that largely stems from overinterpreting what “random variation” means in the evolutionary context.&lt;/p&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-Eerkens2005&quot;&gt;
&lt;p&gt;Eerkens, J.W., and C.P. Lipo. 2005. “Cultural Transmission, Copying Errors, and the Generation of Variation in Material Culture and the Archaeological Record.” &lt;em&gt;Journal of Anthropological Archaeology&lt;/em&gt; 24 (4). Elsevier: 316–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Gabora2013a&quot;&gt;
&lt;p&gt;Gabora, Liane. 2013. “An Evolutionary Framework for Cultural Change: Selectionism Versus Communal Exchange.” &lt;em&gt;Physics of Life Reviews&lt;/em&gt; 10 (2): 117–45. doi:&lt;a href=&quot;https://doi.org/10.1016/j.plrev.2013.03.006&quot;&gt;10.1016/j.plrev.2013.03.006&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Huang:2010uj&quot;&gt;
&lt;p&gt;Huang, S J, R Jin, and Z H Zhou. 2010. “Active learning by querying informative and representative examples.” &lt;em&gt;Advances in Neural Information …&lt;/em&gt;. &lt;a href=&quot;http://papers.nips.cc/paper/4176-active-learning-by-querying-informative-and-representative-examples&quot; class=&quot;uri&quot;&gt;http://papers.nips.cc/paper/4176-active-learning-by-querying-informative-and-representative-examples&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Jamieson:2015vp&quot;&gt;
&lt;p&gt;Jamieson, K G, L Jain, and C Fernandez. 2015. “NEXT: A System for Real-World Development, Evaluation, and Application of Active Learning.” &lt;em&gt;Advances in Neural …&lt;/em&gt;. &lt;a href=&quot;http://papers.nips.cc/paper/5868-next-a-system-for-real-world-development-evaluation-and-application-of-active-learning&quot; class=&quot;uri&quot;&gt;http://papers.nips.cc/paper/5868-next-a-system-for-real-world-development-evaluation-and-application-of-active-learning&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Kearns:1998:ENL:293347.293351&quot;&gt;
&lt;p&gt;Kearns, Michael. 1998. “Efficient Noise-Tolerant Learning from Statistical Queries.” &lt;em&gt;J. ACM&lt;/em&gt; 45 (6). New York, NY, USA: ACM: 983–1006. doi:&lt;a href=&quot;https://doi.org/10.1145/293347.293351&quot;&gt;10.1145/293347.293351&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-kearns1994introduction&quot;&gt;
&lt;p&gt;Kearns, Michael J, and Umesh Virkumar Vazirani. 1994. &lt;em&gt;An Introduction to Computational Learning Theory&lt;/em&gt;. MIT press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Madsen2013a&quot;&gt;
&lt;p&gt;Madsen, Mark E., and Carl P. Lipo. 2013. “Saving Culture from Selection: Comment on an Evolutionary Framework for Cultural Change: Selectionism Versus Communal Exchange, by L. Gabora.” &lt;em&gt;Physics of Life Reviews&lt;/em&gt; 10 (2): 149–50. doi:&lt;a href=&quot;https://doi.org/10.1016/j.plrev.2013.03.008&quot;&gt;10.1016/j.plrev.2013.03.008&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Power:2015cc&quot;&gt;
&lt;p&gt;Power, Daniel A, Richard A Watson, E rs Szathm ry, Rob Mills, Simon T Powers, C Patrick Doncaster, and Blazej Czapp. 2015. “What can ecosystems learn? Expanding evolutionary ecology with learning theory.” &lt;em&gt;Biology Direct&lt;/em&gt;, December. Biology Direct, 1–24. doi:&lt;a href=&quot;https://doi.org/10.1186/s13062-015-0094-1&quot;&gt;10.1186/s13062-015-0094-1&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Settles:2008wh&quot;&gt;
&lt;p&gt;Settles, B, M Craven, and S Ray. 2008. “Multiple-instance active learning.” &lt;em&gt;Advances in Neural Information …&lt;/em&gt;. &lt;a href=&quot;http://papers.nips.cc/paper/3252-multiple-instance-active-learning&quot; class=&quot;uri&quot;&gt;http://papers.nips.cc/paper/3252-multiple-instance-active-learning&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-valiant2013probably&quot;&gt;
&lt;p&gt;Valiant, Leslie. 2013. &lt;em&gt;Probably Approximately Correct: Nature’s Algorithms for Learning and Prospering in a Complex World&lt;/em&gt;. Basic Books.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Wilson:2016bc&quot;&gt;
&lt;p&gt;Wilson, David Sloan. 2016. “Intentional cultural change.” &lt;em&gt;Current Opinion in Psychology&lt;/em&gt; 8 (April): 190–93. doi:&lt;a href=&quot;https://doi.org/10.1016/j.copsyc.2015.12.012&quot;&gt;10.1016/j.copsyc.2015.12.012&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Zhang:2015wz&quot;&gt;
&lt;p&gt;Zhang, C, and K Chaudhuri. 2015. “Active learning from weak and strong labelers.” &lt;em&gt;Advances in Neural Information Processing …&lt;/em&gt;. &lt;a href=&quot;http://papers.nips.cc/paper/5988-active-learning-from-weak-and-strong-labelers&quot; class=&quot;uri&quot;&gt;http://papers.nips.cc/paper/5988-active-learning-from-weak-and-strong-labelers&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</content><category term="intentionality" /><category term="cultural transmission" /><category term="PAC" /><category term="learning theory" /><category term="statistical query learning" /><summary>(the following is a continuation and completion of a post begun in 2013)</summary></entry><entry><title>Limits of model resolution for seriation classification</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/22/equifinality-model-variants-seriation-classification.html" rel="alternate" type="text/html" title="Limits of model resolution for seriation classification" /><published>2016-02-22T00:00:00-08:00</published><updated>2016-02-22T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/22/equifinality-model-variants-seriation-classification</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/22/equifinality-model-variants-seriation-classification.html">&lt;h3 id=&quot;model-resolution-and-equifinality&quot;&gt;Model Resolution and Equifinality&lt;/h3&gt;
&lt;p&gt;Experiment &lt;code&gt;sc-2&lt;/code&gt; was designed to examine the opposite question as &lt;code&gt;sc-1&lt;/code&gt;; that is, when do we lose the ability to distinguish between regional interaction models by examining the structure of seriations from cultural traits transmitted through those interaction networks? This is a question of equifinality of models: do different models have empirical consequences which are indistinguishable given a particular observation technique?&lt;/p&gt;
&lt;p&gt;To test this, I set up four models which I believe to be very “close” to each other:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Lineage splitting where 1 lineage turns into 2 lineages, the split occurring 30% of the way through the time sequence (“early split”)&lt;/li&gt;
&lt;li&gt;Lineage splitting where 1 &lt;span class=&quot;math inline&quot;&gt;\(\rightarrow\)&lt;/span&gt; 2 lineages, split occurring 70% of the way through the time sequence (“late split”)&lt;/li&gt;
&lt;li&gt;Lineage coalescence where 2 lineages turn into a single linage, the event occurring 30% of the way through the sequence (“early coalescence”)&lt;/li&gt;
&lt;li&gt;Lineage coalescence where 2 &lt;span class=&quot;math inline&quot;&gt;\(\rightarrow\)&lt;/span&gt; 1 lineages, split occuring 70% of the way through the time sequence (“late coalescence”)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In all other respects, simulation of cultural transmission across these regional networks was identical, using the same prior distributions for innovation and migration rates, population sizes, and so on.&lt;/p&gt;
&lt;p&gt;My expectation going in was that the lineage splitting and coalescence models should generate seriations which are almost indistinguishable from one another, except for their temporal orientation, and with the paired early/late comparisons, it may be difficult to tell any of these models from one another without additional feature information. In particular, I expected roughly chance performance on classification unless we could provide temporal orientation, and even then, we may only be able to tell coalescence from splitting models.&lt;/p&gt;
&lt;h3 id=&quot;initial-sc-2-analysis&quot;&gt;Initial SC-2 Analysis&lt;/h3&gt;
&lt;p&gt;The analysis of &lt;code&gt;sc-2&lt;/code&gt; followed the method used in the &lt;a href=&quot;http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/16/feature-engineering-seriation-classification.html&quot;&gt;second trial of &lt;code&gt;sc-1&lt;/code&gt;&lt;/a&gt;, calculating the Laplacian eigenvalue spectrum of the final seriation solution graphs (specifically, the &lt;code&gt;minmax-by-weight&lt;/code&gt; solutions for continuity seriation), and using the sorted eigenvalues as features for a gradient boosted classifier.&lt;/p&gt;
&lt;p&gt;The initial results seem indicative of real trouble telling these models apart. For guidance, class labels are as follows:&lt;/p&gt;
&lt;ol start=&quot;0&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Early splitting&lt;/li&gt;
&lt;li&gt;Early coalescence&lt;/li&gt;
&lt;li&gt;Late split&lt;/li&gt;
&lt;li&gt;Late coalescence&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given a hold-out test set, we see the following performance:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
          predicted 0  predicted 1  predicted 2  predicted 3
actual 0            3            2            0            0
actual 1            1            1            0            0
actual 2            0            0            2            2
actual 3            0            0            4            5
             precision    recall  f1-score   support

          0       0.75      0.60      0.67         5
          1       0.33      0.50      0.40         2
          2       0.33      0.50      0.40         4
          3       0.71      0.56      0.63         9

avg / total       0.61      0.55      0.57        20

Accuracy on test: 0.550&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The overall accuracy is low, but the pattern of misclassifications is key here. We never see “early” models misclassified as “late” models, but we do see splitting misclassified as coalescence (possibly because we have no orienting information). So while overall accuracy is low, we actually have perfect discrimination along one dimension of the models: when events that alter lineage structure occur, in relative terms. Not bad, considering how “close” in structure these regional interaction models are.&lt;/p&gt;
&lt;h3 id=&quot;optimizing-classification-performance&quot;&gt;Optimizing Classification Performance&lt;/h3&gt;
&lt;p&gt;The above was conducted with “reasonable” hyperparameters for the gradient boosted classifier, but I want to understand our best performance in separating these models. This is accomplished by setting the hyperparameters through cross-validation. In this case, I used a grid search of the following variables and parameters for the learning rate penalty, and the number of boosting rounds (number of estimators):&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;     &lt;span class=&quot;co&quot;&gt;&amp;#39;clf__learning_rate&amp;#39;&lt;/span&gt;: [&lt;span class=&quot;fl&quot;&gt;5.0&lt;/span&gt;,&lt;span class=&quot;fl&quot;&gt;2.0&lt;/span&gt;,&lt;span class=&quot;fl&quot;&gt;1.0&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.75&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.5&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.25&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.1&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.05&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.01&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.005&lt;/span&gt;],
     &lt;span class=&quot;co&quot;&gt;&amp;#39;clf__n_estimators&amp;#39;&lt;/span&gt;: [&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;25&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;50&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;250&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;500&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using 5-fold cross validation, this produced 350 different fits of the classifer, with the following results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Best score: 0.593
Best parameters:
param: clf__learning_rate: 1.0
param: clf__n_estimators: 50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some improvement is seen on overall training set accuracy, but the real surprise is test performance on the hold-out data, using the optimal hyperparameters:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;          predicted 0  predicted 1  predicted 2  predicted 3
actual 0            3            2            0            0
actual 1            1            1            0            0
actual 2            0            0            3            1
actual 3            0            0            3            6
             precision    recall  f1-score   support

          0       0.75      0.60      0.67         5
          1       0.33      0.50      0.40         2
          2       0.50      0.75      0.60         4
          3       0.86      0.67      0.75         9

avg / total       0.71      0.65      0.66        20

Accuracy on test: 0.650&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Overall accuracy is greatly improved, which is unusual (normally I would expect test accuracy to be less than training accuracy, but the test set is small). But we can see that we improved mainly because of our ability to predict classes 2 and 3, although the overall pattern is still the same: we have misclassification within early and within late, but perfect discrimination between the two.&lt;/p&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;Given how close these models were, I expected to have great trouble in identifying them from seriations. What I found is that I can identify part of the model class with great accuracy, and that the other modeling dimension (coalescence/splitting) with much less accuracy. This leads me to suspect that I could predict both dimensions with very high accuracy if I were to find a way to encode temporal orientation as a feature.&lt;/p&gt;
&lt;p&gt;I will pursue this, since we often have at least &lt;strong&gt;some&lt;/strong&gt; information on temporal orientation, perhaps by knowing that one assemblage in a set is much earlier or later than the rest. The challenge is finding a way to provide this kind of hint for the synthetic seriation graphs. More soon on this.&lt;/p&gt;
&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/gist/anonymous/f6a18712ee1077d1a329&quot;&gt;Full analysis notebook&lt;/a&gt; on NBViewer, from the Github repository.&lt;/p&gt;
&lt;p&gt;Github Repository: &lt;a href=&quot;https://github.com/mmadsen/experiment-seriation-classification&quot;&gt;experiment-seriation-classification&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><summary>Model Resolution and Equifinality</summary></entry><entry><title>Feature Engineering for Seriation Classification</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/16/feature-engineering-seriation-classification.html" rel="alternate" type="text/html" title="Feature Engineering for Seriation Classification" /><published>2016-02-16T00:00:00-08:00</published><updated>2016-02-16T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/16/feature-engineering-seriation-classification</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/16/feature-engineering-seriation-classification.html">&lt;h3 id=&quot;feature-engineering&quot;&gt;Feature Engineering&lt;/h3&gt;
&lt;p&gt;In my &lt;a href=&quot;http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/14/seriation-classification-experiment.html&quot;&gt;previous note&lt;/a&gt;, I used the graph spectral distance (i.e., the euclidean distance between Laplacian eigenvalue spectra from two seriation solutions) in a kNN classifer to predict which regional network model generated a seriation graph. This achieved accuracy around 80% with 3 nearest neighbors.&lt;/p&gt;
&lt;p&gt;Doing better meant changing approaches, and giving the classifier a larger space within which to draw decision boundaries. My first thought was to not reduce the Laplacian spectrum to distances, but instead of use the spectra themselves as numeric features. This would require that, say, column 1 represented the largest eigenvalue in each graph’s spectrum, column 2 the second largest, etc, which is easily accomplished.&lt;/p&gt;
&lt;p&gt;The resulting feature matrix is then suitable for any classifier algorithm. I chose gradient boosted trees because of their high accuracy (essentially equivalent to random forests or better in most applications), and without any hyperparameter tuning at all, achieve anywhere from 85% to 100% accuracy depending upon the train/test split (it’s a small sample size). Optimizing hyperparameters improves this and I can get 100% pretty often with different train test splits.&lt;/p&gt;
&lt;p&gt;So this might be the standard method for seriation classification for the moment. The good thing is that it lends itself to direct interpretation as an ABC (approximate Bayesian computation) estimator, as described in &lt;span class=&quot;citation&quot; data-cites=&quot;pudlo2014abc&quot;&gt;(Pudlo et al. 2014)&lt;/span&gt;, especially if I actually use random forests (although I’m not sure the random forest bit is terribly important).&lt;/p&gt;
&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h3&gt;
&lt;p&gt;The following code snippet takes a list of NetworkX graph objects, and returns a Numpy matrix with a chosen number of eigenvalues (it isn’t clear how many are relevant):&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; graphs_to_eigenvalue_matrix(graph_list, num_eigenvalues &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;):
    &lt;span class=&quot;co&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    Given a list of NetworkX graphs, returns a numeric matrix where rows represent graphs, &lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    and columns represent the reverse sorted eigenvalues of the Laplacian matrix for each graph,&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    possibly trimmed to only use the num_eigenvalues largest values.  If num_eigenvalues is &lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    unspecified, all eigenvalues are used.&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;co&quot;&gt;# peek at the first graph and see how many eigenvalues there are&lt;/span&gt;
    tg &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; graph_list[&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;]
    n &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(nx.spectrum.laplacian_spectrum(tg, weight&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;))
    
    &lt;span class=&quot;co&quot;&gt;# we either use all of the eigenvalues, or we use the smaller of&lt;/span&gt;
    &lt;span class=&quot;co&quot;&gt;# the requested number or the actual number (if it is smaller than requested)&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; num_eigenvalues &lt;span class=&quot;op&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;:
        ev_used &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; n
    &lt;span class=&quot;cf&quot;&gt;else&lt;/span&gt;:
        ev_used &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;min&lt;/span&gt;(n, num_eigenvalues)

    &lt;span class=&quot;bu&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;(debug) eigenvalues - test graph: &lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; num_eigenvalues: &lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; ev_used: &lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;%&lt;/span&gt; (n, num_eigenvalues, ev_used)
    
    data_mat &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; np.zeros((&lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(graph_list),ev_used))
    &lt;span class=&quot;co&quot;&gt;#print &amp;quot;data matrix shape: &amp;quot;, data_mat.shape&lt;/span&gt;
    
    &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; ix &lt;span class=&quot;op&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;range&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(graph_list)):
        spectrum &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;sorted&lt;/span&gt;(nx.spectrum.laplacian_spectrum(graph_list[ix], weight&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;), reverse&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;True&lt;/span&gt;)
        data_mat[ix,:] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; spectrum[&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;:ev_used]
        
    &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; data_mat&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/github/mmadsen/experiment-seriation-classification/blob/master/analysis/sc-1-3/sc-1-seriation-feature-engineering.ipynb&quot;&gt;Full analysis notebook&lt;/a&gt; on NBViewer, from the Github repository.&lt;/p&gt;
&lt;p&gt;Github Repository: &lt;a href=&quot;https://github.com/mmadsen/experiment-seriation-classification&quot;&gt;experiment-seriation-classification&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-pudlo2014abc&quot;&gt;
&lt;p&gt;Pudlo, Pierre, Jean-Michel Marin, Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier, and Christian P Robert. 2014. “ABC Model Choice via Random Forests.” &lt;em&gt;ArXiv Preprint ArXiv:1406.6288&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><summary>Feature Engineering</summary></entry><entry><title>Identifying Metapopulation Models from Simulated CT and Seriations</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/14/seriation-classification-experiment.html" rel="alternate" type="text/html" title="Identifying Metapopulation Models from Simulated CT and Seriations" /><published>2016-02-14T00:00:00-08:00</published><updated>2016-02-14T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/14/seriation-classification-experiment</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/14/seriation-classification-experiment.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;In a &lt;a href=&quot;http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2016/01/26/quantifying-similarity-seriations.html&quot;&gt;previous note&lt;/a&gt;, I described the problem of inferring the goodness of fit between a regional network model and the sampled output of cultural transmission on that regional network, as measured through seriations. I am now ready with the simulation and inference code to start testing the spectral similarity metric I discussed in that note across pairs and sets of regional network models.&lt;/p&gt;
&lt;p&gt;Here, I describe the first such comparison.&lt;/p&gt;
&lt;h3 id=&quot;experiment-sc-1&quot;&gt;Experiment: SC-1&lt;/h3&gt;
&lt;p&gt;SC-1 is a simple contrast between two regional network models. A regional network model is a time-transgressive description of the interaction patterns that existed among a set of subpopulations, described by an “interval temporal network” representation (see &lt;a href=&quot;http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2014/11/28/more-temporal-networks-python.html&quot;&gt;note 2&lt;/a&gt; and &lt;a href=&quot;http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2014/07/28/implementing-temporal-networks-in-python.html&quot;&gt;note 1&lt;/a&gt; about the implementation of such models).&lt;/p&gt;
&lt;p&gt;Both models are composed of 10 time slices.&lt;/p&gt;
&lt;p&gt;Model #1 is called “linear” in the experiment directory because it should ideally yield simple, linear seriation solutions because the only thing occurring is sampling through time. At any given time, the metapopulation is composed of 64 subpopulations each. Each subpopulation is fully connected, so that any subpopulation can exchange migrants with any other, and there are no differences in edge weights (and thus migration rates). Each subpopulation in slice &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt; is the child of a random subpopulation in slice &lt;span class=&quot;math inline&quot;&gt;\(N-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Model #2 is called “lineage” in the experiment directory because it features 4 clusters of subpopulations, with 8 subpopulations per cluster. Subpopulations are fully connected within a cluster, with edge weight 10. Subpopulations are connected between subpopulations at fraction 0.1, with edge weight 1. This yields a strong tendency to exchange migrants within clusters, and at a much lower rate between clusters. For the first 4 time slices, all four clusters of subpopulations are interconnected, but in slice 4, a “split” occurs, removing interconnections between two sets of clusters, leaving &lt;span class=&quot;math inline&quot;&gt;\({1,2}\)&lt;/span&gt; interconnected and &lt;span class=&quot;math inline&quot;&gt;\({3,4}\)&lt;/span&gt; interconnected, but no connections between these sets. The resulting clusters then evolve for 6 more time slices on separate trajectories, giving rise to a “lineage split.”&lt;/p&gt;
&lt;p&gt;Neutral cultural transmission is simulated across these two network models, with 50 simulation replicates on each model, and a common set of parameters:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode json&quot;&gt;&lt;code class=&quot;sourceCode json&quot;&gt;&lt;span class=&quot;fu&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;theta_low&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.00001&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;theta_high&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;maxinittraits&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;numloci&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;popsize&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;250&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;simlength&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;8000&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;samplefraction&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;migrationfraction_low&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;migrationfraction_high&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;replicates&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;fu&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Innovation rates and migration fractions are chosen uniformly at random from the ranges given for each simulation run. Each locus evolves randomly, but we track combinations of loci as “classes” in the archaeological sense of the term as our observable variables. Populations evolve for 8000 generations, giving approximately 800 transmission events per individual during a time slice (i.e., step in the regional metapopulation evolution). We can think of that as approximately monthly opportunities for copying artifacts or behavioral traits, over a lifetime of approximately 65 years.&lt;/p&gt;
&lt;p&gt;The raw data from each community in a simulated network model are then aggregated over the duration that community persists, giving us a time averaged picture of the frequency of cultural traits.&lt;/p&gt;
&lt;p&gt;I then perform the following sampling steps:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;I take a sample of each time averaged data collection similar to the size of a typical archaeological surface collection: 500 samples are taken without replacement from each community, with probability proportional to class frequency. This yields a smaller set of classes, since many hundreds or thousands of combinations are only seen once or twice in the simulation data, and thus are very hard to measure from empirical samples.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I then take a sample of communities to seriate. From each of the two network models, I take temporally stratified samples, with 3 communities per time slice sampled out of the 64 present in each slice. In real sampling situations, we would not know how communities break down temporally, but we often do attempt to get samples of assemblages which cover our entire study duration. In the case of Model #1, we take a 5% sample of communities in each time slice. In the case of Model #2, given the different structure of the model, we take a 12% sample of communities in each time slice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Within the overall sample from each simulation run, comprised now of the time averaged class frequencies from 30 sampled communities, I examine the classes/types themselves, and drop any types (columns) which do not have data values for at least 3 communities. This is standard pre-processing for seriation analyses (or was in the Fordian manual days of seriation analysis) since without more than 3 values, a column does not contribute to ordering the communities.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/clipo/idss-seriation&quot;&gt;IDSS Seriation&lt;/a&gt; package is then used to seriate the stratified, filtered data files for each simulation run across the two models.&lt;/p&gt;
&lt;h3 id=&quot;first-classification-attempt&quot;&gt;First Classification Attempt&lt;/h3&gt;
&lt;p&gt;For a first classification attempt, I did a “leave one out” cross validation run, in which each seriation graph was sequentially deleted from the training set of 99 seriations (one lineage graph had issues with duplicate frequencies and became stuck in frequency seriation), and the distance from the hold-out target graph to all others calculated using &lt;a href=&quot;http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2016/01/26/quantifying-similarity-seriations.html&quot;&gt;Laplacian spectral distance&lt;/a&gt;. The label of the target graph was then predicted as the majority vote of the 5 nearest neighboring graphs. No attempt was made to tune the number of nearest neighbors using a second cross validation pass, but that will be the next experiment.&lt;/p&gt;
&lt;p&gt;In general, the ability to predict the label (network model) which gave rise to the target seriation graph is decent: 76.8%.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Classification Report:

          predicted 0  predicted 1
actual 0           41            9
actual 1           14           35
             precision    recall  f1-score   support

          0       0.75      0.82      0.78        50
          1       0.80      0.71      0.75        49

avg / total       0.77      0.77      0.77        99

Accuracy on test: 0.768&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The details of classifier accuracy seem to show that we have better ability to correctly classify seriations which result from Model #1 (“linear”) model than seriations originating from the lineage splitting Model #2. In particular, we correctly identify instances of Model #1 82% of the time (recall), although there are clear issues with false positives. The ability to identify instances of Model #2 is worse, with a considerable number of false negatives.&lt;/p&gt;
&lt;p&gt;In the next experiment, I intend to see if different values of the k-Nearest Neighbor parameter affect this accuracy, but I expect that achieving higher accuracy might require augmenting the approach. One possibility that bears exploration is to not simply use the spectral distance, but to instead use the Laplacian eigenvalues themselves (sorted in decreasing order) directly as features, in addition to other graph theoretic properties such as average degree and tree radius, and use a more traditional classifier like boosted decision trees. That will probably be my third experiment.&lt;/p&gt;
&lt;h3 id=&quot;second-attempt&quot;&gt;Second Attempt&lt;/h3&gt;
&lt;p&gt;In a second run, I examined the effect of the number of nearest neighbors which “vote” on the label of the target seriation, still with leave-one-out cross validation. The results seem to indicate that (at least for these models) that best performance is 3 nearest neighbors, with accuracy worsening for 5, 7, and 9 neighbors, and then plateauing a bit for 11 and 15 neighbors. But with 3 neighbors, we achieve almost 80% accuracy, which is encouraging.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;

knn &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; [&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;11&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;15&lt;/span&gt;]
&lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; nn &lt;span class=&quot;op&quot;&gt;in&lt;/span&gt; knn:
    gclf &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; skm.GraphEigenvalueNearestNeighbors(n_neighbors&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;nn)
    test_pred &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; []
    &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; ix &lt;span class=&quot;op&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;range&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(train_graphs)):
        train_loo_graphs, train_loo_labels, test_graph, test_label &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; leave_one_out_cv(ix, train_graphs, train_labels)
        gclf.fit(train_loo_graphs, train_loo_labels)
        test_pred.append(gclf.predict([test_graph])[&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;])
    &lt;span class=&quot;bu&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Accuracy on test for &lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; neighbors: &lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;%0.3f&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;%&lt;/span&gt; (nn, accuracy_score(train_labels, test_pred)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Accuracy on test for 1 neighbors: 0.788
Accuracy on test for 3 neighbors: 0.798
Accuracy on test for 5 neighbors: 0.768
Accuracy on test for 7 neighbors: 0.747
Accuracy on test for 9 neighbors: 0.758
Accuracy on test for 11 neighbors: 0.788
Accuracy on test for 15 neighbors: 0.788&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;p&gt;The full Github repository for this and related seriation classification experiments is: &lt;a href=&quot;https://github.com/mmadsen/experiment-seriation-classification&quot;&gt;experiment-seriation-classification&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/github/mmadsen/experiment-seriation-classification/blob/master/analysis/sc-1-3/sc-1-seriation-classification-analysis.ipynb&quot;&gt;Full iPython notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/github/mmadsen/experiment-seriation-classification/blob/master/analysis/sc-1-3/sc-1-eigenvalue-classification-dataprep.ipynb&quot;&gt;Data Preparation Notebook&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><summary>Background</summary></entry><entry><title>Loss Functions for ABC Model Selection with Seriation Graphs as Data</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/01/26/quantifying-similarity-seriations.html" rel="alternate" type="text/html" title="Loss Functions for ABC Model Selection with Seriation Graphs as Data" /><published>2016-01-26T00:00:00-08:00</published><updated>2016-01-26T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/01/26/quantifying-similarity-seriations</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/01/26/quantifying-similarity-seriations.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;Traditional archaeological seriation generated solutions that always had a static linear order, either by use of a multidimensional scaling algorithm or via Fordian manual shuffling of assemblages. In such context, departures from a “correct” solution can be measured by the number of “out of order” assemblages (or via a more opaque “stress” statistic in some cases).&lt;/p&gt;
&lt;p&gt;Last year Carl Lipo and I &lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0124942&quot;&gt;introduced a method&lt;/a&gt; for finding sets of classical Fordian seriations from a set of assemblages via an agglomerative iterative procedure &lt;span class=&quot;citation&quot; data-cites=&quot;Lipo2015&quot;&gt;(Lipo 2015)&lt;/span&gt;. We called the resulting seriations “iterative deterministic seriation solutions” or IDSS. IDSS is capable of using any metric for joining assemblages, including classic unimodality, or various distance measures (something we’ll be &lt;a href=&quot;http://notebook.madsenlab.org/essays/2015/09/06/saa2016-abstract.html&quot;&gt;talking about at the SAA meetings this year&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;When IDSS finds groups of data points that seriate together, but do not seriate (within thresholds) with other groups of points, several solutions are created rather than forcing all of the data points into a single scaling or solution (as has been typical practice with statistical seriation by archaeologists in the past). Instead, IDSS outputs all valid solution sets, since the discontinuities between solution sets may indicate lack of cultural interaction, poor recovery sampling of the archaeological record, or other real-world factors.&lt;/p&gt;
&lt;p&gt;Frequently, one assemblage or data point will occur in multiple solutions, and when this occurs IDSS overlays the solutions and forms a tree. From our paper, Figure 1 shows an IDSS frequency seriation solution for archaeological sites in the Central Mississippi River Valley. Each assemblage is represented by decorated ceramic class frequencies, and we can see that there are at least three major solutions which share assemblage 13-O-7, and a number of minor branches.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/idss-fig12-pfg-solution.png&quot; alt=&quot;Figure 1: IDSS seriation solution for PFG sites in Cental Mississippi River Valley&quot; /&gt;&lt;figcaption&gt;Figure 1: IDSS seriation solution for PFG sites in Cental Mississippi River Valley&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;the-problem&quot;&gt;The Problem&lt;/h3&gt;
&lt;p&gt;My current research is aimed at developing ways of treating IDSS seriation solution graphs as data points in machine learning algorithms, with the goal of fitting coarse grained, regional models of cultural transmission and information diffusion to our data. Seriations are the perfect tool for capturing regional-scale social influences that are time-transgressive.&lt;/p&gt;
&lt;p&gt;There are several challenges in this work. The first, which is described in a &lt;a href=&quot;http://localhost:4000/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2014/06/17/seriationct-requirements.html&quot;&gt;previous post&lt;/a&gt; is to create regional metapopulation models with cultural transmission that yield seriations, rather than just type or variant frequencies, as their observable output.&lt;/p&gt;
&lt;p&gt;The second challenge is then to find an inferential framework for model selection which allows one to measure the goodness-of-fit between the theoretical model’s output, and specific empirical seriations like Figure 1.&lt;/p&gt;
&lt;p&gt;This second challenge is perfectly suited to an Approximate Bayesian Computation approach &lt;span class=&quot;citation&quot; data-cites=&quot;Beaumont2002 Csillery2010 Marin2012 Toni2009&quot;&gt;(Beaumont, Zhang, and Balding 2002; Csilléry et al. 2010; Marin et al. 2012; Toni et al. 2009)&lt;/span&gt;, since while we can simulate data from each social learning model, writing down the likelihood function for each model is generally an intractable problem.&lt;/p&gt;
&lt;p&gt;In brief, ABC model selection involves simulating a large number of synthetic data points from each model, calculating summary statistics from those data points, measuring the distance (losses) between summary statistics and observed data, and choosing the model whose losses are the smallest.&lt;/p&gt;
&lt;p&gt;In my current work, the “models” are actually a combination of:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;A social learning model (e.g., unbiased cultural transmission or conformist transmission)&lt;/li&gt;
&lt;li&gt;Parameters for that social learning model (e.g., innovation rates)&lt;/li&gt;
&lt;li&gt;A spatiotemporal population model that describes how communities evolved in a region and what the pattern of community interaction was over time&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a fairly complex “model” to fit, but we can easily simulate samples of cultural variant frequencies across subpopulations from each model (using the &lt;a href=&quot;https://github.com/mmadsen/seriationct&quot;&gt;&lt;code&gt;SeriationCT&lt;/code&gt; software suite&lt;/a&gt;, and then use IDSS to seriate those variant frequencies, yielding simulated versions of Figure 1.&lt;/p&gt;
&lt;p&gt;What we need, then, is a way to measure the distance or loss between Figure 1 and various simulated seriation graphs. Once we have a good loss function, we should be able to perform ABC model selection on seriation graphs.&lt;/p&gt;
&lt;h3 id=&quot;distanceloss-for-unlabelled-unordered-graphs&quot;&gt;Distance/Loss for Unlabelled, Unordered Graphs&lt;/h3&gt;
&lt;p&gt;In general, we want a function which measures the structural similarity of two graphs. There are many approaches to the problem (see &lt;span class=&quot;citation&quot; data-cites=&quot;zager2008graph&quot;&gt;(Zager and Verghese 2008)&lt;/span&gt; for a review). Graph edit distance would be a fairly natural metric, but most algorithms for edit distance rely on matching node identities, and many algorithms also rely upon graphs being ordered as well as labelled.&lt;/p&gt;
&lt;p&gt;The simulated seriations which come out of the &lt;code&gt;SeriationCT&lt;/code&gt; package are unlabelled, and we can’t label them with the identities of the assemblages in our empirical data. So our loss function has to measure structural similarity without reference to node identity, or any notion of ordering or orientation for the graphs.&lt;/p&gt;
&lt;p&gt;This leads fairly directly to using purely algebraic properties of the seriation graphs, and in particular the spectral properties of various matrices associated with the graphs. We know that the eigenvalues of the (possibly binarized) adjacency matrix are related to the edge structure of a graph, and that the eigenvalues of the Laplacian of the adjacency matrix are an even more sensitive indicator of structure &lt;span class=&quot;citation&quot; data-cites=&quot;godsil2001algebraic&quot;&gt;(Godsil and Royle 2001)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a technical report, Koutra et al. &lt;span class=&quot;citation&quot; data-cites=&quot;koutra2011algorithms&quot;&gt;(Koutra et al. 2011)&lt;/span&gt; suggest using the sum of squared differences between the Laplacian spectra of two graphs, trimming the spectra to use only 90% of the total eigenvalue weight. This is attractive from a statistical perspective, essentially giving us the L2 loss between Laplacian spectra for two graphs.&lt;/p&gt;
&lt;p&gt;Given adjacency matrices &lt;span class=&quot;math inline&quot;&gt;\(A_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(A_2\)&lt;/span&gt; for graphs &lt;span class=&quot;math inline&quot;&gt;\(G_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(G_2\)&lt;/span&gt;, the Laplacian matrix is simply &lt;span class=&quot;math inline&quot;&gt;\(L_1 = D_1 - A_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(L_2 = D_2 - A_2\)&lt;/span&gt;, where &lt;span class=&quot;math inline&quot;&gt;\(D_i\)&lt;/span&gt; are the corresponding diagonal matrix of vertex degrees. The spectrum is then the set of eigenvalues &lt;span class=&quot;math inline&quot;&gt;\((\lambda_1 \ldots \lambda_n)\)&lt;/span&gt;. Since the Laplacian is positive semi-definite, all of the eigenvalues in the spectrum are positive real numbers. It is possible to put some bounds on the eigenvalues, and thus predetermine the range of possible loss function values for graphs with a given number of vertices, but I leave that to a future post.&lt;/p&gt;
&lt;p&gt;If we use the full spectrum of values, our L2 spectral loss function is then:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{L}_{sp} = \sum_{i=1}^{n} (\lambda_{1i} - \lambda_{2i})^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and if we only use a trimmed set of eigenvalues (say, those which contribute to the top 90% of the total sum of the spectrum), we sort the list of those &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt; eigenvalues and replace &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt; with &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt; in the previous equation.&lt;/p&gt;
&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;In Python, we can implement this loss function for NetworkX graph objects as follows (now part of the &lt;code&gt;seriationct.analytics&lt;/code&gt; module):&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;
&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; networkx &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; nx
&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; np

&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; graph_spectral_similarity(g1, g2, threshold &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.9&lt;/span&gt;):
    &lt;span class=&quot;co&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    Returns the eigenvector similarity, between [0, 1], for two NetworkX graph objects, as&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    the sum of squared differences between the sets of Laplacian matrix eigenvalues that account&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    for a given fraction of the total sum of the eigenvalues (default = 90%).&lt;/span&gt;

&lt;span class=&quot;co&quot;&gt;    Similarity scores of 0.0 indicate identical graphs (given the adjacency matrix, not necessarily&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    node identity or annotations), and large scores indicate strong dissimilarity.  The statistic is&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    unbounded above.&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    l1 &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; nx.spectrum.laplacian_spectrum(g1, weight&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;)
    l2 &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; nx.spectrum.laplacian_spectrum(g2, weight&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;)
    k1 &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; _get_num_eigenvalues_sum_to_threshold(l1, threshold&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;threshold)
    k2 &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; _get_num_eigenvalues_sum_to_threshold(l2, threshold&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;threshold)
    k &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;min&lt;/span&gt;(k1,k2)
    sim &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;sum&lt;/span&gt;((l1[:k] &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt; l2[:k]) &lt;span class=&quot;op&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;)
    &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; sim


&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; _get_num_eigenvalues_sum_to_threshold(spectrum, threshold &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.9&lt;/span&gt;):
    &lt;span class=&quot;co&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    Given a spectrum of eigenvalues, find the smallest number of eigenvalues (k)&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    such that the sum of the k largest eigenvalues of the spectrum&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    constitutes at least a fraction (threshold, default = 0.9) of the sum of all the eigenvalues.&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; threshold &lt;span class=&quot;op&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;:
        &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(spectrum)

    total &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;sum&lt;/span&gt;(spectrum)
    &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; total &lt;span class=&quot;op&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.0&lt;/span&gt;:
        &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(spectrum)

    spectrum &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;sorted&lt;/span&gt;(spectrum, reverse&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;True&lt;/span&gt;)
    running_total &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.0&lt;/span&gt;

    &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;op&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;range&lt;/span&gt;(&lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(spectrum)):
        running_total &lt;span class=&quot;op&quot;&gt;+=&lt;/span&gt; spectrum[i]
        &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; running_total &lt;span class=&quot;op&quot;&gt;/&lt;/span&gt; total &lt;span class=&quot;op&quot;&gt;&amp;gt;=&lt;/span&gt; threshold:
            &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; i &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;co&quot;&gt;# guard&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(spectrum)


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-Beaumont2002&quot;&gt;
&lt;p&gt;Beaumont, Mark A, W Zhang, and D J Balding. 2002. “Approximate Bayesian computation in population genetics.” &lt;em&gt;Genetics&lt;/em&gt;. &lt;a href=&quot;http://www.genetics.org/content/162/4/2025.short&quot; class=&quot;uri&quot;&gt;http://www.genetics.org/content/162/4/2025.short&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Csillery2010&quot;&gt;
&lt;p&gt;Csilléry, Katalin, Michael G B Blum, Oscar E Gaggiotti, and Olivier François. 2010. “Approximate Bayesian Computation (ABC) in practice.” &lt;em&gt;Trends in Ecology &amp;amp; Evolution&lt;/em&gt; 25 (7): 410–18. doi:&lt;a href=&quot;https://doi.org/10.1016/j.tree.2010.04.001&quot;&gt;10.1016/j.tree.2010.04.001&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-godsil2001algebraic&quot;&gt;
&lt;p&gt;Godsil, Christopher David, and Gordon Royle. 2001. &lt;em&gt;Algebraic Graph Theory&lt;/em&gt;. Vol. 8. Springer New York.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-koutra2011algorithms&quot;&gt;
&lt;p&gt;Koutra, Danai, Ankur Parikh, Aaditya Ramdas, and Jing Xiang. 2011. “Algorithms for Graph Similarity and Subgraph Matching.” Technical Report of Carnegie-Mellon-University.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Lipo2015&quot;&gt;
&lt;p&gt;Lipo, Mark E. AND Dunnell, Carl P. AND Madsen. 2015. “A Theoretically-Sufficient and Computationally-Practical Technique for Deterministic Frequency Seriation.” &lt;em&gt;PLoS ONE&lt;/em&gt; 10 (4). Public Library of Science: e0124942. doi:&lt;a href=&quot;https://doi.org/10.1371/journal.pone.0124942&quot;&gt;10.1371/journal.pone.0124942&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Marin2012&quot;&gt;
&lt;p&gt;Marin, Jean-Michel, Pierre Pudlo, Christian P Robert, and Robin J Ryder. 2012. “Approximate Bayesian Computational Methods.” &lt;em&gt;Statistics and Computing&lt;/em&gt; 22 (6). Springer: 1167–80.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Toni2009&quot;&gt;
&lt;p&gt;Toni, Tina, David Welch, Natalja Strelkowa, Andreas Ipsen, and Michael P H Stumpf. 2009. “Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems.” &lt;em&gt;Journal of Royal Society Interface&lt;/em&gt; 6 (31). The Royal Society: 187–202. doi:&lt;a href=&quot;https://doi.org/10.1098/rsif.2008.0172&quot;&gt;10.1098/rsif.2008.0172&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-zager2008graph&quot;&gt;
&lt;p&gt;Zager, Laura A, and George C Verghese. 2008. “Graph Similarity Scoring and Matching.” &lt;em&gt;Applied Mathematics Letters&lt;/em&gt; 21 (1). Elsevier: 86–94.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><category term="ABC" /><summary>Background</summary></entry><entry><title>Amazon EC2 AMI for deep neural networks and classification problems</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/2016/01/24/neural-network-ami-for-ec2.html" rel="alternate" type="text/html" title="Amazon EC2 AMI for deep neural networks and classification problems" /><published>2016-01-24T00:00:00-08:00</published><updated>2016-01-24T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/2016/01/24/neural-network-ami-for-ec2</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/2016/01/24/neural-network-ami-for-ec2.html">&lt;h3 id=&quot;ami-for-deep-neural-networks&quot;&gt;AMI for Deep Neural Networks&lt;/h3&gt;
&lt;p&gt;Given the increasing quality of libraries for building deep neural network architectures, I’ve been exploring whether DNNs can improve my accuracy in distinguishing between the empirical signatures of cultural transmission models. I started out with Lasagne driving Theano, since Theano itself is pretty nuts and bolts and I occasionally find getting the right tensor shapes a bit baffling. I quickly found Keras, which is another (and very well designed) wrapper around &lt;strong&gt;both&lt;/strong&gt; TensorFlow and Theano, which also allows one to switch between backends. Keras is designed for quick prototyping and research, which is perfect for this context.&lt;/p&gt;
&lt;p&gt;Being able to switch backends also turns out to be fairly handy if you develop on OS X, because if you’ve got Yosemite and the current version of Xcode (7.1), the CLANG compiler toolchain is not compatible with Theano and the CUDA nvcc compiler. Which meant that I’d been developing in an Ubuntu 15.10 virtual machine on my laptop, which was sufficient for testing, and then pushing code to a box with an NVIDIA GPU. By switching the Keras backend to TensorFlow, I can test the main body of my model code on the Mac, and then switch it back to Theano to run on a box with a GPU (since Theano on GPU on Linux is generally thought to be better performing than TensorFlow in general right now).&lt;/p&gt;
&lt;p&gt;I looked around a bit for AMIs to quickly spin up an EC2 instance, and mostly found a lot of broken links to AMIs that weren’t available in my default &lt;code&gt;us-east-1&lt;/code&gt; availability zone. So, I spent a couple of hours today building one with everything I needed.&lt;/p&gt;
&lt;p&gt;The public AMI &lt;code&gt;ami-5cc6e636&lt;/code&gt; is based upon Ubuntu 14.04, with the following added:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Caffe direct from Github bleeding edge&lt;/li&gt;
&lt;li&gt;cuDNN v3.0 from NVIDIA&lt;/li&gt;
&lt;li&gt;CUDA 7.0, with driver built for the Linux 3.13 kernel that Ubuntu 14.04 uses&lt;/li&gt;
&lt;li&gt;Anaconda Python 2.7&lt;/li&gt;
&lt;li&gt;Theano from bleeding edge, which means 0.8 at this point in time&lt;/li&gt;
&lt;li&gt;Lasagne from bleeding edge, which means 0.2 at this point&lt;/li&gt;
&lt;li&gt;Keras, from &lt;code&gt;pip install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;nolearn, from &lt;code&gt;pip install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Seaborn for visualization, from &lt;code&gt;pip install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Graphviz&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The image has a mounted EBS volume for data, and a 32GB root partition since there’s a lot of stuff installed here.&lt;/p&gt;
&lt;p&gt;Fire this AMI up on an &lt;code&gt;g2.2xlarge&lt;/code&gt; instance type for 8 CPU cores, 15GB of RAM, and a GPU with 1,536 CUDA cores. I will try to keep an AMI with these specs up to date, and since the AMI ID will change, check &lt;a href=&quot;http://notebook.madsenlab.org/tag/aws.html&quot;&gt;the tag &lt;code&gt;AWS&lt;/code&gt;&lt;/a&gt; for updates.&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="experiments" /><category term="computing" /><category term="aws" /><category term="ML" /><category term="deep learning" /><category term="HPC" /><category term="classification" /><summary>AMI for Deep Neural Networks</summary></entry><entry><title>Abstract for the 2016 SAA Conference</title><link href="http://notebook.madsenlab.org/essays/2015/09/06/saa2016-abstract.html" rel="alternate" type="text/html" title="Abstract for the 2016 SAA Conference" /><published>2015-09-06T00:00:00-07:00</published><updated>2015-09-06T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/essays/2015/09/06/saa2016-abstract</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/2015/09/06/saa2016-abstract.html">&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Measuring Cultural Relatedness Using Multiple Seriation Ordering Algorithms&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Seriation is a long-standing archaeological method for relative dating that has proven effective in probing regional-scale patterns of inheritance, social networks, and cultural contact in their full spatiotemporal context. The orderings produced by seriation are produced by the continuity of class distributions and unimodality of class frequencies, properties that are related to social learning and transmission models studied by evolutionary archaeologists. Linking seriation to social learning and transmission enables one to consider ordering principles beyond the classic unimodal curve. Unimodality is a highly visible property that can be used to probe and measure the relationships between assemblages, and it was especially useful when seriation was accomplished with simple algorithms and manual effort. With modern algorithms and computing power, multiple ordering principles can be employed to better understand the spatiotemporal relations between assemblages. Ultimately, the expansion of seriation to additional ordering algorithms allows us an ability to more thoroughly explore underlying models of cultural contact, social networks, and modes of social learning. In this paper, we review our progress to date in extending seriation to multiple ordering algorithms, with examples from Eastern North America and Oceania.&lt;/p&gt;
&lt;h3 id=&quot;motivating-notes&quot;&gt;Motivating Notes&lt;/h3&gt;
&lt;p&gt;Archaeological seriation extracts an ordering from a set of class descriptions, usually rendered as type frequencies within assemblages, by reordering those descriptions until each type displays a continuous and unimodal distribution. This combination of criteria dates from the earliest days of seriation, and appears to originate in empirical generalizations concerning both archaeological distributions and the behavior of object “styles” in contemporary populations.&lt;/p&gt;
&lt;p&gt;Those empirical generalizations are easy to tie to the ideas behind cultural transmission and other diffusion models in a broad, descriptive way. It is clear, for example, that in many cases, a new cultural variant is introduced, spreads within a population, increasing in prevalence relative to other variants, and eventually declines in frequency as other, perhaps newer, variants are introduced and themselves grow. But it is also clear to anyone who has studied the behavior of diffusion and transmission models that unimodal growth and decline is far from the &lt;strong&gt;only&lt;/strong&gt; pattern that variants display. Especially in diffusion models with no selection, many classes display complex multimodal distributions.&lt;/p&gt;
&lt;p&gt;Similarly, we expect most cultural variants to display relatively “smooth” distributions, where the prevalence of a trait is similar among points close in space or time, without major discontinuities. We also expect culture-historical “styles” or types to display a single spacetime distribution without recurrence – this is the basis of occurrence seriation, and of course is an unspoken corollary of the unimodal frequency criterion.&lt;/p&gt;
&lt;p&gt;In classical seriation methods, the major ordering principles are unimodality and nonrecurrence (i.e., no holes). Neither of these, however, are the deep ordering principle which allow us to order descriptions into a full spatiotemporal map of a diffusion process. That principle, for underlying processes which are relatively continuous, is given by the “smoothness” principle, which requires that diffusion makes no major “jumps” and thus is continuous in the mathematical, analytic sense of the term.&lt;/p&gt;
&lt;p&gt;We call this the “analytic continuity” principle, and it is constructed in empirical cases by finding the ordering of assemblage descriptions which globally minimize the inter-assemblage distance, where distance is measured by an appropriate vector distance (e.g., cosine or angular distance).&lt;/p&gt;
&lt;p&gt;Why was this principle not employed earlier in archaeological seriation? After all, practitioners like James A. Ford fully understood the spatiotemporal nature of the cultural diffusion process which underlay seriation results (ref to the 1938 diagram). We believe that it was not used in real seriations because there is no way to accomplish such orderings in real cases without significant computational support, and by the time computer support for seriation was available, the nature of the problem had been recast as one of matrix ordering of similiarity coefficients, with significiant changes in the nature of the solutions accepted as a result. The difference is one of detail, but the details are significant.&lt;/p&gt;
&lt;p&gt;The other ordering principles – unimodality and “topological continuity” (or the “no holes” principle) – relate to smoothness continuity in a global/local fashion. The latter provides a global view of diffusion, while the latter allow us to find structure &lt;strong&gt;within&lt;/strong&gt; the field of assemblages, and find groups of assemblages which fit together more than they fit with surrounding groups.&lt;/p&gt;
&lt;p&gt;We propose, therefore, extensions to the seriation method which employ all three ordering principles: analytic continuity, to provide the global map of diffusion across all of the assemblage descriptions in a given data set, and the use of unimodality and non-recurrence (“no holes”) to help probe smaller-scale or “mesoscopic” structure within the data set.&lt;/p&gt;
&lt;p&gt;This redescription of the overall seriation method provides a way to link the method to both to an overall cultural transmission model, as well as a principled way to describe the internal structure of solutions at smaller scales, and is ideally suited to analysis at scales ranging from a few assemblages to large regional surveys. In a future paper, we intend to explore the extension of this method between regional and larger scales by exploring the interface between seriation and cladistics, which we believe extends the mesoscopic scale to macroscopic large scale analysis of cultural relatedness.&lt;/p&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="SAA2016" /><category term="seriation" /><category term="cultural transmission" /><category term="dissertation" /><summary>Abstract</summary></entry><entry><title>IDSS Seriation Software Version 2.3 released</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/2015/09/01/idss-seration-version-2.3-released.html" rel="alternate" type="text/html" title="IDSS Seriation Software Version 2.3 released" /><published>2015-09-01T00:00:00-07:00</published><updated>2015-09-01T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/2015/09/01/idss-seration-version-2.3-released</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/2015/09/01/idss-seration-version-2.3-released.html">&lt;p&gt;Today, Carl Lipo and I released &lt;a href=&quot;https://github.com/clipo/idss-seriation/releases/tag/v2.3&quot;&gt;Version 2.3&lt;/a&gt; of our &lt;a href=&quot;https://github.com/clipo/idss-seriation&quot;&gt;IDSS seriation software&lt;/a&gt;, described in our &lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0124942&quot;&gt;recent PLoS One article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This version contains:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;A bug fix for a triple ordering issue found by &lt;a href=&quot;http://bsc-es.academia.edu/XavierRubio&quot;&gt;Xavier Rubio-Campillo&lt;/a&gt; of the Barcelona Supercomputing Center (thank you, Xavi!). We did not incorporate his full pull request, since we are undecided about some of the changes, but did integrate this fix.&lt;/li&gt;
&lt;li&gt;A performance rewrite of the spatial/geographic significance bootstrap test, which improved the average runtime of this test about 4.5x (which helps with larger numbers of assemblages).&lt;/li&gt;
&lt;li&gt;Every run now gets a UUID and is associated with the release tag and Github commit identifier, to allow results to be tracked to specific software commits, not just periodic numbered releases.&lt;/li&gt;
&lt;li&gt;UUID and software commit identifiers are written to a “metadata” file in the output directory, and to the database if you use the MongoDB driver script.&lt;/li&gt;
&lt;li&gt;The pickled temporary directory (used for parallel processing) is now written in /tmp for convenience, and unless you specify –preservepickle 1, it is cleaned up at the end of the seriation run. This yields a cleaner setup for batch processing.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The easiest way to try out the software is to grab the source for the PLoS One article, which contains the data and a Makefile for regenerating our seriation output:&lt;/p&gt;
&lt;pre class=&quot;shell&quot;&gt;&lt;code&gt;$ git clone git@github.com:clipo/idss-seriation.git
...ignoring lines...
$ cd idss-seriation
$ python setup.py install
$ cd ..
$ git clone git@github.com:mmadsen/lipomadsen2015-idss-seriation-paper.git
$ cd lipomadsen2015-idss-seriation-paper/seriation-analysis
$ make&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see output that looks like this:&lt;/p&gt;
&lt;pre class=&quot;shell&quot;&gt;&lt;code&gt;IDSS seriation Version 2.3
2015-09-01 14:56:37,451 INFO: seriation run identifier: urn:uuid:5183fa22-50f4-11e5-959b-b8f6b1154c9b
Bootstrap CI calculation using 1000 samples - elapsed time: 0.037674 sec
Time elapsed for frequency seriation processing: 28 seconds
Seriation complete.
Maximum size of seriation: 3
Number of frequency seriation solutions at last step: 1139
Assemblages not part of final solution:
*** All assemblages used in seriations.***
Time elapsed for completion of program: 83 seconds&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and several graph windows will pop up. The “minmax” graph shows the best solution with branching to show how different partial seriations relate. There will be an “output” directory with full output.&lt;/p&gt;
&lt;p&gt;Please contact me or Carl Lipo if you have questions about IDSS and this release!&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="seriation" /><summary>Today, Carl Lipo and I released Version 2.3 of our IDSS seriation software, described in our recent PLoS One article.</summary></entry><entry><title>Improving Simulation Performance for CT Models with C++ and OpenMP</title><link href="http://notebook.madsenlab.org/essays/2015/08/24/simulation-performance-explorations.html" rel="alternate" type="text/html" title="Improving Simulation Performance for CT Models with C++ and OpenMP" /><published>2015-08-24T00:00:00-07:00</published><updated>2015-08-24T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/essays/2015/08/24/simulation-performance-explorations</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/2015/08/24/simulation-performance-explorations.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;A side project this summer has been to improve the performance of my basic simulation model codes, so I can increase my experiment “velocity” and do more analyses with larger sample and/or population sizes. Since getting a Xeon/Phi workstation (the machine I &lt;a href=&quot;/essays/2015/07/07/new-directions-ct-computing.html&quot;&gt;described a few months ago&lt;/a&gt;), I’ve been retraining on C++11 for situations which call for mixed Python/C++ performance, or for all C++ simulations when I can’t figure out a good offload or parallelism model for Python code. At the moment I’m using a pure C++ simulation of the Wright-Fisher neutral model, with infinite-alleles mutation on several loci for my testing. The &lt;a href=&quot;https://github.com/mmadsen/neutral-model-cpp&quot;&gt;simulation code is available here&lt;/a&gt; with Makefiles for several compilers, including clang (i.e., modern OS X), GCC 5.2 (which has first-gen support for compiling offload for the Xeon Phi coprocessors), and the Intel C/C++ compiler. (Mental note, I should include a plain vanilla Makefile which would use which ever “GCC” is present…).&lt;/p&gt;
&lt;p&gt;These notes describe what I’ve found so far about improving performance in this code.&lt;/p&gt;
&lt;h3 id=&quot;implementation-notes&quot;&gt;Implementation Notes&lt;/h3&gt;
&lt;p&gt;The simulation model is (hopefully) designed to allow for both parallelism across cores/threads, and for use of the Intel Xeon’s native (and considerable) vectorization infrastructure.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;The “population” is represented by an array of size numloci columns by popsize rows, with integers representing trait ID’s. This block is allocated a single linear block, and at least on the Intel compiler is 64-byte aligned to maximize use of the wide vectorization units (and the even wider ones on the Phi).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This creates “unit stride” access for many common operations, like calculating trait counts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complex operations like calling the C++11 random number library are extracted from nested loops and done separately, to allow the results of random number generation to be used in loops which are then parallelized AND vectorized.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;OpenMP is used to parallelize nested loops for actual trait transmission.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Explicit vectorization via &lt;code&gt;#pragma SIMD&lt;/code&gt; is forced in a couple of places where the compiler cannot necessarily figure out the loop count easily.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;test-setup&quot;&gt;Test Setup&lt;/h3&gt;
&lt;p&gt;The resulting program, &lt;code&gt;neutral-test&lt;/code&gt;, is called as follows to generate timing and performance information:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;export OMP_NUM_THREADS=4; time ./neutral-test --popsize 10000 --numloci 5 --inittraits 6 --innovrate 0.0001 --simlength 25000 --debug 1 --ruletype wfia&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The population size is large enough to generate opportunities both for vectorization and effective use of OpenMP parallel for loops, and the simulation length is long enough to create stable timing.&lt;/p&gt;
&lt;h3 id=&quot;no-optimizations&quot;&gt;No Optimizations&lt;/h3&gt;
&lt;p&gt;If we turn off all optimizations (using &lt;code&gt;-O0 -no-vec&lt;/code&gt; and not passing &lt;code&gt;-openmp&lt;/code&gt; to the Intel compiler), we get the straight line, single core, un-vectorized performance:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;real    0m42.556s
user    0m42.176s
sys     0m0.178s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the vast majority of time is spent in user code, mostly in tight loops, or in the C++11 library (which isn’t kernel code), and that kernel code occupies a tiny fraction of execution time (mostly some I/O at the beginning and end since I have logging turned way down). Real time and user time is basically the same, showing that we are totally single-threaded.&lt;/p&gt;
&lt;h3 id=&quot;vectorization-without-parallelism&quot;&gt;Vectorization Without Parallelism&lt;/h3&gt;
&lt;p&gt;If we turn off OpenMP parallelism, but enable vectorization and aggressive compiler optimization (&lt;code&gt;-O3&lt;/code&gt;), we see dramatic improvement:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;real    0m11.765s
user    0m11.666s
sys     0m0.045s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a speedup of 3.62x, simply by allowing the compiler to vectorize code in a situation where memory access has been designed for easy vectorization (i.e., unit stride access).&lt;/p&gt;
&lt;h3 id=&quot;parallelism-without-vectorization&quot;&gt;Parallelism Without Vectorization&lt;/h3&gt;
&lt;p&gt;If we enable OpenMP parallelism but turn off vectorization, we allow the use of multiple cores for copying tasks, although by the nature of this code base not all loops should be handled in parallel. The code is compiled with &lt;code&gt;-O3 -openmp -no-vec&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;real    0m10.370s
user    0m39.686s
sys     0m0.918s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we get 4.1x speedup, and we can see the effect of parallelism, where the user portion of code executes for 3.9x the wall clock time. So there is real benefit from parallelism here, and I’m exploring whether more of the loops can be parallelized.&lt;/p&gt;
&lt;h3 id=&quot;parallelism-and-vectorization&quot;&gt;Parallelism and Vectorization&lt;/h3&gt;
&lt;p&gt;Finally, I enable everything (&lt;code&gt;-O3 -openmp&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;real    0m10.211s
user    0m39.313s
sys     0m0.811s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The improvement with both is tiny over either approach alone. But this may be due to the small problem size.&lt;/p&gt;
&lt;h3 id=&quot;population-size-100000&quot;&gt;Population Size 100,000&lt;/h3&gt;
&lt;p&gt;If we switch to a larger population size, here are the results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;No optimization - 6m38.924s

Vec w/o Parallelism  - 2m4.405s

Parallel w/o Vec   -   1m54.437s

Vec and Parallelism  - 1m36.847s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see the separate contributions of vectorization and parallelism. The overall speedup is constant at 4.1x for both parallelism and vectorization, but this speedup is only achieved by having both. With the larger problem size, vectorization alone achieves a 3.2x speedup, and parallelism alone about 3.5x. As problem size grows, the importance of combining both approaches only grows (an intermediate problem size of 50K population size has intermediate results, for example).&lt;/p&gt;
&lt;p&gt;The biggest challenge in optimizing this kind of simulation code, it seems, is handling random number generation, which now seems to be the major hotspot. It is also complex enough that including a random generator call in a loop is sufficient to disqualify the loop from vectorization. It is also not clear whether it is safe to use the C++11 random library from multiple threads without locking, and introducing explicit thread-local generators obviates some of the benefit of having OpenMP automatically do thread management and decide the optimal threading strategy.&lt;/p&gt;
&lt;p&gt;The obvious way around this is to generate pools of random variates before going into a loop nest, so that the random variates are simply an array access like any other and thus can be used in vectorized, OpenMP parallelized loops. The current code reflects this strategy for determining the individuals who will be cultural models (targets of copying) in each generation.&lt;/p&gt;
&lt;h3 id=&quot;parallelized-random-number-generation&quot;&gt;Parallelized Random Number Generation&lt;/h3&gt;
&lt;p&gt;Creating 100K random variates is still a fairly time-consuming step, and this may be an area where using a GPU (if available) is a good idea, since at least CUDA (if present) has a high performance random number library. I hate to have hardware-specific code, though, especially if Apple moves away from NVIDIA cards in the Macbook Pros, which makes it hard to develop while mobile or away from home.&lt;/p&gt;
&lt;p&gt;There’s no good reason why I shouldn’t be able to parallelize random number generation using OpenMP, however, as long as each thread has its own private engine variable, which is accomplished by initializing the engine and distributions inside the parallel region, instead of during object construction:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode cpp&quot;&gt;&lt;code class=&quot;sourceCode cpp&quot;&gt;&lt;span class=&quot;ot&quot;&gt;#pragma omp parallel shared(test_par)&lt;/span&gt;
{
        std::random_device rd;
        std::mt19937_64 eng(rd());
        std::uniform_int_distribution&amp;lt;&lt;span class=&quot;dt&quot;&gt;int&lt;/span&gt;&amp;gt; uniform_int{&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, test - &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;};
        num_threads = omp_get_num_threads();

        &lt;span class=&quot;ot&quot;&gt;#pragma omp for&lt;/span&gt;
        &lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt;(i = &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;; i &amp;lt; test; i++) {
                test_par[i] = uniform_int(eng);
        }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The result with 100K individuals and 25K generations (comparable to the above timing experiments) is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;31.24 real       119.92 user         3.02 sys&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is a 12.76x speedup over the serial unvectorized code. I’m not likely to push the performance experiments too much further, since I have a lot of other code to write, but it was a good experiment and now I have code modules to use in future simulations where I need the performance. And, others are encouraged to start with it and adapt it to your own transmission simulations, since it’s known to work and work quickly.&lt;/p&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="cultural transmission" /><category term="simulation" /><category term="computing" /><category term="HPC" /><category term="computational science" /><summary>Background</summary></entry><entry><title>Performance of simuPOP in SeriationCT Across Platforms</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/07/27/simupop-performance-in-seriationct.html" rel="alternate" type="text/html" title="Performance of simuPOP in SeriationCT Across Platforms" /><published>2015-07-27T00:00:00-07:00</published><updated>2015-07-27T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/07/27/simupop-performance-in-seriationct</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/07/27/simupop-performance-in-seriationct.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mmadsen/seriationct&quot;&gt;SeriationCT&lt;/a&gt; uses the &lt;a href=&quot;http://http://simupop.sourceforge.net/&quot;&gt;simuPOP&lt;/a&gt; framework, building custom actions on top of it but leveraging the excellent support Bo Peng has created for transmission modeling. simuPOP has support for running across multiple threads or cores, but this support has been disabled lately on Mac OS X given problems with OpenMP and GCC 4.2.1 (according to Bo).&lt;/p&gt;
&lt;p&gt;Along with Bo, I’ve noted that OS X has excellent performance even without it, exceeding performance on Linux by a reasonable factor (sometimes even 2x).&lt;/p&gt;
&lt;p&gt;Now that I’m running on my &lt;a href=&quot;http://notebook.madsenlab.org/essays/2015/07/07/new-directions-ct-computing.html&quot;&gt;new simulation system&lt;/a&gt;, a 10-core Xeon E5-2650 V3 system equipped with a Xeon Phi 5110P coprocessor, I wanted to see what level of thread use in simuPOP provided the best performance.&lt;/p&gt;
&lt;p&gt;The test setup runs the following simulation as the only active code on the system, apart from the MongoDB database which is serving as data repository. I ran this exact simulation with the same seed across thread count from 1 to 20, and then repeated this loop several times with different seeds, to generate multiple timing values for each thread count. The whole process was duplicated for 5 replicates and 10 replicates.&lt;/p&gt;
&lt;pre class=&quot;shell&quot;&gt;&lt;code&gt;[mark@ewens sctcal-1]$ sim-seriationct-networkmodel.py  --dbhost localhost --dbport 27017 --experiment sctcal-1 --popsize 250 --maxinittraits 5 --numloci 3 --innovrate 8.86017736123e-05 --simlength 12000 --debug 0 --seed 166602270 --reps 1 --samplefraction 0.5 --migrationfraction 0.0517857031302 --devel 0 --networkmodel networks/two-node-three-slice.zip --cores 10&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;single-replication&quot;&gt;Single Replication&lt;/h3&gt;
&lt;p&gt;The results are shown below, with a loess smoothed trend line for each half of the graph. On the left, thread count is less than the number of physical cores, and it appears that threads are distributed among cores with scatter affinity (confirmed by observation in htop). On the right, hyperthreading is used to assign up to 2 threads per physical core, which is within the design spec of the E5-2650V3 processor. Some quick tests using 25 and 30 threads show that execution time shoots up precipitously given that execution is largely CPU-bound, and those data points are excluded in order to focus on the fine detail.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/simupop-perf-by-core-web.png&quot; alt=&quot;Figure 1: Execution Time Given Threads Used&quot; /&gt;&lt;figcaption&gt;Figure 1: Execution Time Given Threads Used&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Immediately apparent is that the process really is CPU-bound, and that the code performs best with no hyperthreading, and a single thread per core. The strong increase around 18-20 threads is indicative of contention between the simulation threads and the MongoDB database which serves as the data repository, it looks like.&lt;/p&gt;
&lt;p&gt;On the left, it is interesting that the absolute best performance is obtained by configuring simuPOP with a single thread, even with 10 physical cores present. Performance immediately worsens with 2 or more threads, before improving slightly between 8 and 10 threads. I have no explanation for why performance should improve, but clearly a single replication runs fastest when given a single execution thread, and the attempt to parallelize a single replication actually worsens the situation. I need to look at the simuPOP source and see why this occurs.&lt;/p&gt;
&lt;h3 id=&quot;performance-with-multiple-replicates&quot;&gt;Performance with Multiple Replicates&lt;/h3&gt;
&lt;p&gt;I also ran tests with the same simulation command line, but with 5 replicates. Sometimes I use replicates with simuPOP, usually when I have a fixed grid of parameters and want to understand variability given the parameter space. Replicates are independent copies of a population, each evolving separately according to the same evolution schedule and operators, so this would seem to be a logical point at which parallelism would be leveraged.&lt;/p&gt;
&lt;p&gt;This experiment I ran only a single series, given the longer execution times. The results are shown in Figure 2.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/simupop-perf-by-core-5rep-web.png&quot; alt=&quot;Figure 2: Execution Time for 5 Replicates Given Threads Used&quot; /&gt;&lt;figcaption&gt;Figure 2: Execution Time for 5 Replicates Given Threads Used&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The pattern is the same, and the execution times are basically 5x the single replication time.&lt;/p&gt;
&lt;h3 id=&quot;parallelism-in-simupop&quot;&gt;Parallelism in simuPOP&lt;/h3&gt;
&lt;p&gt;Looking at the version 1.1.6 source code, it appears that multiple threads are used in very specific situations. The simulator and its evolve function are single-threaded, with no OpenMP sections. Instead, parallelism is employed in specific loops, such as initializing a population or processing the individuals in a subpopulation, as in this example:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode cpp&quot;&gt;&lt;code class=&quot;sourceCode cpp&quot;&gt;        &lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt; (; sp != sp_end; ++sp) {
                pop.activateVirtualSubPop(*sp);
                size_t numValues = m_values.size();
                &lt;span class=&quot;kw&quot;&gt;if&lt;/span&gt; (numThreads() &amp;gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; &amp;amp;&amp;amp; !values.empty()) {
&lt;span class=&quot;ot&quot;&gt;#ifdef _OPENMP&lt;/span&gt;
&lt;span class=&quot;ot&quot;&gt;#  pragma omp parallel firstprivate (idx)&lt;/span&gt;
                        {
                                size_t id = omp_get_thread_num();
                                IndIterator ind = pop.indIterator(sp-&amp;gt;subPop(), id);
                                idx = idx + id * (pop.subPopSize(sp-&amp;gt;subPop()) / numThreads());
                                &lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt; (; ind.valid(); ++ind, ++idx)
                                        &lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt; (size_t i = &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;; i &amp;lt; infoIdx.size(); ++i) {
                                                ind-&amp;gt;setInfo(values[idx % numValues], infoIdx[i]);
                                        }
                        }
                        idx = idx + pop.subPopSize(sp-&amp;gt;subPop());
&lt;span class=&quot;ot&quot;&gt;#endif&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where tiles of individuals are processed by each of the N threads configured.&lt;/p&gt;
&lt;p&gt;Examining the simulator itself, I can see why parallelism isn’t expressed at the replicate or subpopulation level. Applications built in simuPOP are capable of using pure-Python operators mixed with built-in C++ operations, and if replicates are threaded rather than being distributed across Unix processes, we run immediately into the GIL issue. So Bo has used OpenMP to parallelize things only &lt;strong&gt;within&lt;/strong&gt; a single operator or function, which makes it safe.&lt;/p&gt;
&lt;p&gt;What does this mean?&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;I’m probably not getting any speedup because the population size in my current simulation is small – 250 individuals – rather than the thousands to millions that one typically sees in genetics work. This means that the OpenMP and thread management overhead is dominating, and the more threads I use, the worse this gets. So, with small population sizes, the guideline here is &lt;strong&gt;always use a single thread&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since simuPOP never parallelizes over replicates, it’s better to keep single replicates in their own Unix processes and use Grid Engine or a job scheduler to parallelize a &lt;strong&gt;set of simulations&lt;/strong&gt; across the available cores.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The latter is what I’ve been doing on StarCluster in previous projects, so it’s a familiar and easily scriptable protocol. And, if I work with large population sizes at some point, the native threading in simuPOP could pay off, but otherwise, it’s best ignored for most of the work I’m doing.&lt;/p&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><summary>Background</summary></entry></feed>
