<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.0">Jekyll</generator><link href="http://notebook.madsenlab.org/feed.xml" rel="self" type="application/atom+xml" /><link href="http://notebook.madsenlab.org/" rel="alternate" type="text/html" /><updated>2016-02-14T20:30:24-08:00</updated><id>http://notebook.madsenlab.org/</id><title>Lab Notebook for Mark E. Madsen</title><author><name>Mark E. Madsen</name></author><entry><title>Identifying Metapopulation Models from Simulated CT and Seriations</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/14/seriation-classification-experiment.html" rel="alternate" type="text/html" title="Identifying Metapopulation Models from Simulated CT and Seriations" /><published>2016-02-14T00:00:00-08:00</published><updated>2016-02-14T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/14/seriation-classification-experiment</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriation-classification/2016/02/14/seriation-classification-experiment.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;In a &lt;a href=&quot;http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2016/01/26/quantifying-similarity-seriations.html&quot;&gt;previous note&lt;/a&gt;, I described the problem of inferring the goodness of fit between a regional network model and the sampled output of cultural transmission on that regional network, as measured through seriations. I am now ready with the simulation and inference code to start testing the spectral similarity metric I discussed in that note across pairs and sets of regional network models.&lt;/p&gt;
&lt;p&gt;Here, I describe the first such comparison.&lt;/p&gt;
&lt;h3 id=&quot;experiment-sc-1&quot;&gt;Experiment: SC-1&lt;/h3&gt;
&lt;p&gt;SC-1 is a simple contrast between two regional network models. A regional network model is a time-transgressive description of the interaction patterns that existed among a set of subpopulations, described by an “interval temporal network” representation (see &lt;a href=&quot;http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2014/11/28/more-temporal-networks-python.html&quot;&gt;note 2&lt;/a&gt; and &lt;a href=&quot;http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2014/07/28/implementing-temporal-networks-in-python.html&quot;&gt;note 1&lt;/a&gt; about the implementation of such models).&lt;/p&gt;
&lt;p&gt;Both models are composed of 10 time slices.&lt;/p&gt;
&lt;p&gt;Model #1 is called “linear” in the experiment directory because it should ideally yield simple, linear seriation solutions because the only thing occurring is sampling through time. At any given time, the metapopulation is composed of 64 subpopulations each. Each subpopulation is fully connected, so that any subpopulation can exchange migrants with any other, and there are no differences in edge weights (and thus migration rates). Each subpopulation in slice &lt;span class=&quot;math&quot;&gt;\(N\)&lt;/span&gt; is the child of a random subpopulation in slice &lt;span class=&quot;math&quot;&gt;\(N-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Model #2 is called “lineage” in the experiment directory because it features 4 clusters of subpopulations, with 8 subpopulations per cluster. Subpopulations are fully connected within a cluster, with edge weight 10. Subpopulations are connected between subpopulations at fraction 0.1, with edge weight 1. This yields a strong tendency to exchange migrants within clusters, and at a much lower rate between clusters. For the first 4 time slices, all four clusters of subpopulations are interconnected, but in slice 4, a “split” occurs, removing interconnections between two sets of clusters, leaving &lt;span class=&quot;math&quot;&gt;\({1,2}\)&lt;/span&gt; interconnected and &lt;span class=&quot;math&quot;&gt;\({3,4}\)&lt;/span&gt; interconnected, but no connections between these sets. The resulting clusters then evolve for 6 more time slices on separate trajectories, giving rise to a “lineage split.”&lt;/p&gt;
&lt;p&gt;Neutral cultural transmission is simulated across these two network models, with 50 simulation replicates on each model, and a common set of parameters:&lt;/p&gt;
&lt;pre class=&quot;sourceCode json&quot;&gt;&lt;code class=&quot;sourceCode json&quot;&gt;&lt;span class=&quot;fu&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;theta_low&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.00001&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;theta_high&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;maxinittraits&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;numloci&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;popsize&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;250&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;simlength&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;8000&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;samplefraction&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;migrationfraction_low&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;migrationfraction_high&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;replicates&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;fu&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Innovation rates and migration fractions are chosen uniformly at random from the ranges given for each simulation run. Each locus evolves randomly, but we track combinations of loci as “classes” in the archaeological sense of the term as our observable variables. Populations evolve for 8000 generations, giving approximately 800 transmission events per individual during a time slice (i.e., step in the regional metapopulation evolution). We can think of that as approximately monthly opportunities for copying artifacts or behavioral traits, over a lifetime of approximately 65 years.&lt;/p&gt;
&lt;p&gt;The raw data from each community in a simulated network model are then aggregated over the duration that community persists, giving us a time averaged picture of the frequency of cultural traits.&lt;/p&gt;
&lt;p&gt;I then perform the following sampling steps:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;I take a sample of each time averaged data collection similar to the size of a typical archaeological surface collection: 500 samples are taken without replacement from each community, with probability proportional to class frequency. This yields a smaller set of classes, since many hundreds or thousands of combinations are only seen once or twice in the simulation data, and thus are very hard to measure from empirical samples.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I then take a sample of communities to seriate. From each of the two network models, I take temporally stratified samples, with 3 communities per time slice sampled out of the 64 present in each slice. In real sampling situations, we would not know how communities break down temporally, but we often do attempt to get samples of assemblages which cover our entire study duration. In the case of Model #1, we take a 5% sample of communities in each time slice. In the case of Model #2, given the different structure of the model, we take a 12% sample of communities in each time slice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Within the overall sample from each simulation run, comprised now of the time averaged class frequencies from 30 sampled communities, I examine the classes/types themselves, and drop any types (columns) which do not have data values for at least 3 communities. This is standard pre-processing for seriation analyses (or was in the Fordian manual days of seriation analysis) since without more than 3 values, a column does not contribute to ordering the communities.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/clipo/idss-seriation&quot;&gt;IDSS Seriation&lt;/a&gt; package is then used to seriate the stratified, filtered data files for each simulation run across the two models.&lt;/p&gt;
&lt;h3 id=&quot;first-classification-attempt&quot;&gt;First Classification Attempt&lt;/h3&gt;
&lt;p&gt;For a first classification attempt, I did a “leave one out” cross validation run, in which each seriation graph was sequentially deleted from the training set of 99 seriations (one lineage graph had issues with duplicate frequencies and became stuck in frequency seriation), and the distance from the hold-out target graph to all others calculated using &lt;a href=&quot;http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2016/01/26/quantifying-similarity-seriations.html&quot;&gt;Laplacian spectral distance&lt;/a&gt;. The label of the target graph was then predicted as the majority vote of the 5 nearest neighboring graphs. No attempt was made to tune the number of nearest neighbors using a second cross validation pass, but that will be the next experiment.&lt;/p&gt;
&lt;p&gt;In general, the ability to predict the label (network model) which gave rise to the target seriation graph is decent: 76.8%.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Classification Report:

          predicted 0  predicted 1
actual 0           41            9
actual 1           14           35
             precision    recall  f1-score   support

          0       0.75      0.82      0.78        50
          1       0.80      0.71      0.75        49

avg / total       0.77      0.77      0.77        99

Accuracy on test: 0.768&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The details of classifier accuracy seem to show that we have better ability to correctly classify seriations which result from Model #1 (“linear”) model than seriations originating from the lineage splitting Model #2. In particular, we correctly identify instances of Model #1 82% of the time (recall), although there are clear issues with false positives. The ability to identify instances of Model #2 is worse, with a considerable number of false negatives.&lt;/p&gt;
&lt;p&gt;In the next experiment, I intend to see if different values of the k-Nearest Neighbor parameter affect this accuracy, but I expect that achieving higher accuracy might require augmenting the approach. One possibility that bears exploration is to not simply use the spectral distance, but to instead use the Laplacian eigenvalues themselves (sorted in decreasing order) directly as features, in addition to other graph theoretic properties such as average degree and tree radius, and use a more traditional classifier like boosted decision trees. That will probably be my third experiment.&lt;/p&gt;
&lt;h3 id=&quot;second-attempt&quot;&gt;Second Attempt&lt;/h3&gt;
&lt;p&gt;In a second run, I examined the effect of the number of nearest neighbors which “vote” on the label of the target seriation, still with leave-one-out cross validation. The results seem to indicate that (at least for these models) that best performance is 3 nearest neighbors, with accuracy worsening for 5, 7, and 9 neighbors, and then plateauing a bit for 11 and 15 neighbors. But with 3 neighbors, we achieve almost 80% accuracy, which is encouraging.&lt;/p&gt;
&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;

knn = [&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;11&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;15&lt;/span&gt;]
&lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt; nn in knn:
    gclf = skm.GraphEigenvalueNearestNeighbors(n_neighbors=nn)
    test_pred = []
    &lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt; ix in &lt;span class=&quot;dt&quot;&gt;range&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;len&lt;/span&gt;(train_graphs)):
        train_loo_graphs, train_loo_labels, test_graph, test_label = leave_one_out_cv(ix, train_graphs, train_labels)
        gclf.fit(train_loo_graphs, train_loo_labels)
        test_pred.append(gclf.predict([test_graph])[&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;])
    &lt;span class=&quot;dt&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Accuracy on test for &lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; neighbors: &lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;%0.3f&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt; % (nn, accuracy_score(train_labels, test_pred)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Accuracy on test for 1 neighbors: 0.788
Accuracy on test for 3 neighbors: 0.798
Accuracy on test for 5 neighbors: 0.768
Accuracy on test for 7 neighbors: 0.747
Accuracy on test for 9 neighbors: 0.758
Accuracy on test for 11 neighbors: 0.788
Accuracy on test for 15 neighbors: 0.788&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;references&quot;&gt;
&lt;h3&gt;References Cited&lt;/h3&gt;
&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><summary>Background</summary></entry><entry><title>Loss Functions for ABC Model Selection with Seriation Graphs as Data</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2016/01/26/quantifying-similarity-seriations.html" rel="alternate" type="text/html" title="Loss Functions for ABC Model Selection with Seriation Graphs as Data" /><published>2016-01-26T00:00:00-08:00</published><updated>2016-01-26T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2016/01/26/quantifying-similarity-seriations</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2016/01/26/quantifying-similarity-seriations.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;Traditional archaeological seriation generated solutions that always had a static linear order, either by use of a multidimensional scaling algorithm or via Fordian manual shuffling of assemblages. In such context, departures from a “correct” solution can be measured by the number of “out of order” assemblages (or via a more opaque “stress” statistic in some cases).&lt;/p&gt;
&lt;p&gt;Last year Carl Lipo and I &lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0124942&quot;&gt;introduced a method&lt;/a&gt; for finding sets of classical Fordian seriations from a set of assemblages via an agglomerative iterative procedure &lt;span class=&quot;citation&quot; data-cites=&quot;Lipo2015&quot;&gt;(Lipo 2015)&lt;/span&gt;. We called the resulting seriations “iterative deterministic seriation solutions” or IDSS. IDSS is capable of using any metric for joining assemblages, including classic unimodality, or various distance measures (something we’ll be &lt;a href=&quot;http://notebook.madsenlab.org/essays/2015/09/06/saa2016-abstract.html&quot;&gt;talking about at the SAA meetings this year&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;When IDSS finds groups of data points that seriate together, but do not seriate (within thresholds) with other groups of points, several solutions are created rather than forcing all of the data points into a single scaling or solution (as has been typical practice with statistical seriation by archaeologists in the past). Instead, IDSS outputs all valid solution sets, since the discontinuities between solution sets may indicate lack of cultural interaction, poor recovery sampling of the archaeological record, or other real-world factors.&lt;/p&gt;
&lt;p&gt;Frequently, one assemblage or data point will occur in multiple solutions, and when this occurs IDSS overlays the solutions and forms a tree. From our paper, Figure 1 shows an IDSS frequency seriation solution for archaeological sites in the Central Mississippi River Valley. Each assemblage is represented by decorated ceramic class frequencies, and we can see that there are at least three major solutions which share assemblage 13-O-7, and a number of minor branches.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/idss-fig12-pfg-solution.png&quot; alt=&quot;Figure 1: IDSS seriation solution for PFG sites in Cental Mississippi River Valley&quot; /&gt;&lt;figcaption&gt;Figure 1: IDSS seriation solution for PFG sites in Cental Mississippi River Valley&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;the-problem&quot;&gt;The Problem&lt;/h3&gt;
&lt;p&gt;My current research is aimed at developing ways of treating IDSS seriation solution graphs as data points in machine learning algorithms, with the goal of fitting coarse grained, regional models of cultural transmission and information diffusion to our data. Seriations are the perfect tool for capturing regional-scale social influences that are time-transgressive.&lt;/p&gt;
&lt;p&gt;There are several challenges in this work. The first, which is described in a &lt;a href=&quot;http://localhost:4000/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2014/06/17/seriationct-requirements.html&quot;&gt;previous post&lt;/a&gt; is to create regional metapopulation models with cultural transmission that yield seriations, rather than just type or variant frequencies, as their observable output.&lt;/p&gt;
&lt;p&gt;The second challenge is then to find an inferential framework for model selection which allows one to measure the goodness-of-fit between the theoretical model’s output, and specific empirical seriations like Figure 1.&lt;/p&gt;
&lt;p&gt;This second challenge is perfectly suited to an Approximate Bayesian Computation approach &lt;span class=&quot;citation&quot; data-cites=&quot;Beaumont2002 Csillery2010 Marin2012 Toni2009&quot;&gt;(Beaumont, Zhang, and Balding 2002; Csill&lt;span&gt;é&lt;/span&gt;ry et al. 2010; Marin et al. 2012; Toni et al. 2009)&lt;/span&gt;, since while we can simulate data from each social learning model, writing down the likelihood function for each model is generally an intractable problem.&lt;/p&gt;
&lt;p&gt;In brief, ABC model selection involves simulating a large number of synthetic data points from each model, calculating summary statistics from those data points, measuring the distance (losses) between summary statistics and observed data, and choosing the model whose losses are the smallest.&lt;/p&gt;
&lt;p&gt;In my current work, the “models” are actually a combination of:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;A social learning model (e.g., unbiased cultural transmission or conformist transmission)&lt;/li&gt;
&lt;li&gt;Parameters for that social learning model (e.g., innovation rates)&lt;/li&gt;
&lt;li&gt;A spatiotemporal population model that describes how communities evolved in a region and what the pattern of community interaction was over time&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a fairly complex “model” to fit, but we can easily simulate samples of cultural variant frequencies across subpopulations from each model (using the &lt;a href=&quot;https://github.com/mmadsen/seriationct&quot;&gt;&lt;code&gt;SeriationCT&lt;/code&gt; software suite&lt;/a&gt;, and then use IDSS to seriate those variant frequencies, yielding simulated versions of Figure 1.&lt;/p&gt;
&lt;p&gt;What we need, then, is a way to measure the distance or loss between Figure 1 and various simulated seriation graphs. Once we have a good loss function, we should be able to perform ABC model selection on seriation graphs.&lt;/p&gt;
&lt;h3 id=&quot;distanceloss-for-unlabelled-unordered-graphs&quot;&gt;Distance/Loss for Unlabelled, Unordered Graphs&lt;/h3&gt;
&lt;p&gt;In general, we want a function which measures the structural similarity of two graphs. There are many approaches to the problem (see &lt;span class=&quot;citation&quot; data-cites=&quot;zager2008graph&quot;&gt;(Zager and Verghese 2008)&lt;/span&gt; for a review). Graph edit distance would be a fairly natural metric, but most algorithms for edit distance rely on matching node identities, and many algorithms also rely upon graphs being ordered as well as labelled.&lt;/p&gt;
&lt;p&gt;The simulated seriations which come out of the &lt;code&gt;SeriationCT&lt;/code&gt; package are unlabelled, and we can’t label them with the identities of the assemblages in our empirical data. So our loss function has to measure structural similarity without reference to node identity, or any notion of ordering or orientation for the graphs.&lt;/p&gt;
&lt;p&gt;This leads fairly directly to using purely algebraic properties of the seriation graphs, and in particular the spectral properties of various matrices associated with the graphs. We know that the eigenvalues of the (possibly binarized) adjacency matrix are related to the edge structure of a graph, and that the eigenvalues of the Laplacian of the adjacency matrix are an even more sensitive indicator of structure &lt;span class=&quot;citation&quot; data-cites=&quot;godsil2001algebraic&quot;&gt;(Godsil and Royle 2001)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a technical report, Koutra et al. &lt;span class=&quot;citation&quot; data-cites=&quot;koutra2011algorithms&quot;&gt;(Koutra et al. 2011)&lt;/span&gt; suggest using the sum of squared differences between the Laplacian spectra of two graphs, trimming the spectra to use only 90% of the total eigenvalue weight. This is attractive from a statistical perspective, essentially giving us the L2 loss between Laplacian spectra for two graphs.&lt;/p&gt;
&lt;p&gt;Given adjacency matrices &lt;span class=&quot;math&quot;&gt;\(A_1\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(A_2\)&lt;/span&gt; for graphs &lt;span class=&quot;math&quot;&gt;\(G_1\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(G_2\)&lt;/span&gt;, the Laplacian matrix is simply &lt;span class=&quot;math&quot;&gt;\(L_1 = D_1 - A_1\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(L_2 = D_2 - A_2\)&lt;/span&gt;, where &lt;span class=&quot;math&quot;&gt;\(D_i\)&lt;/span&gt; are the corresponding diagonal matrix of vertex degrees. The spectrum is then the set of eigenvalues &lt;span class=&quot;math&quot;&gt;\((\lambda_1 \ldots \lambda_n)\)&lt;/span&gt;. Since the Laplacian is positive semi-definite, all of the eigenvalues in the spectrum are positive real numbers. It is possible to put some bounds on the eigenvalues, and thus predetermine the range of possible loss function values for graphs with a given number of vertices, but I leave that to a future post.&lt;/p&gt;
&lt;p&gt;If we use the full spectrum of values, our L2 spectral loss function is then:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\(\mathcal{L}_{sp} = \sum_{i=1}^{n} (\lambda_{1i} - \lambda_{2i})^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and if we only use a trimmed set of eigenvalues (say, those which contribute to the top 90% of the total sum of the spectrum), we sort the list of those &lt;span class=&quot;math&quot;&gt;\(k\)&lt;/span&gt; eigenvalues and replace &lt;span class=&quot;math&quot;&gt;\(n\)&lt;/span&gt; with &lt;span class=&quot;math&quot;&gt;\(k\)&lt;/span&gt; in the previous equation.&lt;/p&gt;
&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;In Python, we can implement this loss function for NetworkX graph objects as follows (now part of the &lt;code&gt;seriationct.analytics&lt;/code&gt; module):&lt;/p&gt;
&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;
&lt;span class=&quot;ch&quot;&gt;import&lt;/span&gt; networkx &lt;span class=&quot;ch&quot;&gt;as&lt;/span&gt; nx
&lt;span class=&quot;ch&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;ch&quot;&gt;as&lt;/span&gt; np

&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; graph_spectral_similarity(g1, g2, threshold = &lt;span class=&quot;fl&quot;&gt;0.9&lt;/span&gt;):
    &lt;span class=&quot;co&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    Returns the eigenvector similarity, between [0, 1], for two NetworkX graph objects, as&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    the sum of squared differences between the sets of Laplacian matrix eigenvalues that account&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    for a given fraction of the total sum of the eigenvalues (default = 90%).&lt;/span&gt;

&lt;span class=&quot;co&quot;&gt;    Similarity scores of 0.0 indicate identical graphs (given the adjacency matrix, not necessarily&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    node identity or annotations), and large scores indicate strong dissimilarity.  The statistic is&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    unbounded above.&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    l1 = nx.spectrum.laplacian_spectrum(g1, weight=&lt;span class=&quot;ot&quot;&gt;None&lt;/span&gt;)
    l2 = nx.spectrum.laplacian_spectrum(g2, weight=&lt;span class=&quot;ot&quot;&gt;None&lt;/span&gt;)
    k1 = _get_num_eigenvalues_sum_to_threshold(l1, threshold=threshold)
    k2 = _get_num_eigenvalues_sum_to_threshold(l2, threshold=threshold)
    k = &lt;span class=&quot;dt&quot;&gt;min&lt;/span&gt;(k1,k2)
    sim = &lt;span class=&quot;dt&quot;&gt;sum&lt;/span&gt;((l1[:k] - l2[:k]) ** &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;)
    &lt;span class=&quot;kw&quot;&gt;return&lt;/span&gt; sim


&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; _get_num_eigenvalues_sum_to_threshold(spectrum, threshold = &lt;span class=&quot;fl&quot;&gt;0.9&lt;/span&gt;):
    &lt;span class=&quot;co&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    Given a spectrum of eigenvalues, find the smallest number of eigenvalues (k)&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    such that the sum of the k largest eigenvalues of the spectrum&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    constitutes at least a fraction (threshold, default = 0.9) of the sum of all the eigenvalues.&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;kw&quot;&gt;if&lt;/span&gt; threshold is &lt;span class=&quot;ot&quot;&gt;None&lt;/span&gt;:
        &lt;span class=&quot;kw&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;dt&quot;&gt;len&lt;/span&gt;(spectrum)

    total = &lt;span class=&quot;dt&quot;&gt;sum&lt;/span&gt;(spectrum)
    &lt;span class=&quot;kw&quot;&gt;if&lt;/span&gt; total == &lt;span class=&quot;fl&quot;&gt;0.0&lt;/span&gt;:
        &lt;span class=&quot;kw&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;dt&quot;&gt;len&lt;/span&gt;(spectrum)

    spectrum = &lt;span class=&quot;dt&quot;&gt;sorted&lt;/span&gt;(spectrum, reverse=&lt;span class=&quot;ot&quot;&gt;True&lt;/span&gt;)
    running_total = &lt;span class=&quot;fl&quot;&gt;0.0&lt;/span&gt;

    &lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt; i in &lt;span class=&quot;dt&quot;&gt;range&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;len&lt;/span&gt;(spectrum)):
        running_total += spectrum[i]
        &lt;span class=&quot;kw&quot;&gt;if&lt;/span&gt; running_total / total &amp;gt;= threshold:
            &lt;span class=&quot;kw&quot;&gt;return&lt;/span&gt; i + &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;co&quot;&gt;# guard&lt;/span&gt;
    &lt;span class=&quot;kw&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;dt&quot;&gt;len&lt;/span&gt;(spectrum)


&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;references&quot;&gt;
&lt;h3&gt;References Cited&lt;/h3&gt;
&lt;p&gt;Beaumont, Mark A, W Zhang, and D J Balding. 2002. “Approximate Bayesian computation in population genetics.” &lt;em&gt;Genetics&lt;/em&gt;. &lt;a href=&quot;http://www.genetics.org/content/162/4/2025.short&quot; class=&quot;uri&quot;&gt;http://www.genetics.org/content/162/4/2025.short&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Csill&lt;span&gt;é&lt;/span&gt;ry, Katalin, Michael G B Blum, Oscar E Gaggiotti, and Olivier Fran&lt;span&gt;ç&lt;/span&gt;ois. 2010. “Approximate Bayesian Computation (ABC) in practice.” &lt;em&gt;Trends in Ecology &amp;amp; Evolution&lt;/em&gt; 25 (7): 410–18. doi:&lt;a href=&quot;http://dx.doi.org/10.1016/j.tree.2010.04.001&quot;&gt;10.1016/j.tree.2010.04.001&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Godsil, Christopher David, and Gordon Royle. 2001. &lt;em&gt;Algebraic Graph Theory&lt;/em&gt;. Vol. 8. Springer New York.&lt;/p&gt;
&lt;p&gt;Koutra, Danai, Ankur Parikh, Aaditya Ramdas, and Jing Xiang. 2011. &lt;em&gt;Algorithms for Graph Similarity and Subgraph Matching&lt;/em&gt;. Technical Report of Carnegie-Mellon-University.&lt;/p&gt;
&lt;p&gt;Lipo, Mark E. AND Dunnell, Carl P. AND Madsen. 2015. “A Theoretically-Sufficient and Computationally-Practical Technique for Deterministic Frequency Seriation.” &lt;em&gt;PLoS ONE&lt;/em&gt; 10 (4). Public Library of Science: e0124942. doi:&lt;a href=&quot;http://dx.doi.org/10.1371/journal.pone.0124942&quot;&gt;10.1371/journal.pone.0124942&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Marin, Jean-Michel, Pierre Pudlo, Christian P Robert, and Robin J Ryder. 2012. “Approximate Bayesian Computational Methods.” &lt;em&gt;Statistics and Computing&lt;/em&gt; 22 (6). Springer: 1167–80.&lt;/p&gt;
&lt;p&gt;Toni, Tina, David Welch, Natalja Strelkowa, Andreas Ipsen, and Michael P H Stumpf. 2009. “Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems.” &lt;em&gt;Journal of Royal Society Interface&lt;/em&gt; 6 (31). The Royal Society: 187–202. doi:&lt;a href=&quot;http://dx.doi.org/10.1098/rsif.2008.0172&quot;&gt;10.1098/rsif.2008.0172&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Zager, Laura A, and George C Verghese. 2008. “Graph Similarity Scoring and Matching.” &lt;em&gt;Applied Mathematics Letters&lt;/em&gt; 21 (1). Elsevier: 86–94.&lt;/p&gt;
&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><category term="ABC" /><summary>Background</summary></entry><entry><title>Amazon EC2 AMI for deep neural networks and classification problems</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/2016/01/24/neural-network-ami-for-ec2.html" rel="alternate" type="text/html" title="Amazon EC2 AMI for deep neural networks and classification problems" /><published>2016-01-24T00:00:00-08:00</published><updated>2016-01-24T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/2016/01/24/neural-network-ami-for-ec2</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/2016/01/24/neural-network-ami-for-ec2.html">&lt;h3 id=&quot;ami-for-deep-neural-networks&quot;&gt;AMI for Deep Neural Networks&lt;/h3&gt;
&lt;p&gt;Given the increasing quality of libraries for building deep neural network architectures, I’ve been exploring whether DNNs can improve my accuracy in distinguishing between the empirical signatures of cultural transmission models. I started out with Lasagne driving Theano, since Theano itself is pretty nuts and bolts and I occasionally find getting the right tensor shapes a bit baffling. I quickly found Keras, which is another (and very well designed) wrapper around &lt;strong&gt;both&lt;/strong&gt; TensorFlow and Theano, which also allows one to switch between backends. Keras is designed for quick prototyping and research, which is perfect for this context.&lt;/p&gt;
&lt;p&gt;Being able to switch backends also turns out to be fairly handy if you develop on OS X, because if you’ve got Yosemite and the current version of Xcode (7.1), the CLANG compiler toolchain is not compatible with Theano and the CUDA nvcc compiler. Which meant that I’d been developing in an Ubuntu 15.10 virtual machine on my laptop, which was sufficient for testing, and then pushing code to a box with an NVIDIA GPU. By switching the Keras backend to TensorFlow, I can test the main body of my model code on the Mac, and then switch it back to Theano to run on a box with a GPU (since Theano on GPU on Linux is generally thought to be better performing than TensorFlow in general right now).&lt;/p&gt;
&lt;p&gt;I looked around a bit for AMIs to quickly spin up an EC2 instance, and mostly found a lot of broken links to AMIs that weren’t available in my default &lt;code&gt;us-east-1&lt;/code&gt; availability zone. So, I spent a couple of hours today building one with everything I needed.&lt;/p&gt;
&lt;p&gt;The public AMI &lt;code&gt;ami-5cc6e636&lt;/code&gt; is based upon Ubuntu 14.04, with the following added:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Caffe direct from Github bleeding edge&lt;/li&gt;
&lt;li&gt;cuDNN v3.0 from NVIDIA&lt;/li&gt;
&lt;li&gt;CUDA 7.0, with driver built for the Linux 3.13 kernel that Ubuntu 14.04 uses&lt;/li&gt;
&lt;li&gt;Anaconda Python 2.7&lt;/li&gt;
&lt;li&gt;Theano from bleeding edge, which means 0.8 at this point in time&lt;/li&gt;
&lt;li&gt;Lasagne from bleeding edge, which means 0.2 at this point&lt;/li&gt;
&lt;li&gt;Keras, from &lt;code&gt;pip install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;nolearn, from &lt;code&gt;pip install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Seaborn for visualization, from &lt;code&gt;pip install&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Graphviz&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The image has a mounted EBS volume for data, and a 32GB root partition since there’s a lot of stuff installed here.&lt;/p&gt;
&lt;p&gt;Fire this AMI up on an &lt;code&gt;g2.2xlarge&lt;/code&gt; instance type for 8 CPU cores, 15GB of RAM, and a GPU with 1,536 CUDA cores. I will try to keep an AMI with these specs up to date, and since the AMI ID will change, check &lt;a href=&quot;http://notebook.madsenlab.org/tag/aws.html&quot;&gt;the tag &lt;code&gt;AWS&lt;/code&gt;&lt;/a&gt; for updates.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="experiments" /><category term="computing" /><category term="aws" /><category term="ML" /><category term="deep learning" /><category term="HPC" /><category term="classification" /><summary>AMI for Deep Neural Networks</summary></entry><entry><title>Abstract for the 2016 SAA Conference</title><link href="http://notebook.madsenlab.org/essays/2015/09/06/saa2016-abstract.html" rel="alternate" type="text/html" title="Abstract for the 2016 SAA Conference" /><published>2015-09-06T00:00:00-07:00</published><updated>2015-09-06T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/essays/2015/09/06/saa2016-abstract</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/2015/09/06/saa2016-abstract.html">&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Measuring Cultural Relatedness Using Multiple Seriation Ordering Algorithms&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Seriation is a long-standing archaeological method for relative dating that has proven effective in probing regional-scale patterns of inheritance, social networks, and cultural contact in their full spatiotemporal context. The orderings produced by seriation are produced by the continuity of class distributions and unimodality of class frequencies, properties that are related to social learning and transmission models studied by evolutionary archaeologists. Linking seriation to social learning and transmission enables one to consider ordering principles beyond the classic unimodal curve. Unimodality is a highly visible property that can be used to probe and measure the relationships between assemblages, and it was especially useful when seriation was accomplished with simple algorithms and manual effort. With modern algorithms and computing power, multiple ordering principles can be employed to better understand the spatiotemporal relations between assemblages. Ultimately, the expansion of seriation to additional ordering algorithms allows us an ability to more thoroughly explore underlying models of cultural contact, social networks, and modes of social learning. In this paper, we review our progress to date in extending seriation to multiple ordering algorithms, with examples from Eastern North America and Oceania.&lt;/p&gt;
&lt;h3 id=&quot;motivating-notes&quot;&gt;Motivating Notes&lt;/h3&gt;
&lt;p&gt;Archaeological seriation extracts an ordering from a set of class descriptions, usually rendered as type frequencies within assemblages, by reordering those descriptions until each type displays a continuous and unimodal distribution. This combination of criteria dates from the earliest days of seriation, and appears to originate in empirical generalizations concerning both archaeological distributions and the behavior of object “styles” in contemporary populations.&lt;/p&gt;
&lt;p&gt;Those empirical generalizations are easy to tie to the ideas behind cultural transmission and other diffusion models in a broad, descriptive way. It is clear, for example, that in many cases, a new cultural variant is introduced, spreads within a population, increasing in prevalence relative to other variants, and eventually declines in frequency as other, perhaps newer, variants are introduced and themselves grow. But it is also clear to anyone who has studied the behavior of diffusion and transmission models that unimodal growth and decline is far from the &lt;strong&gt;only&lt;/strong&gt; pattern that variants display. Especially in diffusion models with no selection, many classes display complex multimodal distributions.&lt;/p&gt;
&lt;p&gt;Similarly, we expect most cultural variants to display relatively “smooth” distributions, where the prevalence of a trait is similar among points close in space or time, without major discontinuities. We also expect culture-historical “styles” or types to display a single spacetime distribution without recurrence – this is the basis of occurrence seriation, and of course is an unspoken corollary of the unimodal frequency criterion.&lt;/p&gt;
&lt;p&gt;In classical seriation methods, the major ordering principles are unimodality and nonrecurrence (i.e., no holes). Neither of these, however, are the deep ordering principle which allow us to order descriptions into a full spatiotemporal map of a diffusion process. That principle, for underlying processes which are relatively continuous, is given by the “smoothness” principle, which requires that diffusion makes no major “jumps” and thus is continuous in the mathematical, analytic sense of the term.&lt;/p&gt;
&lt;p&gt;We call this the “analytic continuity” principle, and it is constructed in empirical cases by finding the ordering of assemblage descriptions which globally minimize the inter-assemblage distance, where distance is measured by an appropriate vector distance (e.g., cosine or angular distance).&lt;/p&gt;
&lt;p&gt;Why was this principle not employed earlier in archaeological seriation? After all, practitioners like James A. Ford fully understood the spatiotemporal nature of the cultural diffusion process which underlay seriation results (ref to the 1938 diagram). We believe that it was not used in real seriations because there is no way to accomplish such orderings in real cases without significant computational support, and by the time computer support for seriation was available, the nature of the problem had been recast as one of matrix ordering of similiarity coefficients, with significiant changes in the nature of the solutions accepted as a result. The difference is one of detail, but the details are significant.&lt;/p&gt;
&lt;p&gt;The other ordering principles – unimodality and “topological continuity” (or the “no holes” principle) – relate to smoothness continuity in a global/local fashion. The latter provides a global view of diffusion, while the latter allow us to find structure &lt;strong&gt;within&lt;/strong&gt; the field of assemblages, and find groups of assemblages which fit together more than they fit with surrounding groups.&lt;/p&gt;
&lt;p&gt;We propose, therefore, extensions to the seriation method which employ all three ordering principles: analytic continuity, to provide the global map of diffusion across all of the assemblage descriptions in a given data set, and the use of unimodality and non-recurrence (“no holes”) to help probe smaller-scale or “mesoscopic” structure within the data set.&lt;/p&gt;
&lt;p&gt;This redescription of the overall seriation method provides a way to link the method to both to an overall cultural transmission model, as well as a principled way to describe the internal structure of solutions at smaller scales, and is ideally suited to analysis at scales ranging from a few assemblages to large regional surveys. In a future paper, we intend to explore the extension of this method between regional and larger scales by exploring the interface between seriation and cladistics, which we believe extends the mesoscopic scale to macroscopic large scale analysis of cultural relatedness.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;
&lt;h3&gt;References Cited&lt;/h3&gt;
&lt;/div&gt;</content><category term="SAA2016" /><category term="seriation" /><category term="cultural transmission" /><category term="dissertation" /><summary>Abstract</summary></entry><entry><title>IDSS Seriation Software Version 2.3 released</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/2015/09/01/idss-seration-version-2.3-released.html" rel="alternate" type="text/html" title="IDSS Seriation Software Version 2.3 released" /><published>2015-09-01T00:00:00-07:00</published><updated>2015-09-01T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/2015/09/01/idss-seration-version-2.3-released</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/2015/09/01/idss-seration-version-2.3-released.html">&lt;p&gt;Today, Carl Lipo and I released &lt;a href=&quot;https://github.com/clipo/idss-seriation/releases/tag/v2.3&quot;&gt;Version 2.3&lt;/a&gt; of our &lt;a href=&quot;https://github.com/clipo/idss-seriation&quot;&gt;IDSS seriation software&lt;/a&gt;, described in our &lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0124942&quot;&gt;recent PLoS One article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This version contains:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;A bug fix for a triple ordering issue found by &lt;a href=&quot;http://bsc-es.academia.edu/XavierRubio&quot;&gt;Xavier Rubio-Campillo&lt;/a&gt; of the Barcelona Supercomputing Center (thank you, Xavi!). We did not incorporate his full pull request, since we are undecided about some of the changes, but did integrate this fix.&lt;/li&gt;
&lt;li&gt;A performance rewrite of the spatial/geographic significance bootstrap test, which improved the average runtime of this test about 4.5x (which helps with larger numbers of assemblages).&lt;/li&gt;
&lt;li&gt;Every run now gets a UUID and is associated with the release tag and Github commit identifier, to allow results to be tracked to specific software commits, not just periodic numbered releases.&lt;/li&gt;
&lt;li&gt;UUID and software commit identifiers are written to a “metadata” file in the output directory, and to the database if you use the MongoDB driver script.&lt;/li&gt;
&lt;li&gt;The pickled temporary directory (used for parallel processing) is now written in /tmp for convenience, and unless you specify –preservepickle 1, it is cleaned up at the end of the seriation run. This yields a cleaner setup for batch processing.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The easiest way to try out the software is to grab the source for the PLoS One article, which contains the data and a Makefile for regenerating our seriation output:&lt;/p&gt;
&lt;pre class=&quot;shell&quot;&gt;&lt;code&gt;$ git clone git@github.com:clipo/idss-seriation.git
...ignoring lines...
$ cd idss-seriation
$ python setup.py install
$ cd ..
$ git clone git@github.com:mmadsen/lipomadsen2015-idss-seriation-paper.git
$ cd lipomadsen2015-idss-seriation-paper/seriation-analysis
$ make&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see output that looks like this:&lt;/p&gt;
&lt;pre class=&quot;shell&quot;&gt;&lt;code&gt;IDSS seriation Version 2.3
2015-09-01 14:56:37,451 INFO: seriation run identifier: urn:uuid:5183fa22-50f4-11e5-959b-b8f6b1154c9b
Bootstrap CI calculation using 1000 samples - elapsed time: 0.037674 sec
Time elapsed for frequency seriation processing: 28 seconds
Seriation complete.
Maximum size of seriation: 3
Number of frequency seriation solutions at last step: 1139
Assemblages not part of final solution:
*** All assemblages used in seriations.***
Time elapsed for completion of program: 83 seconds&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and several graph windows will pop up. The “minmax” graph shows the best solution with branching to show how different partial seriations relate. There will be an “output” directory with full output.&lt;/p&gt;
&lt;p&gt;Please contact me or Carl Lipo if you have questions about IDSS and this release!&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="seriation" /><summary>Today, Carl Lipo and I released Version 2.3 of our IDSS seriation software, described in our recent PLoS One article.</summary></entry><entry><title>Improving Simulation Performance for CT Models with C++ and OpenMP</title><link href="http://notebook.madsenlab.org/essays/2015/08/24/simulation-performance-explorations.html" rel="alternate" type="text/html" title="Improving Simulation Performance for CT Models with C++ and OpenMP" /><published>2015-08-24T00:00:00-07:00</published><updated>2015-08-24T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/essays/2015/08/24/simulation-performance-explorations</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/2015/08/24/simulation-performance-explorations.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;A side project this summer has been to improve the performance of my basic simulation model codes, so I can increase my experiment “velocity” and do more analyses with larger sample and/or population sizes. Since getting a Xeon/Phi workstation (the machine I &lt;a href=&quot;/essays/2015/07/07/new-directions-ct-computing.html&quot;&gt;described a few months ago&lt;/a&gt;), I’ve been retraining on C++11 for situations which call for mixed Python/C++ performance, or for all C++ simulations when I can’t figure out a good offload or parallelism model for Python code. At the moment I’m using a pure C++ simulation of the Wright-Fisher neutral model, with infinite-alleles mutation on several loci for my testing. The &lt;a href=&quot;https://github.com/mmadsen/neutral-model-cpp&quot;&gt;simulation code is available here&lt;/a&gt; with Makefiles for several compilers, including clang (i.e., modern OS X), GCC 5.2 (which has first-gen support for compiling offload for the Xeon Phi coprocessors), and the Intel C/C++ compiler. (Mental note, I should include a plain vanilla Makefile which would use which ever “GCC” is present…).&lt;/p&gt;
&lt;p&gt;These notes describe what I’ve found so far about improving performance in this code.&lt;/p&gt;
&lt;h3 id=&quot;implementation-notes&quot;&gt;Implementation Notes&lt;/h3&gt;
&lt;p&gt;The simulation model is (hopefully) designed to allow for both parallelism across cores/threads, and for use of the Intel Xeon’s native (and considerable) vectorization infrastructure.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;The “population” is represented by an array of size numloci columns by popsize rows, with integers representing trait ID’s. This block is allocated a single linear block, and at least on the Intel compiler is 64-byte aligned to maximize use of the wide vectorization units (and the even wider ones on the Phi).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This creates “unit stride” access for many common operations, like calculating trait counts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complex operations like calling the C++11 random number library are extracted from nested loops and done separately, to allow the results of random number generation to be used in loops which are then parallelized AND vectorized.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;OpenMP is used to parallelize nested loops for actual trait transmission.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Explicit vectorization via &lt;code&gt;#pragma SIMD&lt;/code&gt; is forced in a couple of places where the compiler cannot necessarily figure out the loop count easily.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;test-setup&quot;&gt;Test Setup&lt;/h3&gt;
&lt;p&gt;The resulting program, &lt;code&gt;neutral-test&lt;/code&gt;, is called as follows to generate timing and performance information:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;export OMP_NUM_THREADS=4; time ./neutral-test --popsize 10000 --numloci 5 --inittraits 6 --innovrate 0.0001 --simlength 25000 --debug 1 --ruletype wfia&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The population size is large enough to generate opportunities both for vectorization and effective use of OpenMP parallel for loops, and the simulation length is long enough to create stable timing.&lt;/p&gt;
&lt;h3 id=&quot;no-optimizations&quot;&gt;No Optimizations&lt;/h3&gt;
&lt;p&gt;If we turn off all optimizations (using &lt;code&gt;-O0 -no-vec&lt;/code&gt; and not passing &lt;code&gt;-openmp&lt;/code&gt; to the Intel compiler), we get the straight line, single core, un-vectorized performance:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;real    0m42.556s
user    0m42.176s
sys     0m0.178s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the vast majority of time is spent in user code, mostly in tight loops, or in the C++11 library (which isn’t kernel code), and that kernel code occupies a tiny fraction of execution time (mostly some I/O at the beginning and end since I have logging turned way down). Real time and user time is basically the same, showing that we are totally single-threaded.&lt;/p&gt;
&lt;h3 id=&quot;vectorization-without-parallelism&quot;&gt;Vectorization Without Parallelism&lt;/h3&gt;
&lt;p&gt;If we turn off OpenMP parallelism, but enable vectorization and aggressive compiler optimization (&lt;code&gt;-O3&lt;/code&gt;), we see dramatic improvement:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;real    0m11.765s
user    0m11.666s
sys     0m0.045s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a speedup of 3.62x, simply by allowing the compiler to vectorize code in a situation where memory access has been designed for easy vectorization (i.e., unit stride access).&lt;/p&gt;
&lt;h3 id=&quot;parallelism-without-vectorization&quot;&gt;Parallelism Without Vectorization&lt;/h3&gt;
&lt;p&gt;If we enable OpenMP parallelism but turn off vectorization, we allow the use of multiple cores for copying tasks, although by the nature of this code base not all loops should be handled in parallel. The code is compiled with &lt;code&gt;-O3 -openmp -no-vec&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;real    0m10.370s
user    0m39.686s
sys     0m0.918s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we get 4.1x speedup, and we can see the effect of parallelism, where the user portion of code executes for 3.9x the wall clock time. So there is real benefit from parallelism here, and I’m exploring whether more of the loops can be parallelized.&lt;/p&gt;
&lt;h3 id=&quot;parallelism-and-vectorization&quot;&gt;Parallelism and Vectorization&lt;/h3&gt;
&lt;p&gt;Finally, I enable everything (&lt;code&gt;-O3 -openmp&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;real    0m10.211s
user    0m39.313s
sys     0m0.811s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The improvement with both is tiny over either approach alone. But this may be due to the small problem size.&lt;/p&gt;
&lt;h3 id=&quot;population-size-100000&quot;&gt;Population Size 100,000&lt;/h3&gt;
&lt;p&gt;If we switch to a larger population size, here are the results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;No optimization - 6m38.924s

Vec w/o Parallelism  - 2m4.405s

Parallel w/o Vec   -   1m54.437s

Vec and Parallelism  - 1m36.847s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see the separate contributions of vectorization and parallelism. The overall speedup is constant at 4.1x for both parallelism and vectorization, but this speedup is only achieved by having both. With the larger problem size, vectorization alone achieves a 3.2x speedup, and parallelism alone about 3.5x. As problem size grows, the importance of combining both approaches only grows (an intermediate problem size of 50K population size has intermediate results, for example).&lt;/p&gt;
&lt;p&gt;The biggest challenge in optimizing this kind of simulation code, it seems, is handling random number generation, which now seems to be the major hotspot. It is also complex enough that including a random generator call in a loop is sufficient to disqualify the loop from vectorization. It is also not clear whether it is safe to use the C++11 random library from multiple threads without locking, and introducing explicit thread-local generators obviates some of the benefit of having OpenMP automatically do thread management and decide the optimal threading strategy.&lt;/p&gt;
&lt;p&gt;The obvious way around this is to generate pools of random variates before going into a loop nest, so that the random variates are simply an array access like any other and thus can be used in vectorized, OpenMP parallelized loops. The current code reflects this strategy for determining the individuals who will be cultural models (targets of copying) in each generation.&lt;/p&gt;
&lt;h3 id=&quot;parallelized-random-number-generation&quot;&gt;Parallelized Random Number Generation&lt;/h3&gt;
&lt;p&gt;Creating 100K random variates is still a fairly time-consuming step, and this may be an area where using a GPU (if available) is a good idea, since at least CUDA (if present) has a high performance random number library. I hate to have hardware-specific code, though, especially if Apple moves away from NVIDIA cards in the Macbook Pros, which makes it hard to develop while mobile or away from home.&lt;/p&gt;
&lt;p&gt;There’s no good reason why I shouldn’t be able to parallelize random number generation using OpenMP, however, as long as each thread has its own private engine variable, which is accomplished by initializing the engine and distributions inside the parallel region, instead of during object construction:&lt;/p&gt;
&lt;pre class=&quot;sourceCode cpp&quot;&gt;&lt;code class=&quot;sourceCode cpp&quot;&gt;&lt;span class=&quot;ot&quot;&gt;#pragma omp parallel shared(test_par)&lt;/span&gt;
{
        std::random_device rd;
        std::mt19937_64 eng(rd());
        std::uniform_int_distribution&amp;lt;&lt;span class=&quot;dt&quot;&gt;int&lt;/span&gt;&amp;gt; uniform_int{&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, test - &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;};
        num_threads = omp_get_num_threads();

        &lt;span class=&quot;ot&quot;&gt;#pragma omp for&lt;/span&gt;
        &lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt;(i = &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;; i &amp;lt; test; i++) {
                test_par[i] = uniform_int(eng);
        }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result with 100K individuals and 25K generations (comparable to the above timing experiments) is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;31.24 real       119.92 user         3.02 sys&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is a 12.76x speedup over the serial unvectorized code. I’m not likely to push the performance experiments too much further, since I have a lot of other code to write, but it was a good experiment and now I have code modules to use in future simulations where I need the performance. And, others are encouraged to start with it and adapt it to your own transmission simulations, since it’s known to work and work quickly.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="cultural transmission" /><category term="simulation" /><category term="computing" /><category term="HPC" /><category term="computational science" /><summary>Background</summary></entry><entry><title>Performance of simuPOP in SeriationCT Across Platforms</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/07/27/simupop-performance-in-seriationct.html" rel="alternate" type="text/html" title="Performance of simuPOP in SeriationCT Across Platforms" /><published>2015-07-27T00:00:00-07:00</published><updated>2015-07-27T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/07/27/simupop-performance-in-seriationct</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/07/27/simupop-performance-in-seriationct.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mmadsen/seriationct&quot;&gt;SeriationCT&lt;/a&gt; uses the &lt;a href=&quot;http://http://simupop.sourceforge.net/&quot;&gt;simuPOP&lt;/a&gt; framework, building custom actions on top of it but leveraging the excellent support Bo Peng has created for transmission modeling. simuPOP has support for running across multiple threads or cores, but this support has been disabled lately on Mac OS X given problems with OpenMP and GCC 4.2.1 (according to Bo).&lt;/p&gt;
&lt;p&gt;Along with Bo, I’ve noted that OS X has excellent performance even without it, exceeding performance on Linux by a reasonable factor (sometimes even 2x).&lt;/p&gt;
&lt;p&gt;Now that I’m running on my &lt;a href=&quot;http://notebook.madsenlab.org/essays/2015/07/07/new-directions-ct-computing.html&quot;&gt;new simulation system&lt;/a&gt;, a 10-core Xeon E5-2650 V3 system equipped with a Xeon Phi 5110P coprocessor, I wanted to see what level of thread use in simuPOP provided the best performance.&lt;/p&gt;
&lt;p&gt;The test setup runs the following simulation as the only active code on the system, apart from the MongoDB database which is serving as data repository. I ran this exact simulation with the same seed across thread count from 1 to 20, and then repeated this loop several times with different seeds, to generate multiple timing values for each thread count. The whole process was duplicated for 5 replicates and 10 replicates.&lt;/p&gt;
&lt;pre class=&quot;shell&quot;&gt;&lt;code&gt;[mark@ewens sctcal-1]$ sim-seriationct-networkmodel.py  --dbhost localhost --dbport 27017 --experiment sctcal-1 --popsize 250 --maxinittraits 5 --numloci 3 --innovrate 8.86017736123e-05 --simlength 12000 --debug 0 --seed 166602270 --reps 1 --samplefraction 0.5 --migrationfraction 0.0517857031302 --devel 0 --networkmodel networks/two-node-three-slice.zip --cores 10&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;single-replication&quot;&gt;Single Replication&lt;/h3&gt;
&lt;p&gt;The results are shown below, with a loess smoothed trend line for each half of the graph. On the left, thread count is less than the number of physical cores, and it appears that threads are distributed among cores with scatter affinity (confirmed by observation in htop). On the right, hyperthreading is used to assign up to 2 threads per physical core, which is within the design spec of the E5-2650V3 processor. Some quick tests using 25 and 30 threads show that execution time shoots up precipitously given that execution is largely CPU-bound, and those data points are excluded in order to focus on the fine detail.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/simupop-perf-by-core-web.png&quot; alt=&quot;Figure 1: Execution Time Given Threads Used&quot; /&gt;&lt;figcaption&gt;Figure 1: Execution Time Given Threads Used&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Immediately apparent is that the process really is CPU-bound, and that the code performs best with no hyperthreading, and a single thread per core. The strong increase around 18-20 threads is indicative of contention between the simulation threads and the MongoDB database which serves as the data repository, it looks like.&lt;/p&gt;
&lt;p&gt;On the left, it is interesting that the absolute best performance is obtained by configuring simuPOP with a single thread, even with 10 physical cores present. Performance immediately worsens with 2 or more threads, before improving slightly between 8 and 10 threads. I have no explanation for why performance should improve, but clearly a single replication runs fastest when given a single execution thread, and the attempt to parallelize a single replication actually worsens the situation. I need to look at the simuPOP source and see why this occurs.&lt;/p&gt;
&lt;h3 id=&quot;performance-with-multiple-replicates&quot;&gt;Performance with Multiple Replicates&lt;/h3&gt;
&lt;p&gt;I also ran tests with the same simulation command line, but with 5 replicates. Sometimes I use replicates with simuPOP, usually when I have a fixed grid of parameters and want to understand variability given the parameter space. Replicates are independent copies of a population, each evolving separately according to the same evolution schedule and operators, so this would seem to be a logical point at which parallelism would be leveraged.&lt;/p&gt;
&lt;p&gt;This experiment I ran only a single series, given the longer execution times. The results are shown in Figure 2.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/simupop-perf-by-core-5rep-web.png&quot; alt=&quot;Figure 2: Execution Time for 5 Replicates Given Threads Used&quot; /&gt;&lt;figcaption&gt;Figure 2: Execution Time for 5 Replicates Given Threads Used&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The pattern is the same, and the execution times are basically 5x the single replication time.&lt;/p&gt;
&lt;h3 id=&quot;parallelism-in-simupop&quot;&gt;Parallelism in simuPOP&lt;/h3&gt;
&lt;p&gt;Looking at the version 1.1.6 source code, it appears that multiple threads are used in very specific situations. The simulator and its evolve function are single-threaded, with no OpenMP sections. Instead, parallelism is employed in specific loops, such as initializing a population or processing the individuals in a subpopulation, as in this example:&lt;/p&gt;
&lt;pre class=&quot;sourceCode cpp&quot;&gt;&lt;code class=&quot;sourceCode cpp&quot;&gt;        &lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt; (; sp != sp_end; ++sp) {
                pop.activateVirtualSubPop(*sp);
                size_t numValues = m_values.size();
                &lt;span class=&quot;kw&quot;&gt;if&lt;/span&gt; (numThreads() &amp;gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; &amp;amp;&amp;amp; !values.empty()) {
&lt;span class=&quot;ot&quot;&gt;#ifdef _OPENMP&lt;/span&gt;
&lt;span class=&quot;ot&quot;&gt;#  pragma omp parallel firstprivate (idx)&lt;/span&gt;
                        {
                                size_t id = omp_get_thread_num();
                                IndIterator ind = pop.indIterator(sp-&amp;gt;subPop(), id);
                                idx = idx + id * (pop.subPopSize(sp-&amp;gt;subPop()) / numThreads());
                                &lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt; (; ind.valid(); ++ind, ++idx)
                                        &lt;span class=&quot;kw&quot;&gt;for&lt;/span&gt; (size_t i = &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;; i &amp;lt; infoIdx.size(); ++i) {
                                                ind-&amp;gt;setInfo(values[idx % numValues], infoIdx[i]);
                                        }
                        }
                        idx = idx + pop.subPopSize(sp-&amp;gt;subPop());
&lt;span class=&quot;ot&quot;&gt;#endif&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where tiles of individuals are processed by each of the N threads configured.&lt;/p&gt;
&lt;p&gt;Examining the simulator itself, I can see why parallelism isn’t expressed at the replicate or subpopulation level. Applications built in simuPOP are capable of using pure-Python operators mixed with built-in C++ operations, and if replicates are threaded rather than being distributed across Unix processes, we run immediately into the GIL issue. So Bo has used OpenMP to parallelize things only &lt;strong&gt;within&lt;/strong&gt; a single operator or function, which makes it safe.&lt;/p&gt;
&lt;p&gt;What does this mean?&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;I’m probably not getting any speedup because the population size in my current simulation is small – 250 individuals – rather than the thousands to millions that one typically sees in genetics work. This means that the OpenMP and thread management overhead is dominating, and the more threads I use, the worse this gets. So, with small population sizes, the guideline here is &lt;strong&gt;always use a single thread&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since simuPOP never parallelizes over replicates, it’s better to keep single replicates in their own Unix processes and use Grid Engine or a job scheduler to parallelize a &lt;strong&gt;set of simulations&lt;/strong&gt; across the available cores.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The latter is what I’ve been doing on StarCluster in previous projects, so it’s a familiar and easily scriptable protocol. And, if I work with large population sizes at some point, the native threading in simuPOP could pay off, but otherwise, it’s best ignored for most of the work I’m doing.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;
&lt;h3&gt;References Cited&lt;/h3&gt;
&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><summary>Background</summary></entry><entry><title>Changing Strategy for Cultural Transmission Computing</title><link href="http://notebook.madsenlab.org/essays/2015/07/07/new-directions-ct-computing.html" rel="alternate" type="text/html" title="Changing Strategy for Cultural Transmission Computing" /><published>2015-07-07T00:00:00-07:00</published><updated>2015-07-07T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/essays/2015/07/07/new-directions-ct-computing</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/2015/07/07/new-directions-ct-computing.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;For the last two or three years, my computing strategy for simulations (and post-simulation analysis) of cultural transmission models has been as follows:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Develop simulation code in Python, sometimes with a framework which has C++ extensions for speed.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Test code on my laptop (quad-core, OS X), and occasionally make performance changes.&lt;/li&gt;
&lt;li&gt;Run production analyses and large parameter sweeps on clusters of Amazon EC2 instances, gathering results in a database.&lt;/li&gt;
&lt;li&gt;Do final analyses locally on my laptop unless they, too, needed a very large machine or a cluster.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This isn’t a bad strategy:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Python is usually fast enough.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Using EC2 means that I don’t have to manage extra hardware, and I’m not worried about investing in hardware that goes obsolete quickly.&lt;/li&gt;
&lt;li&gt;Using EC2 also means that I can select the right &lt;strong&gt;amount&lt;/strong&gt; of hardware for problem, and only pay for what I use.&lt;/li&gt;
&lt;li&gt;Using straight Python has meant that development velocity is fast, given the forgiving nature of a dynamically typed scripting language and the huge library support.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The negatives been cost and performance. On average, I’ve been spending anywhere from $250 - $600 a month when I’m doing heavy analyses, and nothing sucks worse than having a cluster of 8 big compute-optimized instances grind away for a full week and then discover that you really needed to do the analysis a bit differently (and that you just burned a hundred bucks or more on CPU time that contributes nothing to the final result).&lt;/p&gt;
&lt;p&gt;The bigger problem has been the “cycle time” on simulation experiments. When it takes a week or two weeks to go from a new simulation code to a set of results one can assess, it can take months to go from an idea to a full set of results. This is just too long, especially as I try to deal with the last, and hardest, element of my dissertation research. I’d like to reduce this cycle time so that I can test more ideas, so that false starts aren’t so costly, and because the search space for the problem I’m addressing is huge, so I need to be able to do large samples.&lt;/p&gt;
&lt;p&gt;I managed to do 27 different numerical experiments in about two months for a conference last winter, where most of the iteration time involved doing a large number of seriations from each round of simulation experiments. While further scripting and automation will help with cycle time, I also need to simply improve computational throughput on seriation and analyses (and secondarily, on the primary CT simulations themselves, although this was less important in the cycle time).&lt;/p&gt;
&lt;h3 id=&quot;improving-cost&quot;&gt;Improving Cost&lt;/h3&gt;
&lt;p&gt;Improving my costs should be easy, over time. Routinely, I’ve been using EC2 instances with 8 or 16 cores so I can run multiple simulations, or give our seriation algorithms enough cores to test possible solutions in parallel. Having such a system inhouse will be cheaper than using EC2 for the majority of my work if I keep it working for at least 18-24 months, given what I’m averaging on EC2 right now, and if I don’t need to “burst” higher with EC2 very often. On a couple of occasions, I’ve used clusters of instances with 64 total cores to handle a large batch of simulations that required long run times.&lt;/p&gt;
&lt;p&gt;So I set out with the goal to buy one mid-sized system with enough computing power to allow me to run most jobs locally, and save EC2 computing for occasional large tasks.&lt;/p&gt;
&lt;h3 id=&quot;many-core-architecture&quot;&gt;Many Core Architecture&lt;/h3&gt;
&lt;p&gt;It’s easy to spec out an 8 core Xeon system, and put a large gamer/prosumer grade NVIDIA GPU into it, and call that a good development system. Except that it’s not clear that the GPU programming model will help me much, although there are certainly areas where being able to offload large matrix calculations to a GPU (using NumbaPro from the &lt;a href=&quot;http://continuum.io&quot;&gt;Anaconda Python folks at Continuum.io&lt;/a&gt;, for example) might help with the fitting of statistical models. GPU programming relies upon a very specific computing model, called “SPMD”, an extension of the general SIMD model for vectorization. In SPMD, a large calculation is broken down into an elemental function which operates on a single cell or small group of cells, performing exactly the same calculation (or “kernel”) on each, which are then combined by the runtime into the result for an entire data block.&lt;/p&gt;
&lt;p&gt;This model is perfect for graphics computation, of course, given the origins of the GPU programming model. But it’s terrible for general parallel programming, and worse than the host CPU for general combinations of serial and parallel code. And unfortunately, most simulation programming in the social sciences doesn’t fit neatly into the SPMD/GPU model. Code which contains conditional branches will block the lockstep flow of computation across all of the compute cores in a GPU, and yield performance even slower than the host processor in many cases. And even if I can rethink the way my simulation codes work, and eliminate if/then branching logic in favor of masked pipelines of small kernels run in sequence, the redevelopment time would be long and there’s no guarantee of a major speedup.&lt;/p&gt;
&lt;p&gt;But we live in interesting times, and Intel’s recent MIC architecture (embodied in the Xeon Phi coprocessor) offers a different path. The current generation of the Phi (the first production generation, code named “Knight’s Corner”) started shipping to general customers in 2012 and 2013, and are now the cornerstone of the fastest supercomputer on record, &lt;a href=&quot;https://en.wikipedia.org/wiki/Tianhe-2&quot;&gt;Tianhe-2&lt;/a&gt;, which has 16K compute nodes, each of which has two Xeon processors and 3 Phi cards.&lt;/p&gt;
&lt;p&gt;The Phi cards shipping today offer an embedded Linux subsystem, with 61 cores capable of over 1 teraflops/sec of double precision floating point performance (if you can feed it enough data smoothly enough). This means that a system with a single Xeon host processor and a Xeon Phi 5110P card, has the same floating point performance as the ASCI Red supercomputer which topped the Top500 supercomputer list in 1999 and 2000. The computing power of a US National Laboratory ten years ago, but it fits on your desktop.&lt;/p&gt;
&lt;p&gt;Naturally, I ordered exactly this configuration, and am waiting for &lt;a href=&quot;http://www.pugetsystems.com&quot;&gt;Puget Systems&lt;/a&gt; to deliver it this week or next. The best part? Right now, &lt;a href=&quot;https://www.pugetsystems.com/labs/articles/Xeon-Phi-5110p-and-Free-Intel-Parallel-Studio-Cluster-Edition-670/&quot;&gt;a developer program with Intel will get you a Xeon Phi 5110P&lt;/a&gt; coprocessor card for less than $500, along with a year’s license for the Intel compiler stack and cluster computing tools, which normally are fairly expensive (although in the same realm as &lt;strong&gt;Mathematica&lt;/strong&gt; for academic use). The downside is that the passively cooled Phi cards need some extra cooling and good system design and testing, unless you simply want to cook a $4-5000 computer down to slag fairly quickly. That’s where Puget Systems and their expertise come in – sizing the power, selecting and testing the cooling system, and making sure that you’re good to go. Even if you pay them a small premium for integrating the box, the work they’re doing is impressive and I think it’s going to be well worth it.&lt;/p&gt;
&lt;p&gt;How to use it?&lt;/p&gt;
&lt;h3 id=&quot;improving-parallelism-and-code-performance&quot;&gt;Improving Parallelism and Code Performance&lt;/h3&gt;
&lt;p&gt;The Phi can augment the host processor in several ways, since it runs a full (but basic) Linux distribution. In “native” mode, you can compile code for the coprocessor, for example using OpenMP to parallelize tasks across the cores, and run code against 61 cores as long as the memory present on the Phi itself (8 or 12GB) is adequate. Which it might very well be, for code which deals with small data (common in agent-based simulation models), but has complex branching structure or algorithms which run on each core.&lt;/p&gt;
&lt;p&gt;The second mode is “offload” mode, in which your code runs on the host processor, but certain operations can be parallelized onto the coprocessor for a boost. Here’s where it gets interesting. It might be possible to offload directly from Python code, and to offload NumPy expressions directly to the Phi with the main program running on the host processor (or across many of the host cores). NumbaPro and OpenCL are paths to this type of offloading, and even if not done explicitly, if your copy of NumPy is linked against the Intel MKL math libraries on Linux, offloading can be done at the library level even if Python is unaware of it.&lt;br /&gt;The latter offers the possibility of offloading work from libraries like scikit-learn for machine learning and statistical algorithms, even if the ML library itself is coprocessor-unaware.&lt;/p&gt;
&lt;p&gt;And if you use R, switch to the RevolutionR Open version of the runtime, which is linked against the Intel MKL math libraries. Again, it may be possible to have R use the Xeon Phi coprocessor for algoritms which involve larger NumPy calculations without modifications to your R code.&lt;/p&gt;
&lt;p&gt;See, it’s getting very interesting, isn’t it?&lt;/p&gt;
&lt;p&gt;I’m not sure how much of this automatic offloading will work “off the shelf” from Python, but given NumbaPro from the Anaconda folks, I don’t imagine it’ll be long before all of it will. And &lt;em&gt;anything&lt;/em&gt; you write in C++ can be made to use the coprocessor immediately, so compute-critical sections in your Python code can always be linked via Cython or SWIG.&lt;/p&gt;
&lt;p&gt;The future looks even more interesting. The next generation of Phi chips starts shipping commercially in 2016, so I didn’t go all out this time. Next year, the “Knight’s Landing” architecture will put the Phi in motherboard sockets, eliminating much of the time needed to move data over the PCI bus to the coprocessor card, and yielding a true 75 or 150 core server system. Pair one of those systems with a box running 1 or 2 “normal” Haswell/Broadwell Xeon processors, and you’ve got a small 160-175 core cluster in the space it used to take for a couple of workstations.&lt;/p&gt;
&lt;h3 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h3&gt;
&lt;p&gt;So I’m bullish on the opportunity to drastically speed up cultural transmission simulations, seriation analyses, the use of “Approximate Bayesian Computation” for statistical inference on these complex models, and even phylogenetic analysis codes, in fairly short order.&lt;/p&gt;
&lt;p&gt;Which is good, because the social sciences are getting more and more computationally complex, and we have less and less funding for buying time on large clusters or renting time from major cloud services.&lt;/p&gt;
&lt;p&gt;I’ll report back when I have some code up and running.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;

&lt;/div&gt;</content><category term="cultural transmission" /><category term="simulation" /><category term="computing" /><category term="HPC" /><category term="seriation" /><category term="ML" /><category term="computational science" /><summary>Background</summary></entry><entry><title>SeriationCT Sample Size Series experiments</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/05/14/seriationct-sample-size-experiments.html" rel="alternate" type="text/html" title="SeriationCT Sample Size Series experiments" /><published>2015-05-14T00:00:00-07:00</published><updated>2015-05-14T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/05/14/seriationct-sample-size-experiments</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/05/14/seriationct-sample-size-experiments.html">&lt;h3 id=&quot;effect-of-varying-sample-size&quot;&gt;Effect of Varying Sample Size&lt;/h3&gt;
&lt;p&gt;After SAA’s, I used some existing simulations (seriationct-27 from the &lt;a href=&quot;http://github.com/mmadsen/experiment-seriationct-2&quot;&gt;experiment-seriationct-2&lt;/a&gt; repository) to prototype examining the effects of sample size on:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;The gross structure of the seriation solutions: how much branching when small? do we get isolates?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Lineage structure recovery&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Working with the existing simulation results and post-processed samples, I wrote a quick modification of the assemblage sampler called &lt;code&gt;seriationct-sample-assemblages-for-samplesize-sequence-seriation.py&lt;/code&gt;. The script takes the output from the &lt;code&gt;seriationct-export-data.py&lt;/code&gt; script and performs subsampling as follows:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Uses a &lt;code&gt;samplefraction&lt;/code&gt; parameter to select an initial sample of assemblages, just like the normal &lt;code&gt;seriationct-sample-assemblages-for-seriation.py&lt;/code&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Selects these samples given the &lt;code&gt;sampletype&lt;/code&gt; parameter, as a pure random sample of assemblages, a spatially stratified sample given NxN quadrats, temporally stratified given N even periods of time, and spatiotemporal sampling which stratifies by quadrats and periods.&lt;/li&gt;
&lt;li&gt;Subsamples the &lt;code&gt;samplefraction&lt;/code&gt; assemblages in a sequence decreasing by 2, so for example, if &lt;code&gt;samplefraction&lt;/code&gt; is 30, the largest set of assemblages will be 30 randomly sampled, and then 28 sampled from the 30, 26 sampled from the original 30, and so on.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The net result is a nested series of samples (rather than independent random samples of different sizes).&lt;/p&gt;
&lt;h3 id=&quot;statistics-brainstorm&quot;&gt;Statistics Brainstorm&lt;/h3&gt;
&lt;p&gt;Some of the things I want to know are:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;How consistent is the lineage structure of each seriation solution, at each sample size? (i.e., along a branch, how mixed are lineage identifiers?)&lt;/li&gt;
&lt;li&gt;How consistent is the temporal sequence of the solution?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;How “branchy” is the solution?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Furthermore, what is the average and variance of these measures, across the sequence of sample sizes? Or perhaps the quantiles and order statistics?&lt;/li&gt;
&lt;li&gt;At what sample size does lineage structure and chronology become visible and semi-stable?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I’m just starting to build measures for these items, since they involve traversal and parsing of the annotated minmax graphs.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;
&lt;h3&gt;References Cited&lt;/h3&gt;
&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><summary>Effect of Varying Sample Size</summary></entry><entry><title>SeriationCT Next Steps</title><link href="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/04/27/seriationct-next-steps.html" rel="alternate" type="text/html" title="SeriationCT Next Steps" /><published>2015-04-27T00:00:00-07:00</published><updated>2015-04-27T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/04/27/seriationct-next-steps</id><content type="html" xml:base="http://notebook.madsenlab.org/project:coarse%20grained%20model/model:seriationct/experiment:experiment-seriationct/2015/04/27/seriationct-next-steps.html">&lt;h3 id=&quot;network-models&quot;&gt;Network Models&lt;/h3&gt;
&lt;p&gt;Prior to the SAA conference in SFO this April, I performed 28 different network model experiments (seriationct-1 through seriationct-28). Each had a different starting temporal network model, generated by one of two network generator programs. The early experiments (through 12 or 15 or so) used an initial cut at network modeling, which was too constrained in various ways. I was not able to recovery what I thought was the structure of the network.&lt;/p&gt;
&lt;p&gt;Subsequently, I simplified the network generator into two executables. It now produces M clusters of N communities. Each community is fully connected to other communities in the same cluster, and in addition, there is a small fraction of communities that are connected between clusters. In addition, each community in the model lasts for a single network slice, before being replaced by another slice of communities, each of which has a randomly sampled parent in the previous time interval.&lt;/p&gt;
&lt;p&gt;In the first program, there is a single “lineage” with the cluster structure just described. The linkage between clusters represents the only mesoscopic structure. I did not get clear recovery of this structure, at least with the number of loci/classes, innovation rate, and migration rates tested.&lt;/p&gt;
&lt;p&gt;In the second program, I produced two models: lineage splitting, and lineage coalescence. In the former, at early times, M clusters of weakly interconnected communities evolve to a splitting time, at which point the single lineage of M clusters splits into L lineages of &lt;span class=&quot;math&quot;&gt;\(M/L\)&lt;/span&gt; clusters of communities each, and continues evolving with weak interconnections within the lineage but NO edges between lineages, until the stop time. The lineage coalescence is a mirror image, starting with L lineages and coalescing them into a single lineage.&lt;/p&gt;
&lt;h3 id=&quot;lineage-splittingmerging-experiments&quot;&gt;Lineage Splitting/Merging Experiments&lt;/h3&gt;
&lt;p&gt;With the lineage split/merge model, the idea is to use seriation to recover the major lineage structure of the model.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/seriationct-27-merge.png&quot; alt=&quot;Figure 1: Example of lineage coalescence&quot; /&gt;&lt;figcaption&gt;Figure 1: Example of lineage coalescence&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The annotation convention used is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shape represents the lineage, with the “parent” or “child” lineage coded as circles (i.e., presplit or post-coalescence), and squares/diamonds/other polygons used to denote separate lineages.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Color denotes cluster membership&lt;/li&gt;
&lt;li&gt;Shape rim thickness indicates time, with thin equalling early samples, and thick indicating late.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Fig. 1, the lineage structure is evident. Two lineages on the right begin early, and come together with some circles to form a single lineage, which continues to the left with thicker shapes. There is some confusion around the branch point, with some later assemblages mixed into the terminal end of the earlier branches. But in general this is darned good.&lt;/p&gt;
&lt;p&gt;The difficulty is that only some samples from a given network model/CT simulation yield clear structure. Some don’t at all, even though they’re samples from the same underlying data set. There are several possibilities for why this would happen:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;It’s difficult to recover this kind of structure, and the nature of your sample of assemblages is important. If you’re missing some key assemblages from a key time period, you may not be able to thread together the changes.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Some of my seriation settings for continuity thresholds, for example, might be too strict given the gaps between sampled assemblages. With more assemblages, the lineage structure might be obvious with a given threshold level, but with smaller samples the gaps will be wider and thus it’s harder to put together certain combinations that actually give us the lineage structure we seek.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Of course, both can be occurring. Now it’s time to figure out what’s going on.&lt;/p&gt;
&lt;h3 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h3&gt;
&lt;p&gt;The next steps are to focus on tweaking the factors just listed and seeing what encourages robust lineage structure recovery. We need:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;A method of telling whether an annotated minmax graph displays the lineage structure from a given ITN (as given by a zip of the GML slices). Given a constrained set of lineage structures, this should not be difficult to assess if I can get a robust way of assessing the number of major branches, and compare these with time direction.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Construct a master XY file for the simulated experiments (we need to retain a real XY file since empirical examples will have nonarbitrary coordinates), so that we can run some of the post-processing scripts batching together different network models into a pipeline – currently there is one point where doing this is hazardous, noted in the debug output.&lt;/li&gt;
&lt;li&gt;Record the network model zipfile to be used for a given simulation run’s output, again to allow us to batch output coming from multiple network models into a pipeline.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Consider using a work queue model to drive the pipeline on a cluster, so that batches can truly run without intervention.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Consider databasing all the intermediate processing status?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Definitely database the results of comparing the annotated minmax graph to the generating network model, along with all the parameters needed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This will allow a large batch with replicates, to see where we get good, reliable recovery, and where we don’t.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;
&lt;h3&gt;References Cited&lt;/h3&gt;
&lt;/div&gt;</content><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><summary>Network Models</summary></entry></feed>
