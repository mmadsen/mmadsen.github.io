<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://notebook.madsenlab.org/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.4.0">Jekyll</generator><link href="http://notebook.madsenlab.org/feed.xml" rel="self" type="application/atom+xml" /><link href="http://notebook.madsenlab.org/" rel="alternate" type="text/html" /><updated>2017-02-12T13:09:15-08:00</updated><id>http://notebook.madsenlab.org//</id><title type="html">Lab Notebook for Mark E. Madsen</title><author><name>Mark E. Madsen</name></author><entry><title type="html">Human Behavior and Evolution Society 2016 Talks</title><link href="http://notebook.madsenlab.org/essays/2016/06/30/HBES-papers.html" rel="alternate" type="text/html" title="Human Behavior and Evolution Society 2016 Talks" /><published>2016-06-30T00:00:00-07:00</published><updated>2016-06-30T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/essays/2016/06/30/HBES-papers</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/2016/06/30/HBES-papers.html">&lt;p&gt;I participated in two papers today at the 2016 meetings of the Human Behavior and Evolution Society, in Vancouver, B.C. The first was solo work, titled “Computational Methods for Identifying Metapopulation Interaction Patterns From Seriation Solutions.” The &lt;a href=&quot;https://figshare.com/articles/madsen2016-hbes-computational-interaction-patterns-slides_pdf/3468650&quot;&gt;presentation slides are on Figshare now.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The second paper was jointly done with Carl P. Lipo, and comprises work on new variations on seriation methods. The title is: “Continuity-based approaches to seriation and the study of patterns of cultural inheritance”. The &lt;a href=&quot;https://figshare.com/articles/lipomadsen2016-hbes-continuity-inheritance_pdf/3468653&quot;&gt;presentation slides are on Figshare now.&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark E. Madsen</name></author><category term="dissertation" /><category term="research" /><category term="cultural transmission" /><category term="coarse graining" /><category term="seriation" /><summary type="html">I participated in two papers today at the 2016 meetings of the Human Behavior and Evolution Society, in Vancouver, B.C. The first was solo work, titled “Computational Methods for Identifying Metapopulation Interaction Patterns From Seriation Solutions.” The presentation slides are on Figshare now.</summary></entry><entry><title type="html">Research Priorities for 2016</title><link href="http://notebook.madsenlab.org/essays/project-coarse%20grained%20model/2016/04/10/post-saa2016-planning.html" rel="alternate" type="text/html" title="Research Priorities for 2016" /><published>2016-04-10T00:00:00-07:00</published><updated>2016-04-10T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/essays/project-coarse%20grained%20model/2016/04/10/post-saa2016-planning</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/project-coarse%20grained%20model/2016/04/10/post-saa2016-planning.html">&lt;p&gt;I am somewhat remiss in discussing research goals for the year, because of some family issues which have taken much of my time. But I’m on a flight back from Orlando from the Society for American Archaeology annual meetings, and I’ve accumulated notes and ideas over the last three days about where my research stands and what my next steps are. I also want to evaluate how I did in addressing the &lt;a href=&quot;/essays/2015/01/01/research-priorities-2015.html&quot;&gt;priorities I set for 2015&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Overall, I got a lot more research done than expected given other responsibilities, but a lot less writing, which I suppose is to be expected. I was able to tuck bits of work in between caregiving duties, while it’s much harder to find blocks of time where I can write, with all of the necessary materials at hand. That remains a challenge this year, and one I need to fix since my time will be impacted on an ongoing basis.&lt;/p&gt;
&lt;p&gt;I started the year understanding the nature of the final conceptual bits of my project to infer metapopulation structure (in terms of cultural transmission patterns) from diachronic seriation solutions, and at this point (early April 2016), the concepts and connections are firming rapidly into an analytic method and a set of applications. That feels really good. I’ve given an exploratory talk about this in &lt;a href=&quot;/essays/2016/03/22/evos-seminar-series-binghamton.html&quot;&gt;Binghamton recently in my EVoS seminar&lt;/a&gt;, and a larger sample of models and more sophisticated analysis will be the subject of a talk at the &lt;a href=&quot;http://www.hbes.com/hbes2016&quot;&gt;Human Behavior and Evolution Society annual meeting&lt;/a&gt; in Vancouver in late June.&lt;/p&gt;
&lt;h3 id=&quot;contents&quot;&gt;Contents&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Seriation and temporal network models&lt;/li&gt;
&lt;li&gt;Parallel seriationct processing completed&lt;/li&gt;
&lt;li&gt;Network models for seriationct finalization&lt;/li&gt;
&lt;li&gt;Likelihood of empirical seriation WRT models&lt;/li&gt;
&lt;li&gt;Seriation of additional data sets&lt;/li&gt;
&lt;li&gt;Relation between seriation and cladistics&lt;/li&gt;
&lt;li&gt;Continuity HBES paper&lt;/li&gt;
&lt;li&gt;SeriationCT HBES paper&lt;/li&gt;
&lt;li&gt;Future questions&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;seriation-and-temporal-network-models&quot;&gt;Seriation and Temporal Network Models&lt;/h3&gt;
&lt;p&gt;The basic question I’m addressing is &lt;strong&gt;whether we can use diachronic seriation solutions, which map trait similarity across both space and time, to infer something of the topology of the temporal network formed (conceptually) by the changing interaction strength between past communities, where “interaction strength” refers to the intensity with which people migrated between communities or engaged in social learning with individuals outside their local subpopulation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In prototype, the answer appears to be &lt;strong&gt;YES&lt;/strong&gt;. Metapopulation interaction structures that have significantly different topologies for weighted edges in an &lt;em&gt;interval temporal network&lt;/em&gt; display different seriation structures, measured as the Laplacian eigenvalues of the seriation expressed as a graph (as we do in our IDSS software). I am testing differentiation of interaction network classes using a standard, high quality machine learning classification algorithm (e.g., gradient boosted trees or random forests).&lt;/p&gt;
&lt;p&gt;There is much unfinished business turning my early results into a solid body of scientific results and a general method. The next two priorities cover the “big” tasks, but I would be remiss if I didn’t mention the important element of ensuring that I am sampling and seriating over randomized instances of interaction network models, not just Monte Carlo samples of time averaged cultural transmission. Some networks have very little scope for randomization, such as the “panmictic” case where interaction is represented as a uniformly weighted complete temporal network (i.e., where the network at any point in time is &lt;span class=&quot;math inline&quot;&gt;\(K_n\)&lt;/span&gt;). Some randomization related to community/assemblage duration could be modeled by randomizing the choices of number of slices and total simulation length, and that probably needs to be done.&lt;/p&gt;
&lt;p&gt;I should also mention that randomization of the network model involved fully rewriting the post-simulation processing chain, such that we pass and use the correct network model to every step of the chain, and can associate parameters and info from each stage of the processing chain to downstream elements for analysis. That work is nearly complete (15 April target).&lt;/p&gt;
&lt;p&gt;The two weightier methodological issues are discussed in their own sections. Once complete, the main computational task is to develop a reference library of simulated seriations resulting from the chosen suite of network models, across priors for both network parameters and CT simulation parameters.&lt;/p&gt;
&lt;h3 id=&quot;network-models-for-seriationct&quot;&gt;Network Models for SeriationCT&lt;/h3&gt;
&lt;p&gt;Right now I have the following interaction network models:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Lineage splitting or coalescence&lt;/li&gt;
&lt;li&gt;Complete network/panmixis&lt;/li&gt;
&lt;li&gt;Approximate nearest neighbor interaction&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This set has not been chosen because it represents the “right” set of models for any specific empirical case, but because I was developing ways of representing various topological characteristics (e.g., distance-respecting interaction, distance-insensitive interaction, large-scale splits in interaction or coalescence). It’s apparent to me that there are really two levels of topological features we might be able to examine:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Mesoscale connection variability: sparseness, evenness of interaction, and the decay of interaction with distance all speak to mesoscale connectivity&lt;/li&gt;
&lt;li&gt;Macroscale history: the history of lineage splitting and coalescence events which give us the structure we see at very large historical scales&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In a mature inference method, we need an ABC reference library of seriations that includes a good spectrum of mesoscale options, expressed in whatever set of macroscale options seem most likely given our gross-scale culture-historical knowledge or previous research.&lt;/p&gt;
&lt;p&gt;Thus, I am going to work on finalizing graph builders that incorporate:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Panmixis&lt;/li&gt;
&lt;li&gt;Nearest neighbor interaction with tunable small world links&lt;/li&gt;
&lt;li&gt;Hierarchical (2- and 3-tier) nearest neighbor interaction with tunable small world links&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of these graph builders should then have the ability, ultimately, to also implement a lineage split or coalescence “on top” of that mesoscale connectivity pattern.&lt;/p&gt;
&lt;h3 id=&quot;likelihood-of-empirical-seriations&quot;&gt;Likelihood of Empirical Seriations&lt;/h3&gt;
&lt;p&gt;Given a reference table of seriations (seriation graph Laplacian eigenvalues, more precisely), a classifier model will always give me an answer as to which interaction model a seriation belongs to. So it’s really the full ABC inference loop that will help us figure out which of the models might be “least wrong,” with the possibility that none are very close always open. I have explored the Euclidean distance/L2 loss between the empirical seriation and the eigenvalue spectra of reference table data points, and that will be the first criterion used, although I want to fully explore Pudlo and Robert’s &lt;span class=&quot;citation&quot; data-cites=&quot;pudlo2014abc&quot;&gt;(2014)&lt;/span&gt; suggestion that a two step random forest analysis could perform better than simpler rejection or threshold methods in this situation.&lt;/p&gt;
&lt;h3 id=&quot;relation-between-seriation-and-cladistics&quot;&gt;Relation Between Seriation and Cladistics&lt;/h3&gt;
&lt;p&gt;This is really just a downpayment on a note, but I talked with Carl Lipo a great deal this weekend about an idea I’ve been developing, that cladistics and seriation are really separated by a “level” (sensu Dunnell &lt;span class=&quot;citation&quot; data-cites=&quot;Dunnell1971&quot;&gt;(1971)&lt;/span&gt;) distinction. Frequency seriation, whatever the ordering algorithm, takes advantage of &lt;strong&gt;trait polymorphism&lt;/strong&gt; in the population to make ordering decisions, whereas standard phylogenetic methods tend towards presence/absence of binary or multivalued variables. Thus, phylogenetics operates at a coarser level of analysis (but not scale!) and makes coarser distinctions.&lt;/p&gt;
&lt;p&gt;Of course, there is a small literature on polymorphic characters in seriation, but it seems to die out and there are no packages I know of that use character frequencies in tree construction. If there were, those methods would be comparable to frequency seriation.&lt;/p&gt;
&lt;p&gt;So really, various types of occurrence or character-state seriation are comparable to cladistics, as “macroevolutionary” methods, and frequency seriation is “mesoevolutionary” at a finer level of analysis.&lt;/p&gt;
&lt;p&gt;TODO: discuss synapomorphies in cultural traits&lt;/p&gt;
&lt;h3 id=&quot;continuity-hbes-paper&quot;&gt;Continuity HBES Paper&lt;/h3&gt;
&lt;p&gt;We have a start on the continuity paper already, from SAA’s. The point of the empirical example in it right now is simply to show that we get the same answers when we examine frequency data with unimodality as the ordering criterion, compared to exact distance minimization. I think perhaps the point needs to shift to demonstrating how we can do much larger data sets with continuity seriation, which is crucial for truly understanding macroevolutionary patterning, while retaining the information about polymorphism that seriation employs (and cladistics generally does not, see previous section). This would be a good place to try to look at the LMV as a whole, adding Mainfort, PFG/Lipo, and anything else we have with enough sample size (it would be good, for example, to incorporate data from Greg Fox’s dissertation, from slightly further north in SE Missouri), and even look to seek if there are any data that would connect us up towards Cahokia.&lt;/p&gt;
&lt;h3 id=&quot;seriationct-hbes-paper&quot;&gt;SeriationCT HBES Paper&lt;/h3&gt;
&lt;p&gt;The goal is to actually give several empirical demonstrations, with brief descriptions of what interaction pattern (or patterns) are believed to hold in each case, and walk through the analysis to describe how it’s done, and show answers for 1-3 data sets. This might be two chunks of Mississippian that have different local interaction patterns, and the Woodland data. I need to think more carefully about the difference between interaction expectations for Woodland vs. Late Prehistoric, since the dispersed vs nucleated issue will bear on what suite of models we examine for each. I may not be at a point where I can develop the reference library of seriation data for Woodland dispersed communities yet, and perhaps I need to focus on 1-3 different late expressions.&lt;/p&gt;
&lt;h3 id=&quot;future-questions&quot;&gt;Future Questions&lt;/h3&gt;
&lt;p&gt;These are questions which arise when we describe the regional structure of social learning and cultural transmission using interval temporal networks as the representation for mesoscale relationships, and employ seriation graphs and their statistics as the data to infer the class of ITN.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;What effect does the mean duration of assemblages (compared to the total span of time) have on our ability to accurately classify seriations as to network model?&lt;/li&gt;
&lt;li&gt;How does classification accuracy scale with the number of assemblages available, and the scheme by which they were sampled? (some of this may be anecdotally necessary to look at PFG, but a systematic computational analysis of the scaling can wait).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;How does assemblage sample size in concert with innovation rates affect classification accuracy?&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-Dunnell1971&quot;&gt;
&lt;p&gt;Dunnell, Robert C. 1971. &lt;em&gt;Systematics in Prehistory&lt;/em&gt;. New York: Free Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-pudlo2014abc&quot;&gt;
&lt;p&gt;Pudlo, Pierre, Jean-Michel Marin, Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier, and Christian P Robert. 2014. “ABC Model Choice via Random Forests.” &lt;em&gt;ArXiv Preprint ArXiv:1406.6288&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Mark E. Madsen</name></author><category term="dissertation" /><category term="research" /><category term="cultural transmission" /><category term="coarse graining" /><category term="seriation" /><summary type="html">I am somewhat remiss in discussing research goals for the year, because of some family issues which have taken much of my time. But I’m on a flight back from Orlando from the Society for American Archaeology annual meetings, and I’ve accumulated notes and ideas over the last three days about where my research stands and what my next steps are. I also want to evaluate how I did in addressing the priorities I set for 2015.</summary></entry><entry><title type="html">Presentation and Paper for SAA 2016 - Measuring Cultural Relatedness Using Multiple Seriation Ordering Algorithms</title><link href="http://notebook.madsenlab.org/essays/2016/04/07/saa-paper-slides.html" rel="alternate" type="text/html" title="Presentation and Paper for SAA 2016 - Measuring Cultural Relatedness Using Multiple Seriation Ordering Algorithms" /><published>2016-04-07T00:00:00-07:00</published><updated>2016-04-07T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/essays/2016/04/07/saa-paper-slides</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/2016/04/07/saa-paper-slides.html">&lt;p&gt;The Society for American Archaeology meetings are coming up in Orlando, and I’ll be participating in a session called:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Evolutionary Archaeologies: New Approaches, Methods, and Empirical Sufficiency&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;along with a number of colleagues. We opted for the “electronic symposium” option this year, which is a slightly confusing description. Instead of presenting or reading a full paper, we submit papers in advance, which are posted online (the “electronic” part). At the conference, we each get a few minutes to re-summarize our work for the audience to ensure that everyone is up to speed, and then we have Q&amp;amp;A and discussion for most of the time.&lt;/p&gt;
&lt;p&gt;The slides I’ll use on Saturday for summarizing the work are located &lt;a href=&quot;https://github.com/mmadsen/saa2016-multiple-seriation-algorithms/blob/master/presentation/madsenlipo2016-saa-continuity-seriation.pdf&quot;&gt;in the presentation directory from the Github repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can read the &lt;a href=&quot;https://github.com/mmadsen/saa2016-multiple-seriation-algorithms/blob/master/pdf-drafts/saa2016-seriation-multiple-approaches.pdf&quot;&gt;conference draft of our paper from the Github repository&lt;/a&gt;. Comments welcome; this will be submitted for publication after expansion and revisions in the next few months so any suggested improvements are greatly appreciated. We will likely incorporate several other data examples in the final version after getting permission from the folks who collected the data sets.&lt;/p&gt;</content><author><name>Mark E. Madsen</name></author><category term="SAA2016" /><category term="seriation" /><category term="cultural transmission" /><category term="dissertation" /><summary type="html">The Society for American Archaeology meetings are coming up in Orlando, and I’ll be participating in a session called:</summary></entry><entry><title type="html">Next Steps for Classifying Seriations to Temporal Network Models</title><link href="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/03/22/seriation-classification-next-steps.html" rel="alternate" type="text/html" title="Next Steps for Classifying Seriations to Temporal Network Models" /><published>2016-03-22T00:00:00-07:00</published><updated>2016-03-22T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/03/22/seriation-classification-next-steps</id><content type="html" xml:base="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/03/22/seriation-classification-next-steps.html">&lt;h3 id=&quot;where-things-stand&quot;&gt;Where Things Stand&lt;/h3&gt;
&lt;p&gt;Initial experiments are promising, when using the sorted Laplacian spectrum as the features for building a classifier model. Even with small samples, it seems to show the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can tell lineage splitting from a complete network from a probabilistic nearest neighbor model&lt;/li&gt;
&lt;li&gt;We can’t tell PNN models from each other given different shapes (aspect ratio) to the region&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This comes from building a multi-class GB tree model from &lt;code&gt;sc-1&lt;/code&gt;, &lt;code&gt;sc-3&lt;/code&gt;, &lt;code&gt;sc-4-nn&lt;/code&gt;, and &lt;code&gt;sc-4-nn&lt;/code&gt;, and predicting the data generating model from a 10% holdout set.&lt;/p&gt;
&lt;p&gt;The classifier results hold pretty steady in a qualitative sets regardless of the random train/test split.&lt;/p&gt;
&lt;p&gt;What doesn’t hold steady is the prediction and class probabilities for the PFG continuity graph. I get different answers depending upon the train/test split, which is probably a function of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Insufficient diversity in the network models used in simulation – I need many examples of each network model&lt;/li&gt;
&lt;li&gt;Sample size overall&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given that there isn’t much overlap in the overall classification itself, my guess is that if we could look at this in the 10 dimensional space of the eigenvalues used, we would see that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The PFG seriation is actually not deeply embeeded in the convex hull of points for any of the classes, but is near the edge of several or even all&lt;/li&gt;
&lt;li&gt;That the decision boundaries shift a lot with respect to the area where the PFG seriation is located&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given this, a different train/test split could shift a decision boundary very slightly, without having a major impact on the overall confusion matrix among models, and thus change the predicted assignment for the PFG sample.&lt;/p&gt;
&lt;p&gt;We might be able to visualize something like the above by using a dimensionality reduction technique and mapping the models against say the first 3 principal components, and then putting PFG on the map. Worth a try.&lt;/p&gt;
&lt;h3 id=&quot;computational-next-steps&quot;&gt;Computational Next Steps&lt;/h3&gt;
&lt;p&gt;But removing this issue and getting stable predictions for PFG is going to be a function of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More classes of network models, since PFG might not really be well described by any of the existing ones&lt;/li&gt;
&lt;li&gt;More network models per class&lt;/li&gt;
&lt;li&gt;More samples of simulations per network model (or model class)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While I develop more network models, I will probably start doing the second and third for the existing four models, but with PNN models collapsed down to a single model. I don’t have the formal infrastructure yet for doing multiple realizations of a single network model, so that’s the first step.&lt;/p&gt;</content><author><name>Mark E. Madsen</name></author><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><summary type="html">Where Things Stand</summary></entry><entry><title type="html">EVoS Seminar Series Talk at Binghamton University</title><link href="http://notebook.madsenlab.org/essays/2016/03/22/evos-seminar-series-binghamton.html" rel="alternate" type="text/html" title="EVoS Seminar Series Talk at Binghamton University" /><published>2016-03-22T00:00:00-07:00</published><updated>2016-03-22T00:00:00-07:00</updated><id>http://notebook.madsenlab.org/essays/2016/03/22/evos-seminar-series-binghamton</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/2016/03/22/evos-seminar-series-binghamton.html">&lt;p&gt;Yesterday I was out in Binghamton, giving a talk in the &lt;a href=&quot;http://binghamton.edu/evos/&quot;&gt;Evolutionary Studies Program&lt;/a&gt; seminar series. Here is a link to the &lt;a href=&quot;http://binghamton.edu/evos/seminar-series/2016spring/madsen.html&quot;&gt;talk flyer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ve posted the slides (with speaker notes) on Figshare: &lt;a href=&quot;https://dx.doi.org/10.6084/m9.figshare.3121420&quot; class=&quot;uri&quot;&gt;https://dx.doi.org/10.6084/m9.figshare.3121420&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here’s the abstract for the talk:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Evolutionary modeling of cultural transmission and cultural change has grown over the past 25 years from a handful of biologist and social scientists, to a major interdisciplinary program that involves research in every social science, cognitive and computer science, biologists, and even physicists. Formal models of cultural transmission and evolution have proliferated, but major challenges exist in testing transmission models against real world data. Difficulties exist even when we have individual-level observations. The challenge is even more profound when the only data we have on a cultural or economic phenomenon come in aggregate form: where our observations refer to groups of people, blocks of time, or both. After examining how aggregated data foil our efforts at inferring the parameters of evolutionary models or accurately choosing between models, I advocate for matching aggregate data with higher level models and research questions. I demonstrate how aggregate data from cultural transmission simulations can accurately discriminate between macroevolutionary transmission models, giving us the ability to understand large scale transmission phenomena even while micro scale causation remains obscure.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;video&quot;&gt;VIDEO&lt;/h3&gt;
&lt;p&gt;Update: &lt;a href=&quot;https://bustream.binghamton.edu:8443/ess/echo/presentation/ffb57417-88f3-439f-8095-7b0983d954d3?ec=true&quot;&gt;The video and slides are now available online!&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark E. Madsen</name></author><category term="talks" /><category term="presentations" /><category term="cultural evolution" /><category term="cultural transmission" /><summary type="html">Yesterday I was out in Binghamton, giving a talk in the Evolutionary Studies Program seminar series. Here is a link to the talk flyer.</summary></entry><entry><title type="html">Intentionality and Cultural Evolution - Towards a Generalized Learning Theory Account</title><link href="http://notebook.madsenlab.org/essays/2016/03/03/cultural-evolution-intentionality-pac-learning.html" rel="alternate" type="text/html" title="Intentionality and Cultural Evolution - Towards a Generalized Learning Theory Account" /><published>2016-03-03T00:00:00-08:00</published><updated>2016-03-03T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/essays/2016/03/03/cultural-evolution-intentionality-pac-learning</id><content type="html" xml:base="http://notebook.madsenlab.org/essays/2016/03/03/cultural-evolution-intentionality-pac-learning.html">&lt;p&gt;(&lt;strong&gt;the following is a continuation and completion of a &lt;a href=&quot;http://notebook.madsenlab.org/cultural%20transmission/theory/essays/2013/06/13/gabora2013-response-notes.html&quot;&gt;post begun in 2013&lt;/a&gt;, stimulated by a recent article by D.S. Wilson&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;Among those who object to framing cultural evolution as a Darwinian theory, one of the most important reasons for objection is the evident importance of intentionality in human behavior (and among many animal species). Liane Gabora has perhaps been one of the most persistent advocates that while culture evolves, it does so by mechanisms other than natural selection, since natural selection requires variation to be “random” &lt;span class=&quot;citation&quot; data-cites=&quot;Gabora2013a&quot;&gt;(Gabora 2013)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;generation-of-variation-in-darwinian-processes&quot;&gt;Generation of Variation in Darwinian Processes&lt;/h3&gt;
&lt;p&gt;Or does it? Lipo and I argued in response that most modern commentators are overinterpreting the term “random” in the core Darwinian paradigm: that the generation of variation is merely &lt;strong&gt;causally unprivileged&lt;/strong&gt; with respect to the differential persistence of that variation &lt;span class=&quot;citation&quot; data-cites=&quot;Madsen2013a&quot;&gt;(Madsen and Lipo 2013)&lt;/span&gt;. David Sloan Wilson, in a recent and very clear article on intentional cultural change &lt;span class=&quot;citation&quot; data-cites=&quot;Wilson:2016bc&quot;&gt;(Wilson 2016)&lt;/span&gt;, makes the same point very crisply:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the standard portrayal of genetic evolution, mutations occur that are arbitrary with respect to their consequences for survival and reproduction (fitness). Those that enhance fitness increase in frequency until they become species-typical. The word ‘arbitrary’ rather than ‘random’ in the previous sentence is deliberate. If a mutation is random, then it results in a new phenotype that deviates from the previous phenotype in any direction with equal probability. The standard portrayal of genetic evolution does not assume that mutations are random in this sense. Instead, the assumption is that mutations do not anticipate the phenotypes that are favored by natural selection. This is the meaning of the word ‘arbitrary’.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In fact, we can look at variation-generating mechanisms along several dimensions, as shown in this diagram. Variation may be “undirected” or “directed.” Directed variations are innovations or errors that occur when a variation-generation mechanism targets specific components of the genome or cultural repertoire. In contrast, undirected variations are errors or innovations which occur in some random component of the genome or cultural repertoire.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/variation-classes.png&quot; alt=&quot;Classes of Variation&quot; /&gt;&lt;figcaption&gt;Classes of Variation&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The classic example of undirected variation is the cosmic ray streaking through a cell nucleus, causing damage to DNA which results in “flipping” one or more nucleotides during the repair process. This is the archetype for what nearly everyone means when they talk about “blind variation” in Darwinian evolution. In cultural evolution, a person copying an artifact will make motor and perceptual errors in copying, which are often random with respect to which physical dimension they occur upon &lt;span class=&quot;citation&quot; data-cites=&quot;Eerkens2005&quot;&gt;(Eerkens and Lipo 2005)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But we now also understand that there are many mechanisms whereby variation can be generated in a “directed” fashion. Environmental stress seems to have a variety of effects on the rate and location of mutations in the genome, and there are a variety of epigenetic mechanisms whereby temporary changes in gene expression can be inherited by the next generation, and thus permanently affect a lineage.&lt;/p&gt;
&lt;p&gt;The important thing to understand about such “directed” mechanisms for generating variation is that while they are &lt;strong&gt;targeted&lt;/strong&gt;, they are &lt;strong&gt;unprivileged&lt;/strong&gt; with respect to knowing how selection will ultimately filter the results of their generation. Causal arrows run one direction, and variation is always generated &lt;strong&gt;before&lt;/strong&gt; selection affects the frequency of variants. Genomic mechanisms which increase the mutation rate in selected regions of the genome are certainly the products of &lt;strong&gt;past&lt;/strong&gt; selection, but there is no causal arrow which gives them information about how the results of their action will fare in &lt;strong&gt;future&lt;/strong&gt; survival and reproduction.&lt;/p&gt;
&lt;p&gt;And this, really, points to the way out of the issue. There is simply no requirement that variation be “random” with respect to…anything. The generation of variation is simply causally uncoupled, or unprivileged, from the “judging” of its fitness or utility down the road. The “two step process” of Darwinian evolution is &lt;strong&gt;defined&lt;/strong&gt; by that uncoupling.&lt;/p&gt;
&lt;h3 id=&quot;intentionality&quot;&gt;Intentionality&lt;/h3&gt;
&lt;p&gt;Which leads us to the thorny issue of “intentional” behavior in a mechanistic, Darwinian theory. My mentor, Robert Dunnell, was vehemently opposed to the inclusion of intentionality in any scientific theory of cultural change, for a variety of reasons which were correct. Mostly, the objection to intentionality is the causal role it plays in most social sciences, short circuiting the “two step” process and asserting that change is often a “one step” process whereby people perceive a problem, choose the best solution to deal with it, and implement that solution. Theories built on this kind of logic include variations of rational actor models in economics, overly simplistic versions of adaptationist evolutionary ecology, public choice theory, and of course a long list of unilinear, progressivist, vitalist, and Lamarckian models of cultural change in anthropology. All of which have been instrumental at various times in preventing us from constructing testable, scientific accounts of cultural change.&lt;/p&gt;
&lt;p&gt;Which presents us with a seeming conundrum. Clearly, humans and many animals exhibit behavior which is “intentional,” and we and other species evolved this capability over a broad span of time (given its taxonomic breadth) by natural selection acting upon variation generated by various directed and undirected mechanisms. But equally clearly, intentional behavior cannot be a “shortcut” around the two step process of variation and selection, since like all other variation, our intentions and strategic planning are causally prior to the outcome of their expression, and often long prior to their downstream affects on our reproductive success, survival, and our ability to spread our ideas and cultural norms.&lt;/p&gt;
&lt;p&gt;In his recent article, David Sloan Wilson &lt;span class=&quot;citation&quot; data-cites=&quot;Wilson:2016bc&quot;&gt;(Wilson 2016)&lt;/span&gt; discussed a number of mechanisms by which intentional behavior has probably evolved in humans and other lineages. The adaptive nature of the vertebrate immune system, for example, is a paradigm example of an adaptive, open-ended process; it is, in fact, a selection process within a selection process. Wilson also describes operant conditioning and explicit decision-making.&lt;/p&gt;
&lt;p&gt;It is worth trying to give a general account of such mechanisms, however, because it may be possible to unify many kinds of “directed” variation mechanisms, including those involved in intentional behavior, and understand what they share in common. The natural framework for unifying such examples is statistical learning theory, which attempts to describe a generalized framework whereby accurate models can be inferred by exposure (in some fashion) to data &lt;span class=&quot;citation&quot; data-cites=&quot;kearns1994introduction&quot;&gt;(M. J. Kearns and Vazirani 1994)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is becoming somewhat fashionable to make connections between learning theory and directed mechanisms in evolution (for example, see &lt;span class=&quot;citation&quot; data-cites=&quot;Power:2015cc&quot;&gt;(Power et al. 2015)&lt;/span&gt; ) after the recent book by Leslie Valiant, one of the founders of formalized statistical learning theory &lt;span class=&quot;citation&quot; data-cites=&quot;valiant2013probably&quot;&gt;(Valiant 2013)&lt;/span&gt;. Valiant’s model, PAC learning, provides a broad guarantee that we can build a statistical model capable of discriminating instances of a target distribution (or “concept” in machine learning). The nature of that guarantee is that, with enough exposure to training samples, we can select a hypothesis with low generalization error (the “approximately correct” part), with high probability (the “probably” part). PAC learning formally underlies many, but not all, “supervised” learning methods in statistics and machine learning, including much regression modeling and various classification and pattern recognition methods.&lt;/p&gt;
&lt;h3 id=&quot;intentional-behavior-and-learning-theories&quot;&gt;Intentional Behavior and Learning Theories&lt;/h3&gt;
&lt;p&gt;But PAC learning is not a “universal” learning theory, as Valiant notes. The basic PAC learning model applies to situations where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The learner receives access to data, in the form of measurements of some number of features or covariates, and a “label” indicating to which model or target class that set of covariates belongs&lt;/li&gt;
&lt;li&gt;The label given can be taken as accurate, and not associated with noise or error&lt;/li&gt;
&lt;li&gt;The learner does not direct the generation of the data, but accepts labeled examples as given&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As Valiant describes in his book, this kind of process doesn’t directly underlie most examples in genetic evolution. PAC learning, self-evidently, does underlie some types of cultural learning, since of course it (and the statistical algorithms that implement it) are cultural constructions by humans, to aid in understanding complex aspects of their environment.&lt;/p&gt;
&lt;p&gt;But the framework is too restrictive to cover all learning in cultural contexts, and thus most of the ways in which humans formulate intentions for action on the basis of information gathered. In fact, each of the restrictions above can be relaxed, and in doing so, result in different learning models.&lt;/p&gt;
&lt;p&gt;In discussing the relation between learning theory and biological evolution, Valiant correctly focuses upon relaxing the first requirement: that the learner see the detailed data. One way to frame biological fitness within learning theory is to treat classes of genomes as “queries” that populations make into the environment, which responds with &lt;strong&gt;summary&lt;/strong&gt; data: average length of survival, average number of offspring for individuals with that class of genome. The population evolves by aggregating this feedback in the form of differential persistence of seemingly successful phenotypes.&lt;/p&gt;
&lt;p&gt;The leaning framework just described is a modification of PAC learning by Kearns called “statistical query learning” &lt;span class=&quot;citation&quot; data-cites=&quot;Kearns:1998:ENL:293347.293351&quot;&gt;(M. Kearns 1998)&lt;/span&gt;, and while the interpretation of fitness as statistical queries against the environment might sound like a bit of a stretch, there are many behavioral contexts which fit such a model nicely. Individuals learning a skill, for example, might make attempts and observe the results, and modify their next attempt accordingly. Rarely will individuals have detailed information about the various contributing factors leading to the outcomes, especially in a complex activity such as hunting or making stone tools. Moreover, the same actions and tools may lead to differing outcomes on different trials, leading to only summary information about the outcome of an action or tool on average.&lt;/p&gt;
&lt;p&gt;But humans do more than simply learn by aggregating data; we can guide the process of data collection and learning to improve performance. In situations where we can make trials, and then consult an “expert” for feedback, we can tune our next trial based upon the feedback, and repeat the loop. Much of human learning and the entire “apprenticeship” model for learning complex skills is based on this kind of model. In formal terms, this is “active learning,” which is the subject of quite active study within machine learning and statistics &lt;span class=&quot;citation&quot; data-cites=&quot;Jamieson:2015vp&quot;&gt;(Jamieson, Jain, and Fernandez 2015)&lt;/span&gt;. Of particular interest is a recent NIPS conference paper which studied active learning when learners have access to both “strong” and “weak” labelers. A strong labeler is very accurate at providing feedback, but expensive to consult, weak labelers are cheap to consult but are occasionally inaccurate. Examples of each in a human context might be asking the master craftsman for feedback rather than students slightly more advanced than oneself, or hiring a skilled attorney rather than Googling for an answer. Zhang and colleagues find, of course, that there are situations where consulting a mix of strong and weak experts can result in highly accurate learning &lt;span class=&quot;citation&quot; data-cites=&quot;Zhang:2015wz&quot;&gt;(Zhang and Chaudhuri 2015)&lt;/span&gt;. This should be unsurprising, because the essential features of multiple-oracle active learning are in play in most human learning environments (e.g., schools, medical residency programs, craft apprenticeships, legal associate programs, and so on).&lt;/p&gt;
&lt;p&gt;Furthermore, learning models must account for structured information. The learning task is rarely simply to recognize or predict a single type of distribution or concept, but instead is multi-stage, with stages building upon one another. Real knowledge has prerequisites, and structure, and we can easily fail to learn a skill if we do not yet have a solid grounding in the information and skills that come before. This will have marked effects on our cultural transmission models, and require close collaboration with learning theorists, but should yield much richer analyses of technological change in particular &lt;span class=&quot;citation&quot; data-cites=&quot;Madsen2015&quot;&gt;(Madsen and Lipo 2015)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, only occasionally do we learn in a focused way where we are regularly getting labeled feedback, from whatever source. Much of our learning about the world comes in a combination of data points, some of which come with feedback, and much of which doesn’t. Such situations fall within so-called “semi-supervised” learning. Or, we get batched feedback, where we get a single evaluation for a number of different trials which may vary in subtle ways &lt;span class=&quot;citation&quot; data-cites=&quot;Settles:2008wh&quot;&gt;(Settles, Craven, and Ray 2008)&lt;/span&gt;. Finally, we often learn not by exploring an entire space of possibilities, but by actively looking for the most “informative” or representative portions of that space of examples &lt;span class=&quot;citation&quot; data-cites=&quot;Huang:2010uj&quot;&gt;(Huang, Jin, and Zhou 2010)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;
&lt;p&gt;The basic point is that many variations on statistical learning models will be applicable in understanding how humans learn, both from the environment and by cultural transmission and teaching. Note, however, that in none of the models does the learner understand the ultimate consequences of incremental steps. Even in active learning models where the learner can take past knowledge to query for specific and informative data samples to guide future action, the progress in accuracy is local and stochastic; overfitting is still a concern and it is still impossible to know ahead of time what the ultimate “generalization error” of one’s strategies will be.&lt;/p&gt;
&lt;p&gt;Cultural evolution is, undoubtedly, a mixture of intentional and unintentional processes, of unthinkingly copying someone else but also deeply studying carefully chosen mentors and experts. There is room in our theories of cultural evolution for pure diffusion processes that look very much like epidemiological or simple population genetic models, and processes that draw deeply from cognitive science, childhood development, and rich ethnography for their details. There is even room for the subtle combinatorial cognitive processes that lead to “creativity” and true invention.&lt;/p&gt;
&lt;p&gt;All of these, and more, are understandable without exiting the Darwinian paradigm, and various theories of statistical learning promise to play a large role in extending Darwinian evolution to those intentional processes. But intentionality, like creativity, are not a sign that natural selection cannot act on culture, or that culture is not Darwinian. The latter is a continuing misconception that largely stems from overinterpreting what “random variation” means in the evolutionary context.&lt;/p&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-Eerkens2005&quot;&gt;
&lt;p&gt;Eerkens, J.W., and C.P. Lipo. 2005. “Cultural Transmission, Copying Errors, and the Generation of Variation in Material Culture and the Archaeological Record.” &lt;em&gt;Journal of Anthropological Archaeology&lt;/em&gt; 24 (4). Elsevier: 316–34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Gabora2013a&quot;&gt;
&lt;p&gt;Gabora, Liane. 2013. “An Evolutionary Framework for Cultural Change: Selectionism Versus Communal Exchange.” &lt;em&gt;Physics of Life Reviews&lt;/em&gt; 10 (2): 117–45. doi:&lt;a href=&quot;https://doi.org/10.1016/j.plrev.2013.03.006&quot;&gt;10.1016/j.plrev.2013.03.006&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Huang:2010uj&quot;&gt;
&lt;p&gt;Huang, S J, R Jin, and Z H Zhou. 2010. “Active learning by querying informative and representative examples.” &lt;em&gt;Advances in Neural Information …&lt;/em&gt;. &lt;a href=&quot;http://papers.nips.cc/paper/4176-active-learning-by-querying-informative-and-representative-examples&quot; class=&quot;uri&quot;&gt;http://papers.nips.cc/paper/4176-active-learning-by-querying-informative-and-representative-examples&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Jamieson:2015vp&quot;&gt;
&lt;p&gt;Jamieson, K G, L Jain, and C Fernandez. 2015. “NEXT: A System for Real-World Development, Evaluation, and Application of Active Learning.” &lt;em&gt;Advances in Neural …&lt;/em&gt;. &lt;a href=&quot;http://papers.nips.cc/paper/5868-next-a-system-for-real-world-development-evaluation-and-application-of-active-learning&quot; class=&quot;uri&quot;&gt;http://papers.nips.cc/paper/5868-next-a-system-for-real-world-development-evaluation-and-application-of-active-learning&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Kearns:1998:ENL:293347.293351&quot;&gt;
&lt;p&gt;Kearns, Michael. 1998. “Efficient Noise-Tolerant Learning from Statistical Queries.” &lt;em&gt;J. ACM&lt;/em&gt; 45 (6). New York, NY, USA: ACM: 983–1006. doi:&lt;a href=&quot;https://doi.org/10.1145/293347.293351&quot;&gt;10.1145/293347.293351&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-kearns1994introduction&quot;&gt;
&lt;p&gt;Kearns, Michael J, and Umesh Virkumar Vazirani. 1994. &lt;em&gt;An Introduction to Computational Learning Theory&lt;/em&gt;. MIT press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Madsen2013a&quot;&gt;
&lt;p&gt;Madsen, Mark E., and Carl P. Lipo. 2013. “Saving Culture from Selection: Comment on an Evolutionary Framework for Cultural Change: Selectionism Versus Communal Exchange, by L. Gabora.” &lt;em&gt;Physics of Life Reviews&lt;/em&gt; 10 (2): 149–50. doi:&lt;a href=&quot;https://doi.org/10.1016/j.plrev.2013.03.008&quot;&gt;10.1016/j.plrev.2013.03.008&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Madsen2015&quot;&gt;
&lt;p&gt;———. 2015. “Behavioral Modernity and the Cultural Transmission of Structured Information: The Semantic Axelrod Model.” In &lt;em&gt;Learning Strategies and Cultural Evolution During the Palaeolithic&lt;/em&gt;, edited by Alex Mesoudi and Kenichi Aoki, 67–83. Replacement of Neanderthals by Modern Humans Series. Springer Japan. doi:&lt;a href=&quot;https://doi.org/10.1007/978-4-431-55363-2_6&quot;&gt;10.1007/978-4-431-55363-2_6&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Power:2015cc&quot;&gt;
&lt;p&gt;Power, Daniel A, Richard A Watson, E rs Szathm ry, Rob Mills, Simon T Powers, C Patrick Doncaster, and Blazej Czapp. 2015. “What can ecosystems learn? Expanding evolutionary ecology with learning theory.” &lt;em&gt;Biology Direct&lt;/em&gt;, December. Biology Direct, 1–24. doi:&lt;a href=&quot;https://doi.org/10.1186/s13062-015-0094-1&quot;&gt;10.1186/s13062-015-0094-1&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Settles:2008wh&quot;&gt;
&lt;p&gt;Settles, B, M Craven, and S Ray. 2008. “Multiple-instance active learning.” &lt;em&gt;Advances in Neural Information …&lt;/em&gt;. &lt;a href=&quot;http://papers.nips.cc/paper/3252-multiple-instance-active-learning&quot; class=&quot;uri&quot;&gt;http://papers.nips.cc/paper/3252-multiple-instance-active-learning&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-valiant2013probably&quot;&gt;
&lt;p&gt;Valiant, Leslie. 2013. &lt;em&gt;Probably Approximately Correct: Nature’s Algorithms for Learning and Prospering in a Complex World&lt;/em&gt;. Basic Books.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Wilson:2016bc&quot;&gt;
&lt;p&gt;Wilson, David Sloan. 2016. “Intentional cultural change.” &lt;em&gt;Current Opinion in Psychology&lt;/em&gt; 8 (April): 190–93. doi:&lt;a href=&quot;https://doi.org/10.1016/j.copsyc.2015.12.012&quot;&gt;10.1016/j.copsyc.2015.12.012&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Zhang:2015wz&quot;&gt;
&lt;p&gt;Zhang, C, and K Chaudhuri. 2015. “Active learning from weak and strong labelers.” &lt;em&gt;Advances in Neural Information Processing …&lt;/em&gt;. &lt;a href=&quot;http://papers.nips.cc/paper/5988-active-learning-from-weak-and-strong-labelers&quot; class=&quot;uri&quot;&gt;http://papers.nips.cc/paper/5988-active-learning-from-weak-and-strong-labelers&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Mark E. Madsen</name></author><category term="intentionality" /><category term="cultural transmission" /><category term="PAC" /><category term="learning theory" /><category term="statistical query learning" /><summary type="html">(the following is a continuation and completion of a post begun in 2013, stimulated by a recent article by D.S. Wilson)</summary></entry><entry><title type="html">Limits of model resolution for seriation classification</title><link href="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/22/equifinality-model-variants-seriation-classification.html" rel="alternate" type="text/html" title="Limits of model resolution for seriation classification" /><published>2016-02-22T00:00:00-08:00</published><updated>2016-02-22T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/22/equifinality-model-variants-seriation-classification</id><content type="html" xml:base="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/22/equifinality-model-variants-seriation-classification.html">&lt;h3 id=&quot;model-resolution-and-equifinality&quot;&gt;Model Resolution and Equifinality&lt;/h3&gt;
&lt;p&gt;Experiment &lt;code&gt;sc-2&lt;/code&gt; was designed to examine the opposite question as &lt;code&gt;sc-1&lt;/code&gt;; that is, when do we lose the ability to distinguish between regional interaction models by examining the structure of seriations from cultural traits transmitted through those interaction networks? This is a question of equifinality of models: do different models have empirical consequences which are indistinguishable given a particular observation technique?&lt;/p&gt;
&lt;p&gt;To test this, I set up four models which I believe to be very “close” to each other:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Lineage splitting where 1 lineage turns into 2 lineages, the split occurring 30% of the way through the time sequence (“early split”)&lt;/li&gt;
&lt;li&gt;Lineage splitting where 1 &lt;span class=&quot;math inline&quot;&gt;\(\rightarrow\)&lt;/span&gt; 2 lineages, split occurring 70% of the way through the time sequence (“late split”)&lt;/li&gt;
&lt;li&gt;Lineage coalescence where 2 lineages turn into a single linage, the event occurring 30% of the way through the sequence (“early coalescence”)&lt;/li&gt;
&lt;li&gt;Lineage coalescence where 2 &lt;span class=&quot;math inline&quot;&gt;\(\rightarrow\)&lt;/span&gt; 1 lineages, split occuring 70% of the way through the time sequence (“late coalescence”)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In all other respects, simulation of cultural transmission across these regional networks was identical, using the same prior distributions for innovation and migration rates, population sizes, and so on.&lt;/p&gt;
&lt;p&gt;My expectation going in was that the lineage splitting and coalescence models should generate seriations which are almost indistinguishable from one another, except for their temporal orientation, and with the paired early/late comparisons, it may be difficult to tell any of these models from one another without additional feature information. In particular, I expected roughly chance performance on classification unless we could provide temporal orientation, and even then, we may only be able to tell coalescence from splitting models.&lt;/p&gt;
&lt;h3 id=&quot;initial-sc-2-analysis&quot;&gt;Initial SC-2 Analysis&lt;/h3&gt;
&lt;p&gt;The analysis of &lt;code&gt;sc-2&lt;/code&gt; followed the method used in the &lt;a href=&quot;http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/16/feature-engineering-seriation-classification.html&quot;&gt;second trial of &lt;code&gt;sc-1&lt;/code&gt;&lt;/a&gt;, calculating the Laplacian eigenvalue spectrum of the final seriation solution graphs (specifically, the &lt;code&gt;minmax-by-weight&lt;/code&gt; solutions for continuity seriation), and using the sorted eigenvalues as features for a gradient boosted classifier.&lt;/p&gt;
&lt;p&gt;The initial results seem indicative of real trouble telling these models apart. For guidance, class labels are as follows:&lt;/p&gt;
&lt;ol start=&quot;0&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Early splitting&lt;/li&gt;
&lt;li&gt;Early coalescence&lt;/li&gt;
&lt;li&gt;Late split&lt;/li&gt;
&lt;li&gt;Late coalescence&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given a hold-out test set, we see the following performance:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
          predicted 0  predicted 1  predicted 2  predicted 3
actual 0            3            2            0            0
actual 1            1            1            0            0
actual 2            0            0            2            2
actual 3            0            0            4            5
             precision    recall  f1-score   support

          0       0.75      0.60      0.67         5
          1       0.33      0.50      0.40         2
          2       0.33      0.50      0.40         4
          3       0.71      0.56      0.63         9

avg / total       0.61      0.55      0.57        20

Accuracy on test: 0.550&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The overall accuracy is low, but the pattern of misclassifications is key here. We never see “early” models misclassified as “late” models, but we do see splitting misclassified as coalescence (possibly because we have no orienting information). So while overall accuracy is low, we actually have perfect discrimination along one dimension of the models: when events that alter lineage structure occur, in relative terms. Not bad, considering how “close” in structure these regional interaction models are.&lt;/p&gt;
&lt;h3 id=&quot;optimizing-classification-performance&quot;&gt;Optimizing Classification Performance&lt;/h3&gt;
&lt;p&gt;The above was conducted with “reasonable” hyperparameters for the gradient boosted classifier, but I want to understand our best performance in separating these models. This is accomplished by setting the hyperparameters through cross-validation. In this case, I used a grid search of the following variables and parameters for the learning rate penalty, and the number of boosting rounds (number of estimators):&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;     &lt;span class=&quot;co&quot;&gt;&amp;#39;clf__learning_rate&amp;#39;&lt;/span&gt;: [&lt;span class=&quot;fl&quot;&gt;5.0&lt;/span&gt;,&lt;span class=&quot;fl&quot;&gt;2.0&lt;/span&gt;,&lt;span class=&quot;fl&quot;&gt;1.0&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.75&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.5&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.25&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.1&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.05&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.01&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;0.005&lt;/span&gt;],
     &lt;span class=&quot;co&quot;&gt;&amp;#39;clf__n_estimators&amp;#39;&lt;/span&gt;: [&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;25&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;50&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;250&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;500&lt;/span&gt;,&lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using 5-fold cross validation, this produced 350 different fits of the classifer, with the following results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Best score: 0.593
Best parameters:
param: clf__learning_rate: 1.0
param: clf__n_estimators: 50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some improvement is seen on overall training set accuracy, but the real surprise is test performance on the hold-out data, using the optimal hyperparameters:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;          predicted 0  predicted 1  predicted 2  predicted 3
actual 0            3            2            0            0
actual 1            1            1            0            0
actual 2            0            0            3            1
actual 3            0            0            3            6
             precision    recall  f1-score   support

          0       0.75      0.60      0.67         5
          1       0.33      0.50      0.40         2
          2       0.50      0.75      0.60         4
          3       0.86      0.67      0.75         9

avg / total       0.71      0.65      0.66        20

Accuracy on test: 0.650&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Overall accuracy is greatly improved, which is unusual (normally I would expect test accuracy to be less than training accuracy, but the test set is small). But we can see that we improved mainly because of our ability to predict classes 2 and 3, although the overall pattern is still the same: we have misclassification within early and within late, but perfect discrimination between the two.&lt;/p&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;Given how close these models were, I expected to have great trouble in identifying them from seriations. What I found is that I can identify part of the model class with great accuracy, and that the other modeling dimension (coalescence/splitting) with much less accuracy. This leads me to suspect that I could predict both dimensions with very high accuracy if I were to find a way to encode temporal orientation as a feature.&lt;/p&gt;
&lt;p&gt;I will pursue this, since we often have at least &lt;strong&gt;some&lt;/strong&gt; information on temporal orientation, perhaps by knowing that one assemblage in a set is much earlier or later than the rest. The challenge is finding a way to provide this kind of hint for the synthetic seriation graphs. More soon on this.&lt;/p&gt;
&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/gist/anonymous/f6a18712ee1077d1a329&quot;&gt;Full analysis notebook&lt;/a&gt; on NBViewer, from the Github repository.&lt;/p&gt;
&lt;p&gt;Github Repository: &lt;a href=&quot;https://github.com/mmadsen/experiment-seriation-classification&quot;&gt;experiment-seriation-classification&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;references-cited&quot;&gt;References Cited&lt;/h3&gt;</content><author><name>Mark E. Madsen</name></author><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><summary type="html">Model Resolution and Equifinality</summary></entry><entry><title type="html">Feature Engineering for Seriation Classification</title><link href="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/16/feature-engineering-seriation-classification.html" rel="alternate" type="text/html" title="Feature Engineering for Seriation Classification" /><published>2016-02-16T00:00:00-08:00</published><updated>2016-02-16T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/16/feature-engineering-seriation-classification</id><content type="html" xml:base="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/16/feature-engineering-seriation-classification.html">&lt;h3 id=&quot;feature-engineering&quot;&gt;Feature Engineering&lt;/h3&gt;
&lt;p&gt;In my &lt;a href=&quot;http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/14/seriation-classification-experiment.html&quot;&gt;previous note&lt;/a&gt;, I used the graph spectral distance (i.e., the euclidean distance between Laplacian eigenvalue spectra from two seriation solutions) in a kNN classifer to predict which regional network model generated a seriation graph. This achieved accuracy around 80% with 3 nearest neighbors.&lt;/p&gt;
&lt;p&gt;Doing better meant changing approaches, and giving the classifier a larger space within which to draw decision boundaries. My first thought was to not reduce the Laplacian spectrum to distances, but instead of use the spectra themselves as numeric features. This would require that, say, column 1 represented the largest eigenvalue in each graph’s spectrum, column 2 the second largest, etc, which is easily accomplished.&lt;/p&gt;
&lt;p&gt;The resulting feature matrix is then suitable for any classifier algorithm. I chose gradient boosted trees because of their high accuracy (essentially equivalent to random forests or better in most applications), and without any hyperparameter tuning at all, achieve anywhere from 85% to 100% accuracy depending upon the train/test split (it’s a small sample size). Optimizing hyperparameters improves this and I can get 100% pretty often with different train test splits.&lt;/p&gt;
&lt;p&gt;So this might be the standard method for seriation classification for the moment. The good thing is that it lends itself to direct interpretation as an ABC (approximate Bayesian computation) estimator, as described in &lt;span class=&quot;citation&quot; data-cites=&quot;pudlo2014abc&quot;&gt;(Pudlo et al. 2014)&lt;/span&gt;, especially if I actually use random forests (although I’m not sure the random forest bit is terribly important).&lt;/p&gt;
&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h3&gt;
&lt;p&gt;The following code snippet takes a list of NetworkX graph objects, and returns a Numpy matrix with a chosen number of eigenvalues (it isn’t clear how many are relevant):&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; graphs_to_eigenvalue_matrix(graph_list, num_eigenvalues &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;):
    &lt;span class=&quot;co&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    Given a list of NetworkX graphs, returns a numeric matrix where rows represent graphs, &lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    and columns represent the reverse sorted eigenvalues of the Laplacian matrix for each graph,&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    possibly trimmed to only use the num_eigenvalues largest values.  If num_eigenvalues is &lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    unspecified, all eigenvalues are used.&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;co&quot;&gt;# peek at the first graph and see how many eigenvalues there are&lt;/span&gt;
    tg &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; graph_list[&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;]
    n &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(nx.spectrum.laplacian_spectrum(tg, weight&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;))
    
    &lt;span class=&quot;co&quot;&gt;# we either use all of the eigenvalues, or we use the smaller of&lt;/span&gt;
    &lt;span class=&quot;co&quot;&gt;# the requested number or the actual number (if it is smaller than requested)&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; num_eigenvalues &lt;span class=&quot;op&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;:
        ev_used &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; n
    &lt;span class=&quot;cf&quot;&gt;else&lt;/span&gt;:
        ev_used &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;min&lt;/span&gt;(n, num_eigenvalues)

    &lt;span class=&quot;bu&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;(debug) eigenvalues - test graph: &lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; num_eigenvalues: &lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; ev_used: &lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;%&lt;/span&gt; (n, num_eigenvalues, ev_used)
    
    data_mat &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; np.zeros((&lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(graph_list),ev_used))
    &lt;span class=&quot;co&quot;&gt;#print &amp;quot;data matrix shape: &amp;quot;, data_mat.shape&lt;/span&gt;
    
    &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; ix &lt;span class=&quot;op&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;range&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(graph_list)):
        spectrum &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;sorted&lt;/span&gt;(nx.spectrum.laplacian_spectrum(graph_list[ix], weight&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;), reverse&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;True&lt;/span&gt;)
        data_mat[ix,:] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; spectrum[&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;:ev_used]
        
    &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; data_mat&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/github/mmadsen/experiment-seriation-classification/blob/master/analysis/sc-1-3/sc-1-seriation-feature-engineering.ipynb&quot;&gt;Full analysis notebook&lt;/a&gt; on NBViewer, from the Github repository.&lt;/p&gt;
&lt;p&gt;Github Repository: &lt;a href=&quot;https://github.com/mmadsen/experiment-seriation-classification&quot;&gt;experiment-seriation-classification&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-pudlo2014abc&quot;&gt;
&lt;p&gt;Pudlo, Pierre, Jean-Michel Marin, Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier, and Christian P Robert. 2014. “ABC Model Choice via Random Forests.” &lt;em&gt;ArXiv Preprint ArXiv:1406.6288&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Mark E. Madsen</name></author><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><summary type="html">Feature Engineering</summary></entry><entry><title type="html">Identifying Metapopulation Models from Simulated CT and Seriations</title><link href="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/14/seriation-classification-experiment.html" rel="alternate" type="text/html" title="Identifying Metapopulation Models from Simulated CT and Seriations" /><published>2016-02-14T00:00:00-08:00</published><updated>2016-02-14T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/14/seriation-classification-experiment</id><content type="html" xml:base="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/02/14/seriation-classification-experiment.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;In a &lt;a href=&quot;http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriationct/2016/01/26/quantifying-similarity-seriations.html&quot;&gt;previous note&lt;/a&gt;, I described the problem of inferring the goodness of fit between a regional network model and the sampled output of cultural transmission on that regional network, as measured through seriations. I am now ready with the simulation and inference code to start testing the spectral similarity metric I discussed in that note across pairs and sets of regional network models.&lt;/p&gt;
&lt;p&gt;Here, I describe the first such comparison.&lt;/p&gt;
&lt;h3 id=&quot;experiment-sc-1&quot;&gt;Experiment: SC-1&lt;/h3&gt;
&lt;p&gt;SC-1 is a simple contrast between two regional network models. A regional network model is a time-transgressive description of the interaction patterns that existed among a set of subpopulations, described by an “interval temporal network” representation (see &lt;a href=&quot;http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriationct/2014/11/28/more-temporal-networks-python.html&quot;&gt;note 2&lt;/a&gt; and &lt;a href=&quot;http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriationct/2014/07/28/implementing-temporal-networks-in-python.html&quot;&gt;note 1&lt;/a&gt; about the implementation of such models).&lt;/p&gt;
&lt;p&gt;Both models are composed of 10 time slices.&lt;/p&gt;
&lt;p&gt;Model #1 is called “linear” in the experiment directory because it should ideally yield simple, linear seriation solutions because the only thing occurring is sampling through time. At any given time, the metapopulation is composed of 64 subpopulations each. Each subpopulation is fully connected, so that any subpopulation can exchange migrants with any other, and there are no differences in edge weights (and thus migration rates). Each subpopulation in slice &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt; is the child of a random subpopulation in slice &lt;span class=&quot;math inline&quot;&gt;\(N-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Model #2 is called “lineage” in the experiment directory because it features 4 clusters of subpopulations, with 8 subpopulations per cluster. Subpopulations are fully connected within a cluster, with edge weight 10. Subpopulations are connected between subpopulations at fraction 0.1, with edge weight 1. This yields a strong tendency to exchange migrants within clusters, and at a much lower rate between clusters. For the first 4 time slices, all four clusters of subpopulations are interconnected, but in slice 4, a “split” occurs, removing interconnections between two sets of clusters, leaving &lt;span class=&quot;math inline&quot;&gt;\({1,2}\)&lt;/span&gt; interconnected and &lt;span class=&quot;math inline&quot;&gt;\({3,4}\)&lt;/span&gt; interconnected, but no connections between these sets. The resulting clusters then evolve for 6 more time slices on separate trajectories, giving rise to a “lineage split.”&lt;/p&gt;
&lt;p&gt;Neutral cultural transmission is simulated across these two network models, with 50 simulation replicates on each model, and a common set of parameters:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode json&quot;&gt;&lt;code class=&quot;sourceCode json&quot;&gt;&lt;span class=&quot;fu&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;theta_low&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.00001&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;theta_high&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;maxinittraits&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;numloci&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;popsize&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;250&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;simlength&amp;quot;&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;8000&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;samplefraction&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;migrationfraction_low&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;migrationfraction_high&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;dt&quot;&gt;&amp;quot;replicates&amp;quot;&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;fu&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Innovation rates and migration fractions are chosen uniformly at random from the ranges given for each simulation run. Each locus evolves randomly, but we track combinations of loci as “classes” in the archaeological sense of the term as our observable variables. Populations evolve for 8000 generations, giving approximately 800 transmission events per individual during a time slice (i.e., step in the regional metapopulation evolution). We can think of that as approximately monthly opportunities for copying artifacts or behavioral traits, over a lifetime of approximately 65 years.&lt;/p&gt;
&lt;p&gt;The raw data from each community in a simulated network model are then aggregated over the duration that community persists, giving us a time averaged picture of the frequency of cultural traits.&lt;/p&gt;
&lt;p&gt;I then perform the following sampling steps:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;I take a sample of each time averaged data collection similar to the size of a typical archaeological surface collection: 500 samples are taken without replacement from each community, with probability proportional to class frequency. This yields a smaller set of classes, since many hundreds or thousands of combinations are only seen once or twice in the simulation data, and thus are very hard to measure from empirical samples.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I then take a sample of communities to seriate. From each of the two network models, I take temporally stratified samples, with 3 communities per time slice sampled out of the 64 present in each slice. In real sampling situations, we would not know how communities break down temporally, but we often do attempt to get samples of assemblages which cover our entire study duration. In the case of Model #1, we take a 5% sample of communities in each time slice. In the case of Model #2, given the different structure of the model, we take a 12% sample of communities in each time slice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Within the overall sample from each simulation run, comprised now of the time averaged class frequencies from 30 sampled communities, I examine the classes/types themselves, and drop any types (columns) which do not have data values for at least 3 communities. This is standard pre-processing for seriation analyses (or was in the Fordian manual days of seriation analysis) since without more than 3 values, a column does not contribute to ordering the communities.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/clipo/idss-seriation&quot;&gt;IDSS Seriation&lt;/a&gt; package is then used to seriate the stratified, filtered data files for each simulation run across the two models.&lt;/p&gt;
&lt;h3 id=&quot;first-classification-attempt&quot;&gt;First Classification Attempt&lt;/h3&gt;
&lt;p&gt;For a first classification attempt, I did a “leave one out” cross validation run, in which each seriation graph was sequentially deleted from the training set of 99 seriations (one lineage graph had issues with duplicate frequencies and became stuck in frequency seriation), and the distance from the hold-out target graph to all others calculated using &lt;a href=&quot;http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriationct/2016/01/26/quantifying-similarity-seriations.html&quot;&gt;Laplacian spectral distance&lt;/a&gt;. The label of the target graph was then predicted as the majority vote of the 5 nearest neighboring graphs. No attempt was made to tune the number of nearest neighbors using a second cross validation pass, but that will be the next experiment.&lt;/p&gt;
&lt;p&gt;In general, the ability to predict the label (network model) which gave rise to the target seriation graph is decent: 76.8%.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Classification Report:

          predicted 0  predicted 1
actual 0           41            9
actual 1           14           35
             precision    recall  f1-score   support

          0       0.75      0.82      0.78        50
          1       0.80      0.71      0.75        49

avg / total       0.77      0.77      0.77        99

Accuracy on test: 0.768&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The details of classifier accuracy seem to show that we have better ability to correctly classify seriations which result from Model #1 (“linear”) model than seriations originating from the lineage splitting Model #2. In particular, we correctly identify instances of Model #1 82% of the time (recall), although there are clear issues with false positives. The ability to identify instances of Model #2 is worse, with a considerable number of false negatives.&lt;/p&gt;
&lt;p&gt;In the next experiment, I intend to see if different values of the k-Nearest Neighbor parameter affect this accuracy, but I expect that achieving higher accuracy might require augmenting the approach. One possibility that bears exploration is to not simply use the spectral distance, but to instead use the Laplacian eigenvalues themselves (sorted in decreasing order) directly as features, in addition to other graph theoretic properties such as average degree and tree radius, and use a more traditional classifier like boosted decision trees. That will probably be my third experiment.&lt;/p&gt;
&lt;h3 id=&quot;second-attempt&quot;&gt;Second Attempt&lt;/h3&gt;
&lt;p&gt;In a second run, I examined the effect of the number of nearest neighbors which “vote” on the label of the target seriation, still with leave-one-out cross validation. The results seem to indicate that (at least for these models) that best performance is 3 nearest neighbors, with accuracy worsening for 5, 7, and 9 neighbors, and then plateauing a bit for 11 and 15 neighbors. But with 3 neighbors, we achieve almost 80% accuracy, which is encouraging.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;

knn &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; [&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;11&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;15&lt;/span&gt;]
&lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; nn &lt;span class=&quot;op&quot;&gt;in&lt;/span&gt; knn:
    gclf &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; skm.GraphEigenvalueNearestNeighbors(n_neighbors&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;nn)
    test_pred &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; []
    &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; ix &lt;span class=&quot;op&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;range&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(train_graphs)):
        train_loo_graphs, train_loo_labels, test_graph, test_label &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; leave_one_out_cv(ix, train_graphs, train_labels)
        gclf.fit(train_loo_graphs, train_loo_labels)
        test_pred.append(gclf.predict([test_graph])[&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;])
    &lt;span class=&quot;bu&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Accuracy on test for &lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;%s&lt;/span&gt;&lt;span class=&quot;st&quot;&gt; neighbors: &lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;%0.3f&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;%&lt;/span&gt; (nn, accuracy_score(train_labels, test_pred)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Accuracy on test for 1 neighbors: 0.788
Accuracy on test for 3 neighbors: 0.798
Accuracy on test for 5 neighbors: 0.768
Accuracy on test for 7 neighbors: 0.747
Accuracy on test for 9 neighbors: 0.758
Accuracy on test for 11 neighbors: 0.788
Accuracy on test for 15 neighbors: 0.788&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;p&gt;The full Github repository for this and related seriation classification experiments is: &lt;a href=&quot;https://github.com/mmadsen/experiment-seriation-classification&quot;&gt;experiment-seriation-classification&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/github/mmadsen/experiment-seriation-classification/blob/master/analysis/sc-1-3/sc-1-seriation-classification-analysis.ipynb&quot;&gt;Full iPython notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/github/mmadsen/experiment-seriation-classification/blob/master/analysis/sc-1-3/sc-1-eigenvalue-classification-dataprep.ipynb&quot;&gt;Data Preparation Notebook&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;references-cited&quot;&gt;References Cited&lt;/h3&gt;</content><author><name>Mark E. Madsen</name></author><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><summary type="html">Background</summary></entry><entry><title type="html">Loss Functions for ABC Model Selection with Seriation Graphs as Data</title><link href="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/01/26/quantifying-similarity-seriations.html" rel="alternate" type="text/html" title="Loss Functions for ABC Model Selection with Seriation Graphs as Data" /><published>2016-01-26T00:00:00-08:00</published><updated>2016-01-26T00:00:00-08:00</updated><id>http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/01/26/quantifying-similarity-seriations</id><content type="html" xml:base="http://notebook.madsenlab.org/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriation-classification/2016/01/26/quantifying-similarity-seriations.html">&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;Traditional archaeological seriation generated solutions that always had a static linear order, either by use of a multidimensional scaling algorithm or via Fordian manual shuffling of assemblages. In such context, departures from a “correct” solution can be measured by the number of “out of order” assemblages (or via a more opaque “stress” statistic in some cases).&lt;/p&gt;
&lt;p&gt;Last year Carl Lipo and I &lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0124942&quot;&gt;introduced a method&lt;/a&gt; for finding sets of classical Fordian seriations from a set of assemblages via an agglomerative iterative procedure &lt;span class=&quot;citation&quot; data-cites=&quot;Lipo2015&quot;&gt;(Lipo 2015)&lt;/span&gt;. We called the resulting seriations “iterative deterministic seriation solutions” or IDSS. IDSS is capable of using any metric for joining assemblages, including classic unimodality, or various distance measures (something we’ll be &lt;a href=&quot;http://notebook.madsenlab.org/essays/2015/09/06/saa2016-abstract.html&quot;&gt;talking about at the SAA meetings this year&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;When IDSS finds groups of data points that seriate together, but do not seriate (within thresholds) with other groups of points, several solutions are created rather than forcing all of the data points into a single scaling or solution (as has been typical practice with statistical seriation by archaeologists in the past). Instead, IDSS outputs all valid solution sets, since the discontinuities between solution sets may indicate lack of cultural interaction, poor recovery sampling of the archaeological record, or other real-world factors.&lt;/p&gt;
&lt;p&gt;Frequently, one assemblage or data point will occur in multiple solutions, and when this occurs IDSS overlays the solutions and forms a tree. From our paper, Figure 1 shows an IDSS frequency seriation solution for archaeological sites in the Central Mississippi River Valley. Each assemblage is represented by decorated ceramic class frequencies, and we can see that there are at least three major solutions which share assemblage 13-O-7, and a number of minor branches.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/idss-fig12-pfg-solution.png&quot; alt=&quot;Figure 1: IDSS seriation solution for PFG sites in Cental Mississippi River Valley&quot; /&gt;&lt;figcaption&gt;Figure 1: IDSS seriation solution for PFG sites in Cental Mississippi River Valley&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;the-problem&quot;&gt;The Problem&lt;/h3&gt;
&lt;p&gt;My current research is aimed at developing ways of treating IDSS seriation solution graphs as data points in machine learning algorithms, with the goal of fitting coarse grained, regional models of cultural transmission and information diffusion to our data. Seriations are the perfect tool for capturing regional-scale social influences that are time-transgressive.&lt;/p&gt;
&lt;p&gt;There are several challenges in this work. The first, which is described in a &lt;a href=&quot;http://localhost:4000/project-coarse%20grained%20model/model-seriationct/experiment-experiment-seriationct/2014/06/17/seriationct-requirements.html&quot;&gt;previous post&lt;/a&gt; is to create regional metapopulation models with cultural transmission that yield seriations, rather than just type or variant frequencies, as their observable output.&lt;/p&gt;
&lt;p&gt;The second challenge is then to find an inferential framework for model selection which allows one to measure the goodness-of-fit between the theoretical model’s output, and specific empirical seriations like Figure 1.&lt;/p&gt;
&lt;p&gt;This second challenge is perfectly suited to an Approximate Bayesian Computation approach &lt;span class=&quot;citation&quot; data-cites=&quot;Beaumont2002 Csillery2010 Marin2012 Toni2009&quot;&gt;(Beaumont, Zhang, and Balding 2002; Csilléry et al. 2010; Marin et al. 2012; Toni et al. 2009)&lt;/span&gt;, since while we can simulate data from each social learning model, writing down the likelihood function for each model is generally an intractable problem.&lt;/p&gt;
&lt;p&gt;In brief, ABC model selection involves simulating a large number of synthetic data points from each model, calculating summary statistics from those data points, measuring the distance (losses) between summary statistics and observed data, and choosing the model whose losses are the smallest.&lt;/p&gt;
&lt;p&gt;In my current work, the “models” are actually a combination of:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;A social learning model (e.g., unbiased cultural transmission or conformist transmission)&lt;/li&gt;
&lt;li&gt;Parameters for that social learning model (e.g., innovation rates)&lt;/li&gt;
&lt;li&gt;A spatiotemporal population model that describes how communities evolved in a region and what the pattern of community interaction was over time&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a fairly complex “model” to fit, but we can easily simulate samples of cultural variant frequencies across subpopulations from each model (using the &lt;a href=&quot;https://github.com/mmadsen/seriationct&quot;&gt;&lt;code&gt;SeriationCT&lt;/code&gt; software suite&lt;/a&gt;, and then use IDSS to seriate those variant frequencies, yielding simulated versions of Figure 1.&lt;/p&gt;
&lt;p&gt;What we need, then, is a way to measure the distance or loss between Figure 1 and various simulated seriation graphs. Once we have a good loss function, we should be able to perform ABC model selection on seriation graphs.&lt;/p&gt;
&lt;h3 id=&quot;distanceloss-for-unlabelled-unordered-graphs&quot;&gt;Distance/Loss for Unlabelled, Unordered Graphs&lt;/h3&gt;
&lt;p&gt;In general, we want a function which measures the structural similarity of two graphs. There are many approaches to the problem (see &lt;span class=&quot;citation&quot; data-cites=&quot;zager2008graph&quot;&gt;(Zager and Verghese 2008)&lt;/span&gt; for a review). Graph edit distance would be a fairly natural metric, but most algorithms for edit distance rely on matching node identities, and many algorithms also rely upon graphs being ordered as well as labelled.&lt;/p&gt;
&lt;p&gt;The simulated seriations which come out of the &lt;code&gt;SeriationCT&lt;/code&gt; package are unlabelled, and we can’t label them with the identities of the assemblages in our empirical data. So our loss function has to measure structural similarity without reference to node identity, or any notion of ordering or orientation for the graphs.&lt;/p&gt;
&lt;p&gt;This leads fairly directly to using purely algebraic properties of the seriation graphs, and in particular the spectral properties of various matrices associated with the graphs. We know that the eigenvalues of the (possibly binarized) adjacency matrix are related to the edge structure of a graph, and that the eigenvalues of the Laplacian of the adjacency matrix are an even more sensitive indicator of structure &lt;span class=&quot;citation&quot; data-cites=&quot;godsil2001algebraic&quot;&gt;(Godsil and Royle 2001)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a technical report, Koutra et al. &lt;span class=&quot;citation&quot; data-cites=&quot;koutra2011algorithms&quot;&gt;(Koutra et al. 2011)&lt;/span&gt; suggest using the sum of squared differences between the Laplacian spectra of two graphs, trimming the spectra to use only 90% of the total eigenvalue weight. This is attractive from a statistical perspective, essentially giving us the L2 loss between Laplacian spectra for two graphs.&lt;/p&gt;
&lt;p&gt;Given adjacency matrices &lt;span class=&quot;math inline&quot;&gt;\(A_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(A_2\)&lt;/span&gt; for graphs &lt;span class=&quot;math inline&quot;&gt;\(G_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(G_2\)&lt;/span&gt;, the Laplacian matrix is simply &lt;span class=&quot;math inline&quot;&gt;\(L_1 = D_1 - A_1\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(L_2 = D_2 - A_2\)&lt;/span&gt;, where &lt;span class=&quot;math inline&quot;&gt;\(D_i\)&lt;/span&gt; are the corresponding diagonal matrix of vertex degrees. The spectrum is then the set of eigenvalues &lt;span class=&quot;math inline&quot;&gt;\((\lambda_1 \ldots \lambda_n)\)&lt;/span&gt;. Since the Laplacian is positive semi-definite, all of the eigenvalues in the spectrum are positive real numbers. It is possible to put some bounds on the eigenvalues, and thus predetermine the range of possible loss function values for graphs with a given number of vertices, but I leave that to a future post.&lt;/p&gt;
&lt;p&gt;If we use the full spectrum of values, our L2 spectral loss function is then:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{L}_{sp} = \sum_{i=1}^{n} (\lambda_{1i} - \lambda_{2i})^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and if we only use a trimmed set of eigenvalues (say, those which contribute to the top 90% of the total sum of the spectrum), we sort the list of those &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt; eigenvalues and replace &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt; with &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt; in the previous equation.&lt;/p&gt;
&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;In Python, we can implement this loss function for NetworkX graph objects as follows (now part of the &lt;code&gt;seriationct.analytics&lt;/code&gt; module):&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;
&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; networkx &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; nx
&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; np

&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; graph_spectral_similarity(g1, g2, threshold &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.9&lt;/span&gt;):
    &lt;span class=&quot;co&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    Returns the eigenvector similarity, between [0, 1], for two NetworkX graph objects, as&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    the sum of squared differences between the sets of Laplacian matrix eigenvalues that account&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    for a given fraction of the total sum of the eigenvalues (default = 90%).&lt;/span&gt;

&lt;span class=&quot;co&quot;&gt;    Similarity scores of 0.0 indicate identical graphs (given the adjacency matrix, not necessarily&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    node identity or annotations), and large scores indicate strong dissimilarity.  The statistic is&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    unbounded above.&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    l1 &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; nx.spectrum.laplacian_spectrum(g1, weight&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;)
    l2 &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; nx.spectrum.laplacian_spectrum(g2, weight&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;)
    k1 &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; _get_num_eigenvalues_sum_to_threshold(l1, threshold&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;threshold)
    k2 &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; _get_num_eigenvalues_sum_to_threshold(l2, threshold&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;threshold)
    k &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;min&lt;/span&gt;(k1,k2)
    sim &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;sum&lt;/span&gt;((l1[:k] &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt; l2[:k]) &lt;span class=&quot;op&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;)
    &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; sim


&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; _get_num_eigenvalues_sum_to_threshold(spectrum, threshold &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.9&lt;/span&gt;):
    &lt;span class=&quot;co&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    Given a spectrum of eigenvalues, find the smallest number of eigenvalues (k)&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    such that the sum of the k largest eigenvalues of the spectrum&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    constitutes at least a fraction (threshold, default = 0.9) of the sum of all the eigenvalues.&lt;/span&gt;
&lt;span class=&quot;co&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; threshold &lt;span class=&quot;op&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;va&quot;&gt;None&lt;/span&gt;:
        &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(spectrum)

    total &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;sum&lt;/span&gt;(spectrum)
    &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; total &lt;span class=&quot;op&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.0&lt;/span&gt;:
        &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(spectrum)

    spectrum &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;sorted&lt;/span&gt;(spectrum, reverse&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;True&lt;/span&gt;)
    running_total &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;fl&quot;&gt;0.0&lt;/span&gt;

    &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;op&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;range&lt;/span&gt;(&lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(spectrum)):
        running_total &lt;span class=&quot;op&quot;&gt;+=&lt;/span&gt; spectrum[i]
        &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; running_total &lt;span class=&quot;op&quot;&gt;/&lt;/span&gt; total &lt;span class=&quot;op&quot;&gt;&amp;gt;=&lt;/span&gt; threshold:
            &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; i &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;co&quot;&gt;# guard&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(spectrum)


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;references-cited&quot; class=&quot;unnumbered&quot;&gt;References Cited&lt;/h3&gt;
&lt;div id=&quot;refs&quot; class=&quot;references&quot;&gt;
&lt;div id=&quot;ref-Beaumont2002&quot;&gt;
&lt;p&gt;Beaumont, Mark A, W Zhang, and D J Balding. 2002. “Approximate Bayesian computation in population genetics.” &lt;em&gt;Genetics&lt;/em&gt;. &lt;a href=&quot;http://www.genetics.org/content/162/4/2025.short&quot; class=&quot;uri&quot;&gt;http://www.genetics.org/content/162/4/2025.short&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Csillery2010&quot;&gt;
&lt;p&gt;Csilléry, Katalin, Michael G B Blum, Oscar E Gaggiotti, and Olivier François. 2010. “Approximate Bayesian Computation (ABC) in practice.” &lt;em&gt;Trends in Ecology &amp;amp; Evolution&lt;/em&gt; 25 (7): 410–18. doi:&lt;a href=&quot;https://doi.org/10.1016/j.tree.2010.04.001&quot;&gt;10.1016/j.tree.2010.04.001&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-godsil2001algebraic&quot;&gt;
&lt;p&gt;Godsil, Christopher David, and Gordon Royle. 2001. &lt;em&gt;Algebraic Graph Theory&lt;/em&gt;. Vol. 8. Springer New York.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-koutra2011algorithms&quot;&gt;
&lt;p&gt;Koutra, Danai, Ankur Parikh, Aaditya Ramdas, and Jing Xiang. 2011. “Algorithms for Graph Similarity and Subgraph Matching.” Technical Report of Carnegie-Mellon-University.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Lipo2015&quot;&gt;
&lt;p&gt;Lipo, Mark E. AND Dunnell, Carl P. AND Madsen. 2015. “A Theoretically-Sufficient and Computationally-Practical Technique for Deterministic Frequency Seriation.” &lt;em&gt;PLoS ONE&lt;/em&gt; 10 (4). Public Library of Science: e0124942. doi:&lt;a href=&quot;https://doi.org/10.1371/journal.pone.0124942&quot;&gt;10.1371/journal.pone.0124942&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Marin2012&quot;&gt;
&lt;p&gt;Marin, Jean-Michel, Pierre Pudlo, Christian P Robert, and Robin J Ryder. 2012. “Approximate Bayesian Computational Methods.” &lt;em&gt;Statistics and Computing&lt;/em&gt; 22 (6). Springer: 1167–80.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-Toni2009&quot;&gt;
&lt;p&gt;Toni, Tina, David Welch, Natalja Strelkowa, Andreas Ipsen, and Michael P H Stumpf. 2009. “Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems.” &lt;em&gt;Journal of Royal Society Interface&lt;/em&gt; 6 (31). The Royal Society: 187–202. doi:&lt;a href=&quot;https://doi.org/10.1098/rsif.2008.0172&quot;&gt;10.1098/rsif.2008.0172&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;ref-zager2008graph&quot;&gt;
&lt;p&gt;Zager, Laura A, and George C Verghese. 2008. “Graph Similarity Scoring and Matching.” &lt;em&gt;Applied Mathematics Letters&lt;/em&gt; 21 (1). Elsevier: 86–94.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Mark E. Madsen</name></author><category term="cultural transmission" /><category term="coarse graining" /><category term="simulation" /><category term="dissertation" /><category term="seriation" /><category term="algorithms" /><category term="ML" /><category term="ABC" /><summary type="html">Background</summary></entry></feed>
